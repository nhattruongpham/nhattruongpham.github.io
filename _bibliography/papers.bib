---
---

@article{basith2023adp,
  abbr={CIBM},
  title={ADP-Fuse: A novel dual layer machine learning predictor to identify antidiabetic peptides and diabetes types using multiview information},
  author={Basith, Shaherin and <b>Pham</b>, <b>Nhat Truong</b> and Song, Minkyung and Lee, Gwang and Manavalan, Balachandran},
  journal={Computers in Biology and Medicine},
  volume={165},
  pages={107386},
  year={2023},
  publisher={Elsevier},
  abstract={Diabetes mellitus has become a major public health concern associated with high mortality and reduced life expectancy and can cause blindness, heart attacks, kidney failure, lower limb amputations, and strokes. A new generation of antidiabetic peptides (ADPs) that act on β-cells or T-cells to regulate insulin production is being developed to alleviate the effects of diabetes. However, the lack of effective peptide-mining tools has hampered the discovery of these promising drugs. Hence, novel computational tools need to be developed urgently. In this study, we present ADP-Fuse, a novel two-layer prediction framework capable of accurately identifying ADPs or non-ADPs and categorizing them into type 1 and type 2 ADPs. First, we comprehensively evaluated 22 peptide sequence-derived features coupled with eight notable machine learning algorithms. Subsequently, the most suitable feature descriptors and classifiers for both layers were identified. The output of these single-feature models, embedded with multiview information, was trained with an appropriate classifier to provide the final prediction. Comprehensive cross-validation and independent tests substantiate that ADP-Fuse surpasses single-feature models and the feature fusion approach for the prediction of ADPs and their types. In addition, the SHapley Additive exPlanation method was used to elucidate the contributions of individual features to the prediction of ADPs and their types. Finally, a user-friendly web server for ADP-Fuse was developed and made publicly accessible (https://balalab-skku.org/ADP-Fuse), enabling the swift screening and identification of novel ADPs and their types. This framework is expected to contribute significantly to antidiabetic peptide identification.},
  html={https://www.sciencedirect.com/science/article/abs/pii/S001048252300851X},
  doi={10.1016/j.compbiomed.2023.107386},
  website={https://balalab-skku.org/ADP-Fuse/},
  dimensions={true},
  preview={ADP-Fuse.png}
}

@inproceedings{pham2023server,
  abbr={ICIIT},
  title={SERVER: Multi-modal Speech Emotion Recognition using TransformeR-based and Vision-based Embeddings},
  author={<b>Pham</b>, <b>Nhat Truong</b> and Dang, Duc Ngoc Minh and Pham, Bich Hong Ngoc and Nguyen<sub>1</sub>, Sy Dzung},
  booktitle={2023 8th International Conference on Intelligent Information Technology},
  pages={234--238},
  year={2023},
  abstract={This paper proposes a multi-modal approach for speech emotion recognition (SER) using both text and audio inputs. The audio embedding is extracted by using a vision-based architecture, namely VGGish, while the text embedding is extracted by using a transformer-based architecture, namely BERT. Then, these embeddings are fused using concatenation to recognize emotional states. To evaluate the effectiveness of the proposed method, the benchmark dataset, namely IEMOCAP, is employed in this study. Experimental results indicate that the proposed method is very competitive and better than most of the latest and state-of-the-art methods using multi-modal analysis for SER. The proposed method achieves 63.00% unweighted accuracy (UA) and 63.10% weighted accuracy (WA) on the IEMOCAP dataset. In the future, an extension of multi-task learning and multi-lingual approaches will be investigated to improve the performance and robustness of multi-modal SER. For reproducibility purposes, our code is publicly available.},
  html={https://dl.acm.org/doi/10.1145/3591569.3591610},
  doi={10.1145/3591569.3591610},
  code={https://github.com/nhattruongpham/mmser},
  dimensions={true}
}

@inproceedings{dang2023uplink,
  abbr={ICIIT},
  title={Uplink registration-based MAC protocol for IEEE 802.11ah networks},
  author={Dang, Duc Ngoc Minh and <b>Pham</b>, <b>Nhat Truong</b>},
  booktitle={2023 8th International Conference on Intelligent Information Technology},
  pages={33--37},
  year={2023},
  abstract={IEEE 802.11ah (Wi-Fi HaLow) operates in license-exempt ISM bands below 1 GHz and provides longer-range connectivity. The main advantage of the IEEE 802.11ah is it provides long range connection with low power consumption. RAW (Restricted Access Window) in IEEE 802.11ah helps to reduce the collision probability and enhance the network throughput when many stations contend the channel. Since stations are assigned to uplink RAW slots based on their Association Identifications (AID), the number of stations that have uplink data packets in each RAW slot is a big difference. It results in low fairness among stations. The paper proposes an uplink registration-based MAC protocol for IEEE 802.11ah networks (UR-MAC). In UR-MAC protocol, stations with uplink data will register with the AP by attaching the uplink registration to the data packet during downlink communications. The AP will allocate RAW slots based on the uplink registered station list. The UR-MAC protocol tries to use up the resources of the RAW slots as well as balance the number of stations with uplink data among the RAW slots. Through the evaluation and comparison analysis, the UR-MAC protocol significantly improves the fairness index compared to the IEEE 802.11ah protocol while still ensuring the probability of successful transmission, the average number of successfully transmitted packets, and power efficiency compared to the IEEE 802.11ah protocol.},
  html={https://dl.acm.org/doi/10.1145/3591569.3591575},
  doi={10.1145/3591569.3591575},
  dimensions={true},
  preview={UR-MAC.jpeg}
}

@article{hu2023drugormerdti,
  abbr={CIBM},
  title={DrugormerDTI: Drug Graphormer for drug--target interaction prediction},
  author={Hu, Jiayue and Yu, Wang and Pang, Chao and Jin, Junru and <b>Pham</b>, <b>Nhat Truong</b> and Manavalan, Balachandran and Wei, Leyi},
  journal={Computers in Biology and Medicine},
  volume={161},
  pages={106946},
  year={2023},
  publisher={Elsevier},
  abstract={Drug-target interactions (DTI) prediction is a crucial task in drug discovery. Existing computational methods accelerate the drug discovery in this respect. However, most of them suffer from low feature representation ability, significantly affecting the predictive performance. To address the problem, we propose a novel neural network architecture named DrugormerDTI, which uses Graph Transformer to learn both sequential and topological information through the input molecule graph and Resudual2vec to learn the underlying relation between residues from proteins. By conducting ablation experiments, we verify the importance of each part of the DrugormerDTI. We also demonstrate the good feature extraction and expression capabilities of our model via comparing the mapping results of the attention layer and molecular docking results. Experimental results show that our proposed model performs better than baseline methods on four benchmarks. We demonstrate that the introduction of Graph Transformer and the design of residue are appropriate for drug-target prediction.},
  html={https://www.sciencedirect.com/science/article/pii/S0010482523004110},
  code={https://github.com/joannacatj/drugormerDTI},
  doi={10.1016/j.compbiomed.2023.106946},
  dimensions={true},
  preview={DrugormerDTI.jpg}
}

@article{mustaqeem2023aad,
  abbr={KBS},
  title={AAD-Net: Advanced end-to-end signal processing system for human emotion detection \& recognition using attention-based deep echo state network},
  author={Khan, Mustaqeem and El Saddik, Abdulmotaleb and Alotaibi, Fahd Saleh and <b>Pham</b>, <b>Nhat Truong</b>},
  journal={Knowledge-Based Systems},
  volume={270},
  pages={110525},
  year={2023},
  publisher={Elsevier},
  abstract={Speech signals are the most convenient way of communication between human beings and the eventual method of Human-Computer Interaction (HCI) to exchange emotions and information. Recognizing emotions from speech signals is a challenging task due to the sparse nature of emotional data and features. In this article, we proposed a Deep Echo-State-Network (DeepESN) system for emotion recognition with a dilated convolution neural network and multi-headed attention mechanism. To reduce the model complexity, we incorporate a DeepESN that combines reservoir computing for higher-dimensional mapping. We also used fine-tuned Sparse Random Projection (SRP) to reduce dimensionality and adopted an early fusion strategy to fuse the extracted cues and passed the joint feature vector via a classification layer to recognize emotions. Our proposed model is evaluated on two public speech corpora, EMO-DB and RAVDESS, and tested for subject/speaker-dependent/independent performance. The results show that our proposed system achieves a high recognition rate, 91.14, 85.57 for EMO-DB, and 82.01, 77.02 for RAVDESS, using speaker-dependent and independent experiments, respectively. Our proposed system outperforms the State-of-The-Art (SOTA) while requiring less computational time.},
  html={https://www.sciencedirect.com/science/article/abs/pii/S0950705123002757},
  doi={10.1016/j.knosys.2023.110525},
  dimensions={true},
  preview={AAD-Net.jpg}
}

@article{kumar2023towards,
  abbr={SOCO},
  title={Towards an efficient machine learning model for financial time series forecasting},
  author={Kumar, Arun and Chauhan, Tanya and Natesan, Srinivasan and <b>Pham</b>, <b>Nhat Truong</b> and Nguyen<sub>2</sub>, Ngoc Duy and Lim, Chee Peng},
  journal={Soft Computing},
  volumev={27},
  pages={11329--11339},
  year={2023},
  publisher={Springer},
  abstract={Financial time series forecasting is a challenging problem owing to the high degree of randomness and absence of residuals in time series data. Existing machine learning solutions normally do not perform well on such data. In this study, we propose an efficient machine learning model for financial time series forecasting through carefully designed feature extraction, elimination, and selection strategies. We leverage a binary particle swarm optimization algorithm to select the appropriate features and propose new evaluation metrics, i.e. mean weighted square error and mean weighted square ratio, for better performance assessment in handling financial time series data. Both indicators ascertain that our proposed model is effective, which outperforms several existing methods in benchmark studies.},
  html={https://link.springer.com/article/10.1007/s00500-023-08676-x},
  doi={10.1007/s00500-023-08676-x},
  dimensions={true},
  preview={StockML.webp}
}

@article{pham2023hybrid,
  abbr={ESWA},
  title={Hybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition},
  author={<b>Pham</b>, <b>Nhat Truong</b> and Dang, Duc Ngoc Minh and Nguyen<sub>2</sub>, Ngoc Duy and Nguyen<sub>3</sub>, Thanh Thi and Nguyen<sub>4</sub>, Hai and Manavalan, Balachandran and Lim, Chee Peng and Nguyen<sub>1</sub>, Sy Dzung},
  journal={Expert Systems with Applications},
  pages={120608},
  year={2023},
  publisher={Elsevier},
  abstract={Recently, speech emotion recognition (SER) has become an active research area in speech processing, particularly with the advent of deep learning (DL). Numerous DL-based methods have been proposed for SER. However, most of the existing DL-based models are complex and require a large amounts of data to achieve a good performance. In this study, a new framework of deep attention-based dilated convolutional-recurrent neural networks coupled with a hybrid data augmentation method was proposed for addressing SER tasks. The hybrid data augmentation method constitutes an upsampling technique for generating more speech data samples based on the traditional and generative adversarial network approaches. By leveraging both convolutional and recurrent neural networks in a dilated form along with an attention mechanism, the proposed DL framework can extract high-level representations from three-dimensional log Mel spectrogram features. Dilated convolutional neural networks acquire larger receptive fields, whereas dilated recurrent neural networks overcome complex dependencies as well as the vanishing and exploding gradient issues. Furthermore, the loss functions are reconfigured by combining the SoftMax loss and the center-based losses to classify various emotional states. The proposed framework was implemented using the Python programming language and the TensorFlow deep learning library. To validate the proposed framework, the EmoDB and ERC benchmark datasets, which are imbalanced and/or small datasets, were employed. The experimental results indicate that the proposed framework outperforms other related state-of-the-art methods, yielding the highest unweighted recall rates of 88.03 &plusmn; 1.39 (%) and 66.56 &plusmn; 0.67 (%) for the EmoDB and ERC datasets, respectively.},
  html={https://www.sciencedirect.com/science/article/pii/S0957417423011107},
  doi={10.1016/j.eswa.2023.120608},
  code={https://github.com/nhattruongpham/hda-adcrnn-ser},
  selected={true},
  dimensions={true},
  preview={HDA_mADCRNN.jpg}
}

@article{charoenkwan2023pretoria,
  abbr={IJBIOMAC},
  title={Pretoria: An effective computational approach for accurate and high-throughput identification of CD8<sup>+</sup> t-cell epitopes of eukaryotic pathogens},
  author={Charoenkwan, Phasit and Schaduangrat, Nalini and <b>Pham</b>, <b>Nhat Truong</b> and Manavalan, Balachandran and Shoombuatong, Watshara},
  journal={International Journal of Biological Macromolecules},
  volume={238},
  pages={124228},
  year={2023},
  publisher={Elsevier},
  abstract={T-cells recognize antigenic epitopes present on major histocompatibility complex (MHC) molecules, triggering an adaptive immune response in the host. T-cell epitope (TCE) identification is challenging because of the extensive number of undetermined proteins found in eukaryotic pathogens, as well as MHC polymorphisms. In addition, conventional experimental approaches for TCE identification are time-consuming and expensive. Thus, computational approaches that can accurately and rapidly identify CD8<sup>+</sup> T-cell epitopes (TCEs) of eukaryotic pathogens based solely on sequence information may facilitate the discovery of novel CD8<sup>+</sup> TCEs in a cost-effective manner. Here, Pretoria (Predictor of CD8<sup>+</sup> TCEs of eukaryotic pathogens) is proposed as the first stack-based approach for accurate and large-scale identification of CD8<sup>+</sup> TCEs of eukaryotic pathogens. In particular, Pretoria enabled the extraction and exploration of crucial information embedded in CD8<sup>+</sup> TCEs by employing a comprehensive set of 12 well-known feature descriptors extracted from multiple groups, including physicochemical properties, composition-transition-distribution, pseudo-amino acid composition, and amino acid composition. These feature descriptors were then utilized to construct a pool of 144 different machine learning (ML)-based classifiers based on 12 popular ML algorithms. Finally, the feature selection method was used to effectively determine the important ML classifiers for the construction of our stacked model. The experimental results indicated that Pretoria is an accurate and effective computational approach for CD8<sup>+</sup> TCE prediction; it was superior to several conventional ML classifiers and the existing method in terms of the independent test, with an accuracy of 0.866, MCC of 0.732, and AUC of 0.921. Additionally, to maximize user convenience for high-throughput identification of CD8<sup>+</sup> TCEs of eukaryotic pathogens, a user-friendly web server of Pretoria (http://pmlabstack.pythonanywhere.com/Pretoria) was developed and made freely available.},
  html={https://www.sciencedirect.com/science/article/abs/pii/S0141813023011224},
  doi={10.1016/j.ijbiomac.2023.124228},
  website={http://pmlabstack.pythonanywhere.com/Pretoria},
  dimensions={true},
  preview={Pretoria.jpg}
}

@article{pham2023exploratory,
  abbr={Heliyon},
  title={An exploratory simulation study and prediction model on human brain behavior and activity using an integration of deep neural network and biosensor Rabi antenna},
  author={<b>Pham</b>, <b>Nhat Truong</b> and Bunruangses, Montree and Youplao, Phichai and Garhwal, Anita and Ray, Kanad and Roy, Arup and Boonkirdram, Sarawoot and Yupapin, Preecha and Jalil, Muhammad Arif and Ali, Jalil and Kaiser, Shamim and Mahmud, Mufti and Mallik, Saurav and Zhao, Zhongming},
  journal={Heliyon},
  volume={9},
  number={5},
  year={2023},
  publisher={Elsevier},
  abstract={The plasmonic antenna probe is constructed using a silver rod embedded in a modified Mach-Zehnder interferometer (MZI) ad-drop filter. Rabi antennas are formed when space-time control reaches two levels of system oscillation and can be used as human brain sensor probes. Photonic neural networks are designed using brain-Rabi antenna communication, and transmissions are connected via neurons. Communication signals are carried by electron spin (up and down) and adjustable Rabi frequency. Hidden variables and deep brain signals can be obtained by external detection. A Rabi antenna has been developed by simulation using computer simulation technology (CST) software. Additionally, a communication device has been developed that uses the Optiwave program with Finite-Difference Time-Domain (OptiFDTD). The output signal is plotted using the MATLAB program with the parameters of the OptiFDTD simulation results. The proposed antenna oscillates in the frequency range of 192 THz to 202 THz with a maximum gain of 22.4 dBi. The sensitivity of the sensor is calculated along with the result of electron spin and applied to form a human brain connection. Moreover, intelligent machine learning algorithms are proposed to identify high-quality transmissions and predict the behavior of transmissions in the near future. During the process, a root mean square error (RMSE) of 2.3332 (&plusmn;0.2338) was obtained. Finally, it can be said that our proposed model can efficiently predict human mind, thoughts, behavior as well as action/reaction, which can be greatly helpful in the diagnosis of various neuro-degenerative/psychological diseases (such as Alzheimer's, dementia, etc.) and for security purposes.},
  html={https://www.sciencedirect.com/science/article/pii/S2405844023029560},
  doi={10.1016/j.heliyon.2023.e15749},
  pdf={https://www.cell.com/heliyon/pdf/S2405-8440(23)02956-0.pdf},
  code={https://github.com/nhattruongpham/Deep_Brain_SigNet},
  dimensions={true},
  preview={Humain_brain.jpg}
}

@article{pham2023speech,
  abbr={T\&F JIT},
  title={Speech emotion recognition using overlapping sliding window and Shapley additive explainable deep neural network},
  author={<b>Pham</b>, <b>Nhat Truong</b> and Nguyen<sub>1</sub>, Sy Dzung and Nguyen, Vu Song Thuy and Pham, Bich Ngoc Hong and Dang, Duc Ngoc Minh},
  journal={Journal of Information and Telecommunication},
  volumevo={7},
  number={3},
  pages={317--335},
  year={2023},
  publisher={Taylor \& Francis},
  abstract={Speech emotion recognition (SER) has several applications, such as e-learning, human-computer interaction, customer service, and healthcare systems. Although researchers have investigated lots of techniques to improve the accuracy of SER, it has been challenging with feature extraction, classifier schemes, and computational costs. To address the aforementioned problems, we propose a new set of 1D features extracted by using an overlapping sliding window (OSW) technique for SER in this study. In addition, a deep neural network-based classifier scheme called the deep Pattern Recognition Network (PRN) is designed to categorize emotional states from the new set of 1D features. We evaluate the proposed method on the Emo-DB and the AESSD datasets that contain several different emotional states. The experimental results show that the proposed method achieves an accuracy of 98.5% and 87.1% on the Emo-DB and AESSD datasets, respectively. It is also more comparable with accuracy to and better than the state-of-the-art and current approaches that use 1D features on the same datasets for SER. Furthermore, the SHAP (SHapley Additive exPlanations) analysis is employed for interpreting the prediction model to assist system developers in selecting the optimal features to integrate into the desired system.},
  html={https://www.tandfonline.com/doi/full/10.1080/24751839.2023.2187278},
  doi={10.1080/24751839.2023.2187278},
  pdf={https://www.tandfonline.com/doi/epdf/10.1080/24751839.2023.2187278?needAccess=true&role=button},
  code={https://github.com/nhattruongpham/osw-1d-prn-shap},
  dimensions={true},
  preview={osw-1d-prn-shap.jpeg}
}

@article{nguyen2023fruit,
  abbr={ESWA},
  title={Fruit-CoV: An efficient vision-based framework for speedy detection and diagnosis of SARS-CoV-2 infections through recorded cough sounds},
  author={Nguyen, Long H and <b>Pham</b><sup>(*)(†)</sup>, <b>Nhat Truong</b> and Do, Van Huong and Nguyen, Liu Tai and Nguyen, Thanh Tin and Nguyen<sub>4</sub>, Hai and Nguyen<sub>2</sub>, Ngoc Duy and Nguyen<sub>3</sub>, Thanh Thi and Nguyen<sub>1</sub>, Sy Dzung and Bhatti, Asim and Lim, Chee Peng},
  journal={Expert Systems with Applications},
  volume={213},
  pages={119212},
  year={2023},
  publisher={Elsevier},
  abstract={COVID-19 is an infectious disease caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). This deadly virus has spread worldwide, leading to a global pandemic since March 2020. A recent variant of SARS-CoV-2 named Delta is intractably contagious and responsible for more than four million deaths globally. Therefore, developing an efficient self-testing service for SARS-CoV-2 at home is vital. In this study, a two-stage vision-based framework, namely Fruit-CoV, is introduced for detecting SARS-CoV-2 infections through recorded cough sounds. Specifically, audio signals are converted into Log-Mel spectrograms, and the EfficientNet-V2 network is used to extract their visual features in the first stage. In the second stage, 14 convolutional layers extracted from the large-scale Pretrained Audio Neural Networks for audio pattern recognition (PANNs) and the Wavegram-Log-Mel-CNN are employed to aggregate feature representations of the Log-Mel spectrograms and the waveform. Finally, the combined features are used to train a binary classifier. In this study, a dataset provided by the AICovidVN 115M Challenge is employed for evaluation. It includes 7,371 recorded cough sounds collected throughout Vietnam, India, and Switzerland. Experimental results indicate that the proposed model achieves an Area Under the Receiver Operating Characteristic Curve (AUC) score of 92.8% and ranks first on the final leaderboard of the AICovidVN 115M Challenge. Our code is publicly available.},
  html={https://www.sciencedirect.com/science/article/pii/S0957417422022308},
  doi={10.1016/j.eswa.2022.119212},
  code={https://github.com/nhattruongpham/Fruit-CoV},
  selected={true},
  dimensions={true},
  preview={Fruit-CoV.jpg}
}

@article{nguyen2023towards,
  abbr={APIN},
  title={Towards designing a generic and comprehensive deep reinforcement learning framework},
  author={Nguyen<sub>2</sub>, Ngoc Duy and Nguyen<sub>3</sub>, Thanh Thi and <b>Pham</b>, <b>Nhat Truong</b> and Nguyen<sub>4</sub>, Hai and Nguyen, Dang Tu and Nguyen, Thanh Dang and Lim, Chee Peng and Johnstone, Michael and Bhatti, Asim and Creighton, Douglas and Nahavandi, Saeid},
  journal={Applied Intelligence},
  volume={53},
  number={3},
  pages={2967--2988},
  year={2023},
  publisher={Springer},
  abstract={Reinforcement learning (RL) has emerged as an effective approach for building an intelligent system, which involves multiple self-operated agents to collectively accomplish a designated task. More importantly, there has been a renewed focus on RL since the introduction of deep learning that essentially makes RL feasible to operate in high-dimensional environments. However, there are many diversified research directions in the current literature, such as multi-agent and multi-objective learning, and human-machine interactions. Therefore, in this paper, we propose a comprehensive software architecture that not only plays a vital role in designing a connect-the-dots deep RL architecture but also provides a guideline to develop a realistic RL application in a short time span. By inheriting the proposed architecture, software managers can foresee any challenges when designing a deep RL-based system. As a result, they can expedite the design process and actively control every stage of software development, which is especially critical in agile development environments. For this reason, we design a deep RL-based framework that strictly ensures flexibility, robustness, and scalability. To enforce generalization, the proposed architecture also does not depend on a specific RL algorithm, a network configuration, the number of agents, or the type of agents.},
  html={https://link.springer.com/article/10.1007/s10489-022-03550-z},
  doi={10.1007/s10489-022-03550-z},
  code={https://github.com/garlicdevs/Fruit-API},
  website={https://fruitlab.org/},
  dimensions={true},
  preview={Fruit-API.webp}
}

@article{tin2022viecap4h,
  abbr={VLSP},
  title={vieCap4H Challenge 2021: Vietnamese Image Captioning for Healthcare Domain using Swin Transformer and Attention-based LSTM},
  author={Nguyen, Thanh Tin and Nguyen, Long H and <b>Pham</b>, <b>Nhat Truong</b> and Nguyen, Liu Tai and Do, Van Huong and Nguyen<sub>4</sub>, Hai and Nguyen<sub>2</sub>, Ngoc Duy},
  journal={VNU Journal of Science: Computer Science and Communication Engineering},
  volume={38},
  number={2},
  year={2022},
  abstract={This study presents our approach on the automatic Vietnamese image captioning for healthcare domain in text processing tasks of Vietnamese Language and Speech Processing (VLSP) Challenge 2021, as shown in Figure 1. In recent years, image captioning often employs a convolutional neural network-based architecture as an encoder and a long short-term memory (LSTM) as a decoder to generate sentences. These models perform  remarkably well in different datasets. Our proposed model also has an encoder and a decoder, but we instead use a Swin Transformer in the encoder, and a LSTM combined with an attention module in the decoder. The study presents our training experiments and techniques used during the competition. Our model achieves a BLEU4 score of 0.293 on the vietCap4H dataset, and the score is ranked the 3<sup>rd</sup> place on the private leaderboard. Our code can be found at https://github.com/ngthanhtin/VLSP_ImageCaptioning for reproducible purposes.},
  html={https://jcsce.vnu.edu.vn/index.php/jcsce/article/view/369},
  doi={10.25073/2588-1086/vnucsce.369},
  arxiv={arXiv:2209.01304},
  code={https://github.com/ngthanhtin/VLSP_ImageCaptioning},
  dimensions={true}
}

@inproceedings{dang2022space,
  abbr={ATC},
  title={Space-Frequency Diversity based MAC protocol for IEEE 802.11 ah networks},
  author={Dang, Duc Ngoc Minh and Tran, Van Thau and Nguyen, Hoang Lam and <b>Pham</b>, <b>Nhat Truong</b> and Tran, Anh Khoa and Dang, Ngoc-Hanh},
  booktitle={2022 International Conference on Advanced Technologies for Communications (ATC)},
  pages={159--164},
  year={2022},
  organization={IEEE},
  abstract={IEEE 802.11ah is a sub-GHz communication technology to offer longer range and low power connectivity for the Internet of Things (IoT) applications. A Restricted Access Window (RAW) is specified to decrease the collision probability. Stations are divided into groups and stations from each group attempt to access the channel by employing the Distributed Coordination Function during their assigned RAW slots. However, the network throughput is limited by a single channel MAC protocol. In this paper, Space-Frequency Diversity-based MAC protocol for the IEEE 802.11ah network (SF-MAC protocol) is proposed to allow stations of different sectors to transmit packets on different channels with the help of Forwarders. The proposed SF-MAC protocol improves the packet delivery ratio and aggregate throughput of the network.},
  html={https://ieeexplore.ieee.org/abstract/document/9943042},
  doi={10.1109/ATC55345.2022.9943042},
  dimensions={true},
  preview={SF-MAC.gif}
}

@inproceedings{pham2022key,
  abbr={GTSD},
  title={Key Information Extraction from Mobile-Captured Vietnamese Receipt Images using Graph Neural Networks Approach},
  author={Pham, Van Dung and Nguyen, Le Quan and <b>Pham</b>, <b>Nhat Truong</b> and Nguyen, Bao Hung and Dang, Duc Ngoc Minh and Nguyen<sub>1</sub>, Sy Dzung},
  booktitle={2022 6th International Conference on Green Technology and Sustainable Development (GTSD)},
  pages={232--237},
  year={2022},
  organization={IEEE},
  abstract={Information extraction and retrieval are growing fields that have a significant role in document parser and analysis systems. Researches and applications developed in recent years show the numerous difficulties and obstacles in extracting key information from documents. Thanks to the raising of graph theory and deep learning, graph representation and graph learning have been widely applied in information extraction to obtain more exact results. In this paper, we propose a solution upon graph neural networks (GNN) for key information extraction (KIE) that aims to extract the key information from mobile-captured Vietnamese receipt images. Firstly, the images are pre-processed using U<sup>2</sup>-Net, and then a CRAFT model is used to detect texts from the pre-processed images. Next, the implemented TransformerOCR model is employed for text recognition. Finally, a GNN-based model is designed to extract the key information based on the recognized texts. For validating the effectiveness of the proposed solution, the publicly available dataset released from the Mobile-Captured Receipt Recognition (MC-OCR) Challenge 2021 is used to train and evaluate. The experimental results indicate that our proposed solution achieves a character error rate (CER) score of 0.25 on the private test set, which is more comparable with all reported solutions in the MC-OCR Challenge 2021 as mentioned in the literature. For reproducing and knowledge-sharing purposes, our implementation of the proposed solution is publicly available at https://github.com/ThorPham/Key_infomation_extraction.},
  html={https://ieeexplore.ieee.org/abstract/document/9989111},
  doi={10.1109/GTSD54989.2022.9989111},
  code={https://github.com/ThorPham/Key_infomation_extraction},
  dimensions={true}
}

@inproceedings{pham2022vietnamese,
  abbr={GTSD},
  title={Vietnamese Scene Text Detection and Recognition using Deep Learning: An Empirical Study},
  author={<b>Pham</b>, <b>Nhat Truong</b> and Pham, Van Dung and Nguyen-Van, Qui and Nguyen, Bao Hung and Dang, Duc Ngoc Minh and Nguyen<sub>1</sub>, Sy Dzung},
  booktitle={2022 6th International Conference on Green Technology and Sustainable Development (GTSD)},
  pages={213--218},
  year={2022},
  organization={IEEE},
  abstract={Scene text detection and recognition are vital challenging tasks in computer vision, which are to detect and recognize sequences of texts in natural scenes. Recently, researchers have investigated a lot of state-of-the-art methods to improve the accuracy and efficiency of text detection and recognition. However, there has been little research on text detection and recognition in natural scenes in Vietnam. In this paper, a deep learning-based empirical investigation of Vietnamese scene text detection and recognition is presented. Firstly, four detection models including differentiable binarization network (DBN), pyramid mask text detector (PMTD), pixel aggregation network (PAN), and Fourier contour embedding network (FCEN), are employed to detect text regions from the images. Then, four text recognition models including convolutional recurrent neural network (CRNN), self-attention text recognition network (SATRN), no-recurrence sequence-to-sequence text recognizer (NRTR), and RobustScanner (RS) are also investigated to recognize the texts. Moreover, data augmentation methods are also applied to enrich data for improving the accuracy and enhancing the performance of scene text detection and recognition. To validate the effectiveness of scene text detection and recognition models, the VinText dataset is employed for evaluation. Empirical results show that PMTD and SATRN achieve the highest scores among the others for text detection and recognition, respectively. For knowledge-sharing, our implementation is publicly available at https://github.com/ThorPham/VN_scene_text_detection_recognition.},
  html={https://ieeexplore.ieee.org/abstract/document/9989248},
  doi={10.1109/GTSD54989.2022.9989248},
  code={https://github.com/ThorPham/VN_scene_text_detection_recognition},
  dimensions={true},
  preview={SceneText.gif}
}

@inproceedings{dang2022safety,
  abbr={ICCE},
  title={Safety Message Broadcast Reliability Enhancement MAC protocol in VANETs},
  author={Dang, Duc Ngoc Minh and Tran, Anh Khoa and <b>Pham</b>, <b>Nhat Truong</b> and Tran, Khanh Duong and Dang, Hanh Ngoc},
  booktitle={2022 IEEE Ninth International Conference on Communications and Electronics (ICCE)},
  pages={69--74},
  year={2022},
  organization={IEEE},
  abstract={Recently, Vehicular Ad-hoc Networks (VANETs) have been considered as an important part of the Intelligent Transportation System. Data transmission in VANET can be safety message and non-safety message transmissions. While the safety message transmission typically requires bounded delay and a high packet delivery ratio, the non-safety message transmission demands sufficiently high throughput. In this paper, a MAC protocol for Safety message broadcast Reliability Enhancement in VANETs, named SRE-MAC protocol, is proposed to ensure both the reliability of safety message transmission and the high throughput for non-safety data transmission. In particular, the proposed SRE-MAC employs a time slot allocation of TDMA and a random-access technique of CSMA schemes for accessing the control channel. To evaluate our proposed SRE-MAC protocol, some extensive simulations are conducted. The simulation results show that the proposed SRE-MAC protocol achieves higher performance in terms of safety packet delivery ratio and throughput of non-safety packets, as compared to the IEEE 1609.4 and the VER-MAC protocol.},
  html={https://ieeexplore.ieee.org/abstract/document/9852052},
  doi={10.1109/ICCE55644.2022.9852052},
  dimensions={true},
  preview={SRE-MAC.gif}
}

@article{tran2022deep,
  abbr={Sci Rep},
  title={A deep learning approach for detecting drill bit failures from a small sound dataset},
  author={Tran, Thanh and <b>Pham</b>, <b>Nhat Truong</b> and Lundgren, Jan},
  journal={Scientific Reports},
  volume={12},
  number={1},
  pages={9623},
  year={2022},
  publisher={Nature Publishing Group UK London},
  abstract={Monitoring the conditions of machines is vital in the manufacturing industry. Early detection of faulty components in machines for stopping and repairing the failed components can minimize the downtime of the machine. In this article, we present a method for detecting failures in drill machines using drill sounds in Valmet AB, a company in Sundsvall, Sweden that supplies equipment and processes for the production of pulp, paper, and biofuels. The drill dataset includes two classes: anomalous sounds and normal sounds. Detecting drill failure effectively remains a challenge due to the following reasons. The waveform of drill sound is complex and short for detection. Furthermore, in realistic soundscapes, both sounds and noise exist simultaneously. Besides, the balanced dataset is small to apply state-of-the-art deep learning techniques. Due to these aforementioned difficulties, sound augmentation methods were applied to increase the number of sounds in the dataset. In this study, a convolutional neural network (CNN) was combined with a long-short-term memory (LSTM) to extract features from log-Mel spectrograms and to learn global representations of two classes. A leaky rectified linear unit (Leaky ReLU) was utilized as the activation function for the proposed CNN instead of the ReLU. Moreover, an attention mechanism was deployed at the frame level after the LSTM layer to pay attention to the anomaly in sounds. As a result, the proposed method reached an overall accuracy of 92.62% to classify two classes of machine sounds on Valmet's dataset. In addition, an extensive experiment on another drilling dataset with short sounds yielded 97.47% accuracy. With multiple classes and long-duration sounds, an experiment utilizing the publicly available UrbanSound8K dataset obtains 91.45%. Extensive experiments on our dataset as well as publicly available datasets confirm the efficacy and robustness of our proposed method. For reproducing and deploying the proposed system, an open-source repository is publicly available at https://github.com/thanhtran1965/DrillFailureDetection_SciRep2022.},
  html={https://www.nature.com/articles/s41598-022-13237-7},
  doi={10.1038/s41598-022-13237-7},
  code={https://github.com/thanhtran1965/DrillFailureDetection_SciRep2022},
  dimensions={true},
  preview={SciRep.webp}
}

@article{pham2022improving,
  abbr={JCC},
  title={Improving ligand-ranking of AutoDock Vina by changing the empirical parameters},
  author={Pham, T Ngoc Han and Nguyen<sub>5</sub>, Trung Hai and Tam, Nguyen Minh and Y. Vu, Thien and <b>Pham</b>, <b>Nhat Truong</b> and Huy, Nguyen Truong and Mai, Binh Khanh and Tung, Nguyen Thanh and Pham, Minh Quan and V. Vu, Van and Ngo, Son Tung},
  journal={Journal of Computational Chemistry},
  volume={43},
  number={3},
  pages={160--169},
  year={2022},
  publisher={Wiley Online Library},
  abstract={AutoDock Vina (Vina) achieved a very high docking-success rate, <i>p&#770;</i>, but give a rather low correlation coefficient, <i>R</i>, for binding affinity with respect to experiments. This low correlation can be an obstacle for ranking of ligand-binding affinity, which is the main objective of docking simulations. In this context, we evaluated the dependence of Vina <i>R</i> coefficient upon its empirical parameters. <i>R</i> is affected more by changing the gauss2 and rotation than other terms. The docking-success rate <i>p&#770;</i> is sensitive to the alterations of the gauss1, gauss2, repulsion, and hydrogen bond parameters. Based on our benchmarks, the parameter set1 has been suggested to be the most optimal. The testing study over 800 complexes indicated that the modified Vina provided higher correlation with experiment <i>R<sub>set1</sub>=0.556&plusmn;0.025</i> compared with <i>R<sub>Default</sub>=0.493&plusmn;0.028</i> obtained by the original Vina and <i>R<sub>Vina 1.2</sub>=0.503&plusmn;0.029</i> by Vina version 1.2. Besides, the modified Vina can be also applied more widely, giving <i>R &ge; 0.500</i> for 32/48 targets, compared with the default package, giving <i>R &ge; 0.500</i> for 31/48 targets. In addition, validation calculations for 1036 complexes obtained from version 2019 of PDBbind refined structures showed that the set1 of parameters gave higher correlation coefficient (<i>R<sub>set1</sub>=0.617&plusmn;0.017</i>) than the default package (<i>R<sub>Default</sub>=0.543&plusmn;0.020</i>) and Vina version 1.2 (<i>R<sub>Vina 1.2</sub>=0.540&plusmn;0.020</i>). The version of Vina with set1 of parameters can be downloaded at https://github.com/sontungngo/mvina. The outcomes would enhance the ranking of ligand-binding affinity using Autodock Vina.},
  dimensions={true},
  doi={10.1002/jcc.26779},
  html={https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.26779},
  code={https://github.com/nhattruongpham/mvina},
  preview={mVina.jpeg}
}

@article{nguyen2022determination,
  abbr={IEEE TFS},
  title={Determination of the optimal number of clusters: a fuzzy-set based method},
  author={Nguyen<sub>1</sub>, Sy Dzung and Nguyen, Vu Song Thuy and <b>Pham</b>, <b>Nhat Truong</b>},
  journal={IEEE Transactions on Fuzzy Systems},
  volume={30},
  number={9},
  pages={3514--3526},
  year={2022},
  publisher={IEEE},
  abstract={The optimal number of clusters (<i>C<sub>opt</sub></i>) is one of the determinants of clustering efficiency. In this article, we present a new method of quantifying <i>C<sub>opt</sub></i> for centroid-based clustering. First, we propose a new clustering validity index named fRisk(<i>C</i>) based on the fuzzy set theory. It takes the role of normalization and accumulation of local risks coming from each action either splitting data from a cluster or merging data into a cluster. fRisk(<i>C</i>) exploits the local distribution information of the database to catch the global information of the clustering process in the form of the risk degree. Based on the monotonous reduction property of fRisk(<i>C</i>), which is proved theoretically, we present a fRisk-based new algorithm named fRisk4-bA for determining <i>C<sub>opt</sub></i>. In the algorithm, the well-known L-method is employed as a supplemented tool to catch <i>C<sub>opt</sub></i> on the graph of the fRisk(<i>C</i>). Along with the stable convergence trend of the method to be proved theoretically, numerical surveys are also carried out. The surveys show that the high reliability and stability, as well as the sensitivity in separating/merging clusters in high-density areas, even if the presence of noise in the databases, are the strong points of the proposed method.},
  html={https://ieeexplore.ieee.org/abstract/document/9562269},
  doi={10.1109/TFUZZ.2021.3118113},
  dimensions={true},
  preview={fRiskC.gif}
}

@inproceedings{nguyen2022hcilab,
  abbr={De-Factify},
  title={HCILab at Memotion 2.0 2022: Analysis of sentiment, emotion and intensity of emotion classes from meme images using single and multi modalities},
  author={Nguyen<sub>6</sub>, Thanh Tin and <b>Pham</b>, <b>Nhat Truong</b> and Nguyen<sub>2</sub>, Ngoc Duy and Nguyen<sub>4</sub>, Hai and Nguyen, Long H and Kim, Yong-Guk},
  booktitle={Proceedings of De-Factify: Workshop on Multimodal Fact Checking and Hate Speech Detection (De-Factify&#64;AAAI 2022), CEUR},
  year={2022},
  abstract={Nowadays, memes found on internet are overwhelming. Although they are innocuous and sometimes entertaining, there exist memes that contain sarcasm, offensive, or motivational feelings. In this study, several approaches are proposed to solve the multiple modality problem in analysing the given meme dataset. The imbalance issue has been addressed by using a new Auto Augmentation method and the uncorrelation issue has been mitigated by adopting deep Canonical Correlation Analysis to find the most correlated projections of visual and textual feature embedding. In addition, both stacked attention and multi-hop attention network are employed to efficiently generate aggregated features. As a result, our team, i.e. HCILab, achieved a weighted F1 score of 0.4995 for sentiment analysis, 0.7414 for emotion classification, and 0.5301 for scale/intensity of emotion classes on the leaderboard. This results are obtained by using concatenation between image and text model and our code can be found at https://github.com/ngthanhtin/Memotion2_AAAI_WS_2022.},
  html={https://ceur-ws.org/Vol-3199/},
  pdf={https://ceur-ws.org/Vol-3199/paper12.pdf},
  code={https://github.com/ngthanhtin/Memotion2_AAAI_WS_2022},
  dimensions={true},
  preview={Memotion2.png}
}

@inproceedings{tran2021separate,
  abbr={SSCI},
  title={Separate sound into STFT frames to eliminate sound noise frames in sound classification},
  author={Tran, Thanh and Huy, Kien Bui and <b>Pham</b>, <b>Nhat Truong</b> and Carrat{\`u}, Marco and Liguori, Consolatina and Lundgren, Jan},
  booktitle={2021 IEEE Symposium Series on Computational Intelligence (SSCI)},
  pages={1--7},
  year={2021},
  organization={IEEE},
  abstract={Sounds always contain acoustic noise and background noise that affects the accuracy of the sound classification system. Hence, suppression of noise in the sound can improve the robustness of the sound classification model. This paper investigated a sound separation technique that separates the input sound into many overlapped-content Short-Time Fourier Transform (STFT) frames. Our approach is different from the traditional STFT conversion method, which converts each sound into a single STFT image. Contradictory, separating the sound into many STFT frames improves model prediction accuracy by increasing variability in the data and therefore learning from that variability. These separated frames are saved as images and then labeled manually as clean and noisy frames which are then fed into transfer learning convolutional neural networks (CNNs) for the classification task. The pre-trained CNN architectures that learn from these frames become robust against the noise. The experimental results show that the proposed approach is robust against noise and achieves 94.14% in terms of classifying 21 classes including 20 classes of sound events and a noisy class. An open-source repository of the proposed method and results is available at https://github.com/nhattruongpham/soundSepsound.},
  html={https://ieeexplore.ieee.org/abstract/document/9660125},
  doi={10.1109/SSCI50451.2021.9660125},
  code={https://github.com/nhattruongpham/soundSepsound},
  dimensions={true},
  preview={soundSepsound.gif}
}

@article{pham2020method,
  abbr={JAEC},
  title={A method upon deep learning for speech emotion recognition},
  author={<b>Pham</b>, <b>Nhat Truong</b> and Dang, Duc Ngoc Minh and Nguyen<sub>1</sub>, Sy Dzung},
  journal={Journal of Advanced Engineering and Computation},
  volume={4},
  number={4},
  pages={273--285},
  year={2020},
  abstract={Feature extraction and emotional classification are significant roles in speech emotion recognition. It is hard to extract and select the optimal features, researchers can not be sure what the features should be. With deep learning approaches, features could be extracted by using hierarchical abstraction layers, but it requires high computational resources and a large number of data. In this article, we choose static, differential, and acceleration coefficients of log Mel-spectrogram as inputs for the deep learning model. To avoid performance degradation, we also add a skip connection with dilated convolution network integration. All representatives are fed into a self-attention mechanism with bidirectional recurrent neural networks to learn long term global features and exploit context for each time step. Finally, we investigate contrastive center loss with softmax loss as loss function to improve the accuracy of emotion recognition. For validating robustness and effectiveness, we tested the proposed method on the Emo-DB and ERC2019 datasets. Experimental results show that the performance of the proposed method is strongly comparable with the existing state-of-the-art methods on the Emo-DB and ERC2019 with 88% and 67%, respectively.},
  html={https://jaec.vn/index.php/JAEC/article/view/311},
  doi={10.25073/jaec.202044.311},
  pdf={https://jaec.vn/index.php/JAEC/article/view/311/147},
  dimensions={true}
}