<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Nhat Truong Pham</title> <meta name="author" content="Nhat Truong Pham"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha256-1oz1+rx2BF/9OiBUPSh1NEUwyUECNU5O70RoKnClGNs" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://nhattruongpham.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/collaborators/">Collaborators</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/tools/">Tools</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/gallery/">Gallery</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Nhat Truong Pham </h1> <p class="desc">Ph.D. candidate in Integrative Biotechnology specializing in üß¨Applied Machine Learning/Deep Learning for Computational Biology and Bioinformaticsüß¨</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile_since_2024-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile_since_2024-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile_since_2024-1400.webp"></source> <img src="/assets/img/profile_since_2024.png" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="profile_since_2024.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>Room 62105, Biotechnology and Bioengineering Building 2, Seobu-ro, Jangan-gu, Suwon-si, Gyeonggi-do, 16419, Republic of Korea</p> </div> </div> <div class="clearfix"> <p><strong>Short Bio:</strong> My name is <i>Ph·∫°m Nh·∫≠t Tr∆∞·ªùng</i> - Vietnamese. I am currently a <del>first</del>/second-year Ph.D. <del>student</del>/candidate at the <a href="https://skb.skku.edu/eng_gene/index.do" rel="external nofollow noopener" target="_blank">Department of Integrative Biotechnology</a>, <a href="https://biotech.skku.edu/eng_biotech/index.do" rel="external nofollow noopener" target="_blank">College of Biotechnology and Bioengineering</a>, <a href="https://www.skku.edu/eng/" rel="external nofollow noopener" target="_blank">Sungkyunkwan University (SKKU)</a>, Republic of Korea. I am working under the guidance of Professor <a href="https://skb.skku.edu/eng_gene/faculty.do?mode=view&amp;perId=LZStrB4DgrAqgzgNgwgGQCwGkAuArAigEQE4BCA5gHYBmMAnkQLw1A" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> in the <a href="https://balalab-skku.org/" rel="external nofollow noopener" target="_blank">Computational Biology and Bioinformatics Laboratory</a>. Prior to joining SKKU, I have worked as an Assistant Researcher at <a href="https://tdtu.edu.vn/en" rel="external nofollow noopener" target="_blank">Ton Duc Thang University (TDTU)</a>, Vietnam. I completed my M.E. degree in Automation and Control at TDTU, where I was supervised by Dr. <a href="https://sites.google.com/view/nguyensydzung" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen</a> and co-supervised by Dr. <a href="https://dnmduc.github.io/" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a>. I also hold a B.E. degree in Electronics and Telecommunication from TDTU, which I obtained in 2019 under the supervision of Dr. <a href="https://dnmduc.github.io/" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a>.</p> <p><strong>Research Interests:</strong> Artificial Intelligence, Computational Intelligence, Deep Learning, Machine Learning, Signal Processing, XAI &amp; Optimization, and their applications in the fields of Affective Computing, Antibody Therapeutics, Antibody Functional &amp; Structural Prediction, Audio &amp; Speech Processing, Bioinformatics, Biological Image Analysis, Computational Biology &amp; Medicine, Computer-Aided Drug Design, Medical Image Analysis, Metagenomics Analysis, Multi-Omics Analysis, NeuroAI, Neurodegenerative Diseases, Peptide Therapeutics, Protein Design, Protein Functional &amp; Structural Prediction, and Water Quality.</p> <style>.no-margin{margin:0}.custom-figure{margin-top:0}.small-figure{width:100%}.reduce-gap{margin-bottom:5}</style> <figure class="custom-figure reduce-gap"> <img src="assets/img/laboratory.gif" class="small-figure"> </figure> <p><strong>Inspirational Quotes:</strong></p> <blockquote class="no-margin reduce-gap"> <p><q><i>Your origin does not determine what kind of person you are. Only you can decide who you become.</i></q> - <a href="https://mengchih.com/about/" rel="external nofollow noopener" target="_blank">Meng Chih Chiang</a></p> <p><q><i>Life can be heavy, especially if you try to carry it all at once.</i></q> - <a href="https://www.taylorswift.com/" rel="external nofollow noopener" target="_blank">Taylor Swift</a></p> <p><q><i>Life can be seen as a balance between positive and negative aspects throughout its entirety. Therefore, it's essential to cherish every moment and embrace life fully.</i></q> - <a href="https://nhattruongpham.github.io/">Nhat Truong Pham</a></p> </blockquote> <center> $$ \begin{align*} \text{Life} &amp;= \int_{\text{birth}}^{\infty} \left(\frac{\text{Happiness} \times \text{Love} \times \text{Peace}}{\text{time}} - \frac{\text{Sadness} \times \text{Revenge} \times \text{Discord}}{\text{time}}\right) \, \text{dtime} \\ &amp;= \int_{\text{birth}}^{\rm{t_{2025}}} \left(\frac{\text{Happiness} \times \text{Love} \times \text{Peace}}{\text{time}} - \frac{\text{Sadness} \times \text{Revenge} \times \text{Discord}}{\text{time}}\right) \, \text{dtime} \\ &amp;\quad + \int_{\rm{t_{2025}}}^{\infty} \left(\frac{\text{Happiness}' \times \text{Love}' \times \text{Peace}'}{\text{time}} - \frac{\text{Sadness}' \times \text{Revenge}' \times \text{Discord}'}{\text{time}}\right) \, \text{dtime} \end{align*} $$ </center> </div> <h2><a href="/news/" style="color: inherit;">Latest News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Nov 28, 2024</th> <td> <u><i><b>Received the SKKU BK21 Innovative Research Scholarship (ÍµêÏú°Ïó∞Íµ¨Îã® Ïû•ÌïôÌòÅÏã† (SKKU BK21 ÌòÅÏã†Ïó∞Íµ¨ Ïû•ÌïôÍ∏à))</b></i></u> </td> </tr> <tr> <th scope="row">Nov 20, 2024</th> <td> One manuscript entitled <b>‚ÄúLeveraging deep transfer learning and explainable AI for accurate COVID-19 diagnosis: Insights from a multi-national chest CT scan study‚Äù</b> has been accepted for publication in the <span style="color: #FF3636;"><i>Computers in Biology and Medicine</i></span> journal </td> </tr> <tr> <th scope="row">Nov 13, 2024</th> <td> <u><i><b>Successfully completed the preliminary defense for my Ph.D. dissertation and became a Ph.D. candidate</b></i></u> </td> </tr> <tr> <th scope="row">Nov 2, 2024</th> <td> One manuscript entitled <b>‚Äú<i>MST-m6A:</i> A Novel Multi-Scale Transformer-based Framework for Accurate Prediction of m6A Modification Sites Across Diverse Cellular Contexts‚Äù</b> has been accepted for publication in the <span style="color: #FF3636;"><i>Journal of Molecular Biology</i></span> </td> </tr> <tr> <th scope="row">Oct 4, 2024</th> <td> One collaborative manuscript entitled <b>‚Äú<i>HuBERT-CLAP:</i> Contrastive Learning-Based Multimodal Emotion Recognition using Self-Alignment Approach‚Äù</b> has been accepted for publication in the <span style="color: #00ab37;"><i>MMAsia ‚Äò24: Proceedings of the 6th ACM International Conference on Multimedia in Asia</i></span> </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">Selected Publications</a></h2> <div class="publications"> <p> (‚Ä†) denotes equal contribution </p> <p> (*) denotes correspondance </p> <p> <span style="display: inline-block; width: 15px; height: 15px; background-color: #600;"></span> denotes journal </p> <p> <span style="display: inline-block; width: 15px; height: 15px; background-color: #215d42;"></span> denotes conference </p> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/XCT-COVID-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/XCT-COVID-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/XCT-COVID-1400.webp"></source> <img src="/assets/img/publication_preview/XCT-COVID.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="XCT-COVID.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pham2025leveraging" class="col-sm-8"> <div class="title">Leveraging deep transfer learning and explainable AI for accurate COVID-19 diagnosis: Insights from a multi-national chest CT scan study</div> <div class="author"> <em><b>Nhat Truong Pham</b></em>,¬†Jinsol Ko,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=W5BcBWoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Masaud Shah</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5WP7pOUAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Rajan Rakkiyappan</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=oM75QQMAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Hyun Goo Woo</a>,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/computers-in-biology-and-medicine" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Computers in Biology and Medicine</a></i></em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010482524015464" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/cbbl-skku-org/XCT-COVID/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://doi.org/10.5281/zenodo.12772023" rel="external nofollow noopener" target="_blank"> <img src="https://zenodo.org/badge/doi/10.5281/zenodo.12772023.svg" alt="DOI"> </a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.compbiomed.2024.109461"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.compbiomed.2024.109461" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The COVID-19 pandemic has emerged as a global health crisis, impacting millions worldwide. Although chest computed tomography (CT) scan images are pivotal in diagnosing COVID-19, their manual interpretation by radiologists is time-consuming and potentially subjective. Automated computer-aided diagnostic (CAD) frameworks offer efficient and objective solutions. However, machine or deep learning methods often face challenges in their reproducibility due to underlying biases and methodological flaws. To address these issues, we propose XCT-COVID, an explainable, transferable, and reproducible CAD framework based on deep transfer learning to predict COVID-19 infection from CT scan images accurately. This is the first study to develop three distinct models within a unified framework by leveraging a previously unexplored large dataset and two widely used smaller datasets. We employed five known convolutional neural network architectures, both with and without pretrained weights, on the larger dataset. We optimized hyperparameters through extensive grid search and 5-fold cross-validation (CV), significantly enhancing the model performance. Experimental results from the larger dataset showed that the VGG16 architecture (XCT-COVID-L) with pretrained weights consistently outperformed other architectures, achieving the best performance, on both 5-fold CV and independent test. When evaluated with the external datasets, XCT-COVID-L performed well with data with similar distributions, demonstrating its transferability. However, its performance significantly decreased on smaller datasets with lower-quality images. To address this, we developed other models, XCT-COVID-S1 and XCT-COVID-S2, specifically for the smaller datasets, outperforming existing methods. Moreover, eXplainable Artificial Intelligence (XAI) analyses were employed to interpret the models‚Äô functionalities. For prediction and reproducibility purposes, the implementation of XCT-COVID is publicly accessible at <a href="https://github.com/cbbl-skku-org/XCT-COVID/" rel="external nofollow noopener" target="_blank">https://github.com/cbbl-skku-org/XCT-COVID/</a>.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2025leveraging</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Leveraging deep transfer learning and explainable AI for accurate COVID-19 diagnosis: Insights from a multi-national chest CT scan study}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Nhat Truong and Ko, Jinsol and Shah, Masaud and Rakkiyappan, Rajan and Woo, Hyun Goo and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computers in Biology and Medicine}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{185}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{109461}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.compbiomed.2024.109461}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Mol2Lang-VLM-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Mol2Lang-VLM-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Mol2Lang-VLM-1400.webp"></source> <img src="/assets/img/publication_preview/Mol2Lang-VLM.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Mol2Lang-VLM.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tran2024mollangvlm" class="col-sm-8"> <div class="title"> <b><i>Mol2Lang-VLM:</i></b> Vision- and Text-Guided Generative Pre-trained Language Models for Advancing Molecule Captioning through Multimodal Fusion</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=kz_chQ4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Duong Thanh Tran<sup>(‚Ä†)</sup></a>,¬†<em><b>Nhat Truong Pham<sup>(‚Ä†)</sup></b></em>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=-aEoZCgAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Nguyen Doan Hieu Nguyen</a>,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em>In <i><a href="https://language-plus-molecules.github.io/" style="color: #00ab37; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Proceedings of the 1st Workshop on Language + Molecules (L+M 2024)</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2024.langmol-1.12/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://aclanthology.org/2024.langmol-1.12.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">PDF</a> <a href="https://github.com/nhattruongpham/mol-lang-bridge/tree/mol2lang" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.18653/v1/2024.langmol-1.12"></span> <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/2024.langmol-1.12" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>This paper introduces Mol2Lang-VLM, an enhanced method for refining generative pre-trained language models for molecule captioning using multimodal features to achieve more accurate caption generation. Our approach leverages the encoder and decoder blocks of the Transformer-based architecture by introducing third sub-layers into both. Specifically, we insert sub-layers in the encoder to fuse features from SELFIES strings and molecular images, while the decoder fuses features from SMILES strings and their corresponding descriptions. Moreover, cross multi-head attention is employed instead of common multi-head attention to enable the decoder to attend to the encoder‚Äôs output, thereby integrating the encoded contextual information for better and more accurate caption generation. Performance evaluation on the CheBI-20 and L+M-24 benchmark datasets demonstrates Mol2Lang-VLM‚Äôs superiority, achieving higher accuracy and quality in caption generation compared to existing methods. Our code and pre-processed data are available at <a href="https://github.com/nhattruongpham/mol-lang-bridge/tree/mol2lang" rel="external nofollow noopener" target="_blank">https://github.com/nhattruongpham/mol-lang-bridge/tree/mol2lang/</a>.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tran2024mollangvlm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mol2Lang-{VLM}: Vision- and Text-Guided Generative Pre-trained Language Models for Advancing Molecule Captioning through Multimodal Fusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tran, Duong Thanh and Pham, Nhat Truong and Nguyen, Nguyen Doan Hieu and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 1st Workshop on Language + Molecules L+M 2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{97--102}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2024.langmol-1.12}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Lang2Mol-Diff-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Lang2Mol-Diff-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Lang2Mol-Diff-1400.webp"></source> <img src="/assets/img/publication_preview/Lang2Mol-Diff.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Lang2Mol-Diff.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="nguyen2024langmoldiff" class="col-sm-8"> <div class="title"> <b><i>Lang2Mol-Diff:</i></b> A Diffusion-Based Generative Model for Language-to-Molecule Translation Leveraging SELFIES Molecular String Representation</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=-aEoZCgAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Nguyen Doan Hieu Nguyen<sup>(‚Ä†)</sup></a>,¬†<em><b>Nhat Truong Pham<sup>(‚Ä†)</sup></b></em>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=kz_chQ4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Duong Thanh Tran</a>,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em>In <i><a href="https://language-plus-molecules.github.io/" style="color: #00ab37; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Proceedings of the 1st Workshop on Language + Molecules (L+M 2024)</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2024.langmol-1.15/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://aclanthology.org/2024.langmol-1.15.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">PDF</a> <a href="https://github.com/nhattruongpham/mol-lang-bridge/tree/lang2mol" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.18653/v1/2024.langmol-1.15"></span> <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/2024.langmol-1.15" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Generating <i>de novo</i> molecules from textual descriptions is challenging due to potential issues with molecule validity in SMILES representation and limitations of autoregressive models. This work introduces Lang2Mol-Diff, a diffusion-based language-to-molecule generative model using the SELFIES representation. Specifically, Lang2Mol-Diff leverages the strengths of two state-of-the-art molecular generative models: BioT5 and TGM-DLM. By employing BioT5 to tokenize the SELFIES representation, Lang2Mol-Diff addresses the validity issues associated with SMILES strings. Additionally, it incorporates a text diffusion mechanism from TGM-DLM to overcome the limitations of autoregressive models in this domain. To the best of our knowledge, this is the first study to leverage the diffusion mechanism for text-based <i>de novo</i> molecule generation using the SELFIES molecular string representation. Performance evaluation on the L+M-24 benchmark dataset shows that Lang2Mol-Diff outperforms all existing methods for molecule generation in terms of validity. Our code and pre-processed data are available at <a href="https://github.com/nhattruongpham/mol-lang-bridge/tree/lang2mol" rel="external nofollow noopener" target="_blank">https://github.com/nhattruongpham/mol-lang-bridge/tree/lang2mol/</a>.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nguyen2024langmoldiff</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Lang2Mol-Diff: A Diffusion-Based Generative Model for Language-to-Molecule Translation Leveraging SELFIES Molecular String Representation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Nguyen Doan Hieu and Pham, Nhat Truong and Tran, Duong Thanh and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 1st Workshop on Language + Molecules L+M 2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{128--134}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2024.langmol-1.15}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/HOTGpred-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/HOTGpred-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/HOTGpred-1400.webp"></source> <img src="/assets/img/publication_preview/HOTGpred.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="HOTGpred.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pham2024hotgpred" class="col-sm-8"> <div class="title"> <b><i>HOTGpred:</i></b> Enhancing human O-linked threonine glycosylation prediction using integrated pretrained protein language model-based features and multi-stage feature selection approach</div> <div class="author"> <em><b>Nhat Truong Pham<sup>(‚Ä†)</sup></b></em>,¬†Ying Zhang<sup>(‚Ä†)</sup>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5WP7pOUAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Rajan Rakkiyappan</a>,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/computers-in-biology-and-medicine" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Computers in Biology and Medicine</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010482524009442" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://balalab-skku.org/HOTGpred/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.compbiomed.2024.108859"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.compbiomed.2024.108859" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>O-linked glycosylation is a complex post-translational modification (PTM) in human proteins that plays a critical role in regulating various cellular metabolic and signaling pathways. In contrast to N-linked glycosylation, O-linked glycosylation lacks specific sequence features and maintains an unstable core structure. Identifying O-linked threonine glycosylation sites (OTGs) remains challenging, requiring extensive experimental tests. While bioinformatics tools have emerged for predicting OTGs, their reliance on limited conventional features and absence of well-defined feature selection strategies limit their effectiveness. To address these limitations, we introduced HOTGpred (Human O-linked Threonine Glycosylation predictor), employing a multi-stage feature selection process to identify the optimal feature set for accurately identifying OTGs. Initially, we assessed 25 different feature sets derived from various pretrained protein language model (PLM)-based embeddings and conventional feature descriptors using nine classifiers. Subsequently, we integrated the top five embeddings linearly and determined the most effective scoring function for ranking hybrid features, identifying the optimal feature set through a process of sequential forward search. Among the classifiers, the extreme gradient boosting (XGBT)-based model, using the optimal feature set (HOTGpred), achieved 92.03% accuracy on the training dataset and 88.25% on the balanced independent dataset. Notably, HOTGpred significantly outperformed the current state-of-the-art methods on both the balanced and imbalanced independent datasets, demonstrating its superior prediction capabilities. Additionally, SHapley Additive exPlanations (SHAP) and ablation analyses were conducted to identify the features contributing most significantly to HOTGpred. Finally, we developed an easy-to-navigate web server, accessible at <a href="https://balalab-skku.org/HOTGpred/" rel="external nofollow noopener" target="_blank">https://balalab-skku.org/HOTGpred/</a>, to support glycobiologists in their research on glycosylation structure and function.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2024hotgpred</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HOTGpred: Enhancing human O-linked threonine glycosylation prediction using integrated pretrained protein language model-based features and multi-stage feature selection approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Nhat Truong and Zhang, Ying and Rakkiyappan, Rajan and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computers in Biology and Medicine}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{179}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{108859}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.compbiomed.2024.108859}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mACPpred2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mACPpred2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mACPpred2-1400.webp"></source> <img src="/assets/img/publication_preview/mACPpred2.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="mACPpred2.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sangaraju2024macppred" class="col-sm-8"> <div class="title"> <b><i>mACPpred 2.0:</i></b> Stacked Deep Learning for Anticancer Peptide Prediction with Integrated Spatial and Probabilistic Feature Representations</div> <div class="author"> Vinoth Kumar Sangaraju<sup>(‚Ä†)</sup>,¬†<em><b>Nhat Truong Pham<sup>(‚Ä†)</sup></b></em>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0EAV03MAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Leyi Wei</a>,¬†Xue Yu,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/journal-of-molecular-biology" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Journal of Molecular Biology</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0022283624002894" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/nhattruongpham/mACPpred2/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://balalab-skku.org/mACPpred2/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://doi.org/10.5281/zenodo.11350064" rel="external nofollow noopener" target="_blank"> <img src="https://zenodo.org/badge/doi/10.5281/zenodo.11350064.svg" alt="DOI"> </a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.jmb.2024.168687"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.jmb.2024.168687" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Anticancer peptides (ACPs), naturally occurring molecules with remarkable potential to target and kill cancer cells. However, identifying ACPs based solely from their primary amino acid sequences remains a major hurdle in immunoinformatics. In the past, several web-based machine learning (ML) tools have been proposed to assist researchers in identifying potential ACPs for further testing. Notably, our meta- approach method, mACPpred, introduced in 2019, has significantly advanced the field of ACP research. Given the exponential growth in the number of characterized ACPs, there is now a pressing need to create an updated version of mACPpred. To develop mACPpred 2.0, we constructed an up-to-date benchmarking dataset by integrating all publicly available ACP datasets. We employed a large-scale of feature descriptors, encompassing both conventional feature descriptors and advanced pre-trained natural language processing (NLP)-based embeddings. We evaluated their ability to discriminate between ACPs and non-ACPs using eleven different classifiers. Subsequently, we employed a stacked deep learning (SDL) approach, incorporating 1D convolutional neural network (1D CNN) blocks and hybrid features. These features included the top seven performing NLP-based features and 90 probabilistic features, allowing us to identify hidden patterns within these diverse features and improve the accuracy of our ACP prediction model. This is the first study to integrate spatial and probabilistic feature representations for predicting ACPs. Rigorous cross-validation and independent tests conclusively demonstrated that mACPpred 2.0 not only surpassed its predecessor (mACPpred) but also outperformed the existing state-of-the-art predictors, highlighting the importance of advanced feature representation capabilities attained through SDL. To facilitate widespread use and accessibility, we have developed a user-friendly for mACPpred 2.0, available at <a href="https://balalab-skku.org/mACPpred2/" rel="external nofollow noopener" target="_blank">https://balalab-skku.org/mACPpred2/</a>.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sangaraju2024macppred</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{mACPpred 2.0: Stacked Deep Learning for Anticancer Peptide Prediction with Integrated Spatial and Probabilistic Feature Representations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sangaraju, Vinoth Kumar and Pham, Nhat Truong and Wei, Leyi and Yu, Xue and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Molecular Biology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{436}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{168687}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.jmb.2024.168687}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ac4C-AFL-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ac4C-AFL-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ac4C-AFL-1400.webp"></source> <img src="/assets/img/publication_preview/ac4C-AFL.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ac4C-AFL.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pham2024ac4c" class="col-sm-8"> <div class="title"> <b><i>ac4C-AFL:</i></b> A high-precision identification of human mRNA N4-acetylcytidine sites based on adaptive feature representation learning</div> <div class="author"> <em><b>Nhat Truong Pham</b></em>,¬†Annie Terrina Terrance,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=DkVnJ-wAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Young-Jun Jeon</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5WP7pOUAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Rajan Rakkiyappan</a>,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><i><a href="https://www.cell.com/molecular-therapy-family/nucleic-acids/home" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Molecular Therapy-Nucleic Acids</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.cell.com/molecular-therapy-family/nucleic-acids/fulltext/S2162-2531(24)00079-9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/ac4C-AFL.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a> <a href="https://balalab-skku.org/ac4C-AFL/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.omtn.2024.102192"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.omtn.2024.102192" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>RNA N4-acetylcytidine (ac4C) is a highly conserved RNA modification that plays a crucial role in controlling mRNA stability, processing, and translation. Consequently, accurate identification of ac4C sites across the genome is critical for understanding gene expression regulation mechanisms. In this study, we have developed ac4C-AFL, a bioinformatics tool that precisely identifies ac4C sites from primary RNA sequences. In ac4C-AFL, we identified the optimal sequence length for model building and implemented an adaptive feature representation strategy that is capable of extracting the most representative features from RNA. To identify the most relevant features, we proposed a novel ensemble feature importance scoring strategy to rank features effectively. We then used this information to conduct the sequential forward search, which individually determine the optimal feature set from the 16 sequence-derived feature descriptors. Utilizing these optimal feature descriptors, we constructed 176 baseline models using 11 popular classifiers. The most efficient baseline models were identified using the two-step feature selection approach, whose predicted scores were integrated and trained with the appropriate classifier to develop the final prediction model. Our rigorous cross-validations and independent tests demonstrate that ac4C-AFL surpasses contemporary tools in predicting ac4C sites. Moreover, we have developed a publicly accessible web server at <a href="https://balalab-skku.org/ac4C-AFL/" rel="external nofollow noopener" target="_blank">https://balalab-skku.org/ac4C-AFL/</a>.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2024ac4c</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ac4C-AFL: A high-precision identification of human mRNA N4-acetylcytidine sites based on adaptive feature representation learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Nhat Truong and Terrance, Annie Terrina and Jeon, Young-Jun and Rakkiyappan, Rajan and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Molecular Therapy-Nucleic Acids}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{102192}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.omtn.2024.102192}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/H2Opred-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/H2Opred-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/H2Opred-1400.webp"></source> <img src="/assets/img/publication_preview/H2Opred.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="H2Opred.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pham2024h2opred" class="col-sm-8"> <div class="title"> <b><i>H2Opred:</i></b> a robust and efficient hybrid deep learning model for predicting 2‚Äô-O-methylation sites in human RNA</div> <div class="author"> <em><b>Nhat Truong Pham</b></em>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5WP7pOUAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Rajan Rakkiyappan</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=_YZClBgAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Jongsun Park</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=IRshBmMAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Adeel Malik</a>,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><i><a href="https://academic.oup.com/bib" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Briefings in Bioinformatics</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://academic.oup.com/bib/article/25/1/bbad476/7510980" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/H2Opred.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a> <a href="https://balalab-skku.org/H2Opred/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://www.ibric.org/s.do?CWXEPvOoln" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1093/bib/bbad476"></span> <span class="__dimensions_badge_embed__" data-doi="10.1093/bib/bbad476" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>2‚Äô-O-methylation (2OM) is the most common post-transcriptional modification of RNA. It plays a crucial role in RNA splicing, RNA stability and innate immunity. Despite advances in high-throughput detection, the chemical stability of 2OM makes it difficult to detect and map in messenger RNA. Therefore, bioinformatics tools have been developed using machine learning (ML) algorithms to identify 2OM sites. These tools have made significant progress, but their performances remain unsatisfactory and need further improvement. In this study, we introduced H2Opred, a novel hybrid deep learning (HDL) model for accurately identifying 2OM sites in human RNA. Notably, this is the first application of HDL in developing four nucleotide-specific models [adenine (A2OM), cytosine (C2OM), guanine (G2OM) and uracil (U2OM)] as well as a generic model (N2OM). H2Opred incorporated both stacked 1D convolutional neural network (1D-CNN) blocks and stacked attention-based bidirectional gated recurrent unit (Bi-GRU-Att) blocks. 1D-CNN blocks learned effective feature representations from 14 conventional descriptors, while Bi-GRU-Att blocks learned feature representations from five natural language processing-based embeddings extracted from RNA sequences. H2Opred integrated these feature representations to make the final prediction. Rigorous cross-validation analysis demonstrated that H2Opred consistently outperforms conventional ML-based single-feature models on five different datasets. Moreover, the generic model of H2Opred demonstrated a remarkable performance on both training and testing datasets, significantly outperforming the existing predictor and other four nucleotide-specific H2Opred models. To enhance accessibility and usability, we have deployed a user-friendly web server for H2Opred, accessible at <a href="https://balalab-skku.org/H2Opred/" rel="external nofollow noopener" target="_blank">https://balalab-skku.org/H2Opred/</a>. This platform will serve as an invaluable tool for accurately predicting 2OM sites within human RNA, thereby facilitating broader applications in relevant research endeavors.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2024h2opred</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{H2Opred: a robust and efficient hybrid deep learning model for predicting 2'-O-methylation sites in human RNA}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Nhat Truong and Rakkiyappan, Rajan and Park, Jongsun and Malik, Adeel and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Briefings in Bioinformatics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{25}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{bbad476}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{OxfOxford University Pressord}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1093/bib/bbad476}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/MeL-STPhos-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/MeL-STPhos-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/MeL-STPhos-1400.webp"></source> <img src="/assets/img/publication_preview/MeL-STPhos.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="MeL-STPhos.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pham2024advancing" class="col-sm-8"> <div class="title">Advancing the accuracy of SARS-CoV-2 phosphorylation site detection via meta-learning approach</div> <div class="author"> <em><b>Nhat Truong Pham<sup>(‚Ä†)</sup></b></em>,¬†Le Thi Phan<sup>(‚Ä†)</sup>,¬†Jimin Seo,¬†Yeonwoo Kim,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=sBSJfo4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Minkyung Song</a>,¬†Sukchan Lee,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=DkVnJ-wAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Young-Jun Jeon</a>,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><i><a href="https://academic.oup.com/bib" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Briefings in Bioinformatics</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://academic.oup.com/bib/article/25/1/bbad433/7459584" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/MeL-STPhos.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a> <a href="https://balalab-skku.org/MeL-STPhos/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://www.ibric.org/s.do?nLdsctWFBW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a> <a href="https://www.ibric.org/s.do?ovlRZSpKQk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Interview</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1093/bib/bbad433"></span> <span class="__dimensions_badge_embed__" data-doi="10.1093/bib/bbad433" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The worldwide appearance of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has generated significant concern and posed a considerable challenge to global health. Phosphorylation is a common post-translational modification that affects many vital cellular functions and is closely associated with SARS-CoV-2 infection. Precise identification of phosphorylation sites could provide more in-depth insight into the processes underlying SARS-CoV-2 infection and help alleviate the continuing coronavirus disease 2019 (COVID-19) crisis. Currently, available computational tools for predicting these sites lack accuracy and effectiveness. In this study, we designed an innovative meta-learning model, Meta-Learning for Serine/Threonine Phosphorylation (MeL-STPhos), to precisely identify protein phosphorylation sites. We initially performed a comprehensive assessment of 29 unique sequence-derived features, establishing prediction models for each using 14 renowned machine learning methods, ranging from traditional classifiers to advanced deep learning algorithms. We then selected the most effective model for each feature by integrating the predicted values. Rigorous feature selection strategies were employed to identify the optimal base models and classifier(s) for each cell-specific dataset. To the best of our knowledge, this is the first study to report two cell-specific models and a generic model for phosphorylation site prediction by utilizing an extensive range of sequence-derived features and machine learning algorithms. Extensive cross-validation and independent testing revealed that MeL-STPhos surpasses existing state-of-the-art tools for phosphorylation site prediction. We also developed a publicly accessible platform at <a href="https://balalab-skku.org/MeL-STPhos/" rel="external nofollow noopener" target="_blank">https://balalab-skku.org/MeL-STPhos/</a>. We believe that MeL-STPhos will serve as a valuable tool for accelerating the discovery of serine/threonine phosphorylation sites and elucidating their role in post-translational regulation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2024advancing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Advancing the accuracy of SARS-CoV-2 phosphorylation site detection via meta-learning approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Nhat Truong and Phan, Le Thi and Seo, Jimin and Kim, Yeonwoo and Song, Minkyung and Lee, Sukchan and Jeon, Young-Jun and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Briefings in Bioinformatics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{25}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{bbad433}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{OxfOxford University Pressord}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1093/bib/bbad433}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/AAD-Net-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/AAD-Net-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/AAD-Net-1400.webp"></source> <img src="/assets/img/publication_preview/AAD-Net.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="AAD-Net.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mustaqeem2023aad" class="col-sm-8"> <div class="title"> <b><i>AAD-Net:</i></b> Advanced end-to-end signal processing system for human emotion detection &amp; recognition using attention-based deep echo state network</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=uEZhRWAAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Mustaqeem Khan</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=VcOjgngAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Abdulmotaleb El Saddik</a>,¬†Fahd Saleh Alotaibi,¬†and¬†<em><b>Nhat Truong Pham</b></em> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/knowledge-based-systems" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Knowledge-Based Systems</a></i></em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705123002757" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.knosys.2023.110525"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.knosys.2023.110525" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Speech signals are the most convenient way of communication between human beings and the eventual method of Human-Computer Interaction (HCI) to exchange emotions and information. Recognizing emotions from speech signals is a challenging task due to the sparse nature of emotional data and features. In this article, we proposed a Deep Echo-State-Network (DeepESN) system for emotion recognition with a dilated convolution neural network and multi-headed attention mechanism. To reduce the model complexity, we incorporate a DeepESN that combines reservoir computing for higher-dimensional mapping. We also used fine-tuned Sparse Random Projection (SRP) to reduce dimensionality and adopted an early fusion strategy to fuse the extracted cues and passed the joint feature vector via a classification layer to recognize emotions. Our proposed model is evaluated on two public speech corpora, EMO-DB and RAVDESS, and tested for subject/speaker-dependent/independent performance. The results show that our proposed system achieves a high recognition rate, 91.14, 85.57 for EMO-DB, and 82.01, 77.02 for RAVDESS, using speaker-dependent and independent experiments, respectively. Our proposed system outperforms the State-of-The-Art (SOTA) while requiring less computational time.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mustaqeem2023aad</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AAD-Net: Advanced end-to-end signal processing system for human emotion detection \ recognition using attention-based deep echo state network}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khan, Mustaqeem and El Saddik, Abdulmotaleb and Alotaibi, Fahd Saleh and Pham, Nhat Truong}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Knowledge-Based Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{270}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{110525}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.knosys.2023.110525}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/HDA_mADCRNN-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/HDA_mADCRNN-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/HDA_mADCRNN-1400.webp"></source> <img src="/assets/img/publication_preview/HDA_mADCRNN.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="HDA_mADCRNN.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pham2023hybrid" class="col-sm-8"> <div class="title">Hybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition</div> <div class="author"> <em><b>Nhat Truong Pham</b></em>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=2UKP440AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=At-Y-H8AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Ngoc Duy Nguyen</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=xr39SOwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Thanh Thi Nguyen</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5b9ncWoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Hai Nguyen</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=an_4IJkAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Chee Peng Lim</a>,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=XFUxOkoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen</a> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/expert-systems-with-applications" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Expert Systems with Applications</a></i></em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0957417423011107" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/HDA_mADCRNN.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a> <a href="https://github.com/nhattruongpham/hda-adcrnn-ser" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.eswa.2023.120608"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.eswa.2023.120608" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Recently, speech emotion recognition (SER) has become an active research area in speech processing, particularly with the advent of deep learning (DL). Numerous DL-based methods have been proposed for SER. However, most of the existing DL-based models are complex and require a large amounts of data to achieve a good performance. In this study, a new framework of deep attention-based dilated convolutional-recurrent neural networks coupled with a hybrid data augmentation method was proposed for addressing SER tasks. The hybrid data augmentation method constitutes an upsampling technique for generating more speech data samples based on the traditional and generative adversarial network approaches. By leveraging both convolutional and recurrent neural networks in a dilated form along with an attention mechanism, the proposed DL framework can extract high-level representations from three-dimensional log Mel spectrogram features. Dilated convolutional neural networks acquire larger receptive fields, whereas dilated recurrent neural networks overcome complex dependencies as well as the vanishing and exploding gradient issues. Furthermore, the loss functions are reconfigured by combining the SoftMax loss and the center-based losses to classify various emotional states. The proposed framework was implemented using the Python programming language and the TensorFlow deep learning library. To validate the proposed framework, the EmoDB and ERC benchmark datasets, which are imbalanced and/or small datasets, were employed. The experimental results indicate that the proposed framework outperforms other related state-of-the-art methods, yielding the highest unweighted recall rates of 88.03 ¬± 1.39 (%) and 66.56 ¬± 0.67 (%) for the EmoDB and ERC datasets, respectively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2023hybrid</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Nhat Truong and Dang, Duc Ngoc Minh and Nguyen, Ngoc Duy and Nguyen, Thanh Thi and Nguyen, Hai and Manavalan, Balachandran and Lim, Chee Peng and Nguyen, Sy Dzung}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Expert Systems with Applications}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{120608}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.eswa.2023.120608}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Fruit-CoV-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Fruit-CoV-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Fruit-CoV-1400.webp"></source> <img src="/assets/img/publication_preview/Fruit-CoV.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Fruit-CoV.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="nguyen2023fruit" class="col-sm-8"> <div class="title"> <b><i>Fruit-CoV:</i></b> An efficient vision-based framework for speedy detection and diagnosis of SARS-CoV-2 infections through recorded cough sounds</div> <div class="author"> Long H Nguyen<sup>(‚Ä†)</sup>,¬†<em><b>Nhat Truong Pham<sup>(‚Ä†)(*)</sup></b></em>,¬†Van Huong Do,¬†Liu Tai Nguyen,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=zSAfD80AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Thanh Tin Nguyen</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5b9ncWoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Hai Nguyen</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=At-Y-H8AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Ngoc Duy Nguyen</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=xr39SOwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Thanh Thi Nguyen</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=XFUxOkoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen</a>,¬†Asim Bhatti,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=an_4IJkAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Chee Peng Lim</a> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/expert-systems-with-applications" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Expert Systems with Applications</a></i></em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0957417422022308" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/nhattruongpham/Fruit-CoV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.eswa.2022.119212"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.eswa.2022.119212" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>COVID-19 is an infectious disease caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). This deadly virus has spread worldwide, leading to a global pandemic since March 2020. A recent variant of SARS-CoV-2 named Delta is intractably contagious and responsible for more than four million deaths globally. Therefore, developing an efficient self-testing service for SARS-CoV-2 at home is vital. In this study, a two-stage vision-based framework, namely Fruit-CoV, is introduced for detecting SARS-CoV-2 infections through recorded cough sounds. Specifically, audio signals are converted into Log-Mel spectrograms, and the EfficientNet-V2 network is used to extract their visual features in the first stage. In the second stage, 14 convolutional layers extracted from the large-scale Pretrained Audio Neural Networks for audio pattern recognition (PANNs) and the Wavegram-Log-Mel-CNN are employed to aggregate feature representations of the Log-Mel spectrograms and the waveform. Finally, the combined features are used to train a binary classifier. In this study, a dataset provided by the AICovidVN 115M Challenge is employed for evaluation. It includes 7,371 recorded cough sounds collected throughout Vietnam, India, and Switzerland. Experimental results indicate that the proposed model achieves an Area Under the Receiver Operating Characteristic Curve (AUC) score of 92.8% and ranks first on the final leaderboard of the AICovidVN 115M Challenge. Our code is publicly available.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">nguyen2023fruit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fruit-CoV: An efficient vision-based framework for speedy detection and diagnosis of SARS-CoV-2 infections through recorded cough sounds}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Long H and Pham, Nhat Truong and Do, Van Huong and Nguyen, Liu Tai and Nguyen, Thanh Tin and Nguyen, Hai and Nguyen, Ngoc Duy and Nguyen, Thanh Thi and Nguyen, Sy Dzung and Bhatti, Asim and Lim, Chee Peng}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Expert Systems with Applications}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{213}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{119212}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.eswa.2022.119212}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mVina-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mVina-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mVina-1400.webp"></source> <img src="/assets/img/publication_preview/mVina.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="mVina.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pham2022improving" class="col-sm-8"> <div class="title">Improving ligand-ranking of AutoDock Vina by changing the empirical parameters</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=_aQ5P4gAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">T Ngoc Han Pham</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=a4xyHScAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Trung Hai Nguyen</a>,¬†Nguyen Minh Tam,¬†Thien Y. Vu,¬†<em><b>Nhat Truong Pham</b></em>,¬†Nguyen Truong Huy,¬†Binh Khanh Mai,¬†Nguyen Thanh Tung,¬†Minh Quan Pham,¬†Van V. Vu,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=_VxSvQkAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Son Tung Ngo</a> </div> <div class="periodical"> <em><i><a href="https://onlinelibrary.wiley.com/journal/1096987x" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Journal of Computational Chemistry</a></i></em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.26779" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/nhattruongpham/mvina" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1002/jcc.26779"></span> <span class="__dimensions_badge_embed__" data-doi="10.1002/jcc.26779" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>AutoDock Vina (Vina) achieved a very high docking-success rate, <i>pÃÇ</i>, but give a rather low correlation coefficient, <i>R</i>, for binding affinity with respect to experiments. This low correlation can be an obstacle for ranking of ligand-binding affinity, which is the main objective of docking simulations. In this context, we evaluated the dependence of Vina <i>R</i> coefficient upon its empirical parameters. <i>R</i> is affected more by changing the gauss2 and rotation than other terms. The docking-success rate <i>pÃÇ</i> is sensitive to the alterations of the gauss1, gauss2, repulsion, and hydrogen bond parameters. Based on our benchmarks, the parameter set1 has been suggested to be the most optimal. The testing study over 800 complexes indicated that the modified Vina provided higher correlation with experiment <i>R<sub>set1</sub>=0.556¬±0.025</i> compared with <i>R<sub>Default</sub>=0.493¬±0.028</i> obtained by the original Vina and <i>R<sub>Vina 1.2</sub>=0.503¬±0.029</i> by Vina version 1.2. Besides, the modified Vina can be also applied more widely, giving <i>R ‚â• 0.500</i> for 32/48 targets, compared with the default package, giving <i>R ‚â• 0.500</i> for 31/48 targets. In addition, validation calculations for 1036 complexes obtained from version 2019 of PDBbind refined structures showed that the set1 of parameters gave higher correlation coefficient (<i>R<sub>set1</sub>=0.617¬±0.017</i>) than the default package (<i>R<sub>Default</sub>=0.543¬±0.020</i>) and Vina version 1.2 (<i>R<sub>Vina 1.2</sub>=0.540¬±0.020</i>). The version of Vina with set1 of parameters can be downloaded at <a href="https://github.com/sontungngo/mvina/" rel="external nofollow noopener" target="_blank">https://github.com/sontungngo/mvina/</a>. The outcomes would enhance the ranking of ligand-binding affinity using Autodock Vina.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2022improving</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improving ligand-ranking of AutoDock Vina by changing the empirical parameters}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, T Ngoc Han and Nguyen, Trung Hai and Tam, Nguyen Minh and Y. Vu, Thien and Pham, Nhat Truong and Huy, Nguyen Truong and Mai, Binh Khanh and Tung, Nguyen Thanh and Pham, Minh Quan and V. Vu, Van and Ngo, Son Tung}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Computational Chemistry}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{43}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{160--169}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Wiley Online Library}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1002/jcc.26779}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/fRiskC-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/fRiskC-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/fRiskC-1400.webp"></source> <img src="/assets/img/publication_preview/fRiskC.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="fRiskC.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="nguyen2022determination" class="col-sm-8"> <div class="title">Determination of the optimal number of clusters: a fuzzy-set based method</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=XFUxOkoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen</a>,¬†Vu Song Thuy Nguyen,¬†and¬†<em><b>Nhat Truong Pham</b></em> </div> <div class="periodical"> <em><i><a href="https://cis.ieee.org/publications/t-fuzzy-systems" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">IEEE Transactions on Fuzzy Systems</a></i></em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/9562269" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/TFUZZ.2021.3118113"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/TFUZZ.2021.3118113" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The optimal number of clusters (<i>C<sub>opt</sub></i>) is one of the determinants of clustering efficiency. In this article, we present a new method of quantifying <i>C<sub>opt</sub></i> for centroid-based clustering. First, we propose a new clustering validity index named fRisk(<i>C</i>) based on the fuzzy set theory. It takes the role of normalization and accumulation of local risks coming from each action either splitting data from a cluster or merging data into a cluster. fRisk(<i>C</i>) exploits the local distribution information of the database to catch the global information of the clustering process in the form of the risk degree. Based on the monotonous reduction property of fRisk(<i>C</i>), which is proved theoretically, we present a fRisk-based new algorithm named fRisk4-bA for determining <i>C<sub>opt</sub></i>. In the algorithm, the well-known L-method is employed as a supplemented tool to catch <i>C<sub>opt</sub></i> on the graph of the fRisk(<i>C</i>). Along with the stable convergence trend of the method to be proved theoretically, numerical surveys are also carried out. The surveys show that the high reliability and stability, as well as the sensitivity in separating/merging clusters in high-density areas, even if the presence of noise in the databases, are the strong points of the proposed method.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">nguyen2022determination</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Determination of the optimal number of clusters: a fuzzy-set based method}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Sy Dzung and Nguyen, Vu Song Thuy and Pham, Nhat Truong}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Fuzzy Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3514--3526}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TFUZZ.2021.3118113}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%70%68%61%6D%6E%68%61%74%74%72%75%6F%6E%67.%73%6B%79%6F@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-8086-6722" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://www.webofscience.com/wos/author/record/ABF-7566-2021" title="Clarivate" rel="external nofollow noopener" target="_blank"><i class="ai ai-clarivate"></i></a> <a href="https://www.scopus.com/authid/detail.uri?authorId=57243980400" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus"></i></a> <a href="https://scholar.google.com/citations?user=HybH2XkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Nhat-Truong-Pham/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://dblp.org/pid/290/9204.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a href="https://www.semanticscholar.org/author/2077529712" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://www.linkedin.com/in/nhattruongpham" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/nhattruong_pham" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a href="https://github.com/nhattruongpham" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.kaggle.com/nhattruongpham" title="Kaggle" rel="external nofollow noopener" target="_blank"><i class="fab fa-kaggle"></i></a> <a href="https://www.hackerrank.com/profile/nhattruong_pham" title="HackerRank" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-hackerrank"></i></a> <a href="https://discord.com/users/673453370991837196" title="Discord" rel="external nofollow noopener" target="_blank"><i class="fab fa-discord"></i></a> </div> <div class="contact-note"> </div> </div> <div style="width: 150px; margin: 0 auto"> <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=RuxfKWte4G6yKP1jx6GjTKLxpqdoMdFm-CPzvceTJNY"></script> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2023 - 2024 Nhat Truong Pham. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-ND 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nd.svg?ref=chooser-v1" alt=""></a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js"></script> <script>addBackToTop();</script> </body> </html>