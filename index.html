<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Nhat Truong Pham </title> <meta name="author" content="Nhat Truong Pham"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?1e5a7fb652b1868ab49bdd3c8fe588a6"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nhattruongpham.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">Home <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/collaborators/">Collaborators </a> </li> <li class="nav-item "> <a class="nav-link" href="/mentees/">Mentees </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Tools </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/molecules/">Molecules</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/dna/">DNA</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rna/">RNA</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/peptide/">Peptide</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/protein/">Protein</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/microbiome/">Microbiome</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/imaging/">Imaging</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Gallery </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/skkurs24/">SKKU-RS-24</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/ijcai24/">IJCAI-24</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/acl24/">ACL-24</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Nhat Truong</span> Pham </h1> <p class="desc">Ph.D. Candidate in Integrative Biotechnology specializing in üß¨Applied Machine Learning/Deep Learning for Computational Biology and Bioinformaticsüß¨</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile_since_2024.png" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/profile_since_2024.png?e213cb3176c3926e8e8e66f33fcf1b23" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="profile_since_2024.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>Room 62105, Biotechnology and Bioengineering Building 2, Seobu-ro, Jangan-gu, Suwon-si, Gyeonggi-do, 16419, Republic of Korea</p> </div> </div> <div class="clearfix"> <p><strong>Short Bio:</strong> My name is <i>Ph·∫°m Nh·∫≠t Tr∆∞·ªùng</i> - Vietnamese. I am currently a <del>first</del>/second-year Ph.D. <del>Student</del>/Candidate at the <a href="https://skb.skku.edu/eng_gene/index.do" rel="external nofollow noopener" target="_blank">Department of Integrative Biotechnology</a>, <a href="https://biotech.skku.edu/eng_biotech/index.do" rel="external nofollow noopener" target="_blank">College of Biotechnology and Bioengineering</a>, <a href="https://www.skku.edu/eng/" rel="external nofollow noopener" target="_blank">Sungkyunkwan University (SKKU)</a>, Republic of Korea. I am working under the guidance of Professor <a href="https://skb.skku.edu/eng_gene/faculty.do?mode=view&amp;perId=LZStrB4DgrAqgzgNgwgGQCwGkAuArAigEQE4BCA5gHYBmMAnkQLw1A" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> in the <a href="https://balalab-skku.org/" rel="external nofollow noopener" target="_blank">Computational Biology and Bioinformatics Laboratory</a>. Prior to joining SKKU, I have worked as an Assistant Researcher at <a href="https://tdtu.edu.vn/en" rel="external nofollow noopener" target="_blank">Ton Duc Thang University (TDTU)</a>, Vietnam. I completed my M.E. degree in Automation and Control at TDTU, where I was supervised by Dr. <a href="https://sites.google.com/view/nguyensydzung" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen</a> and co-supervised by Dr. <a href="https://dnmduc.github.io/" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a>. I also hold a B.E. degree in Electronics and Telecommunication from TDTU, which I obtained in 2019 under the supervision of Dr. <a href="https://dnmduc.github.io/" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a>.</p> <p><strong>Research Interests:</strong> Artificial Intelligence, Computational Intelligence, Deep Learning, Machine Learning, Signal Processing, XAI &amp; Optimization, and their applications in the fields of Affective Computing, Antibody Therapeutics, Antibody Functional &amp; Structural Prediction, Audio &amp; Speech Processing, Bioinformatics, Biological Image Analysis, Computational Biology &amp; Medicine, Computer-Aided Drug Design, Environmental Science, Intelligent &amp; Optimal Control, Medical Image Analysis, Metagenomics Analysis, Multi-Omics Analysis, NeuroAI, Neurodegenerative Diseases, Peptide Therapeutics, Protein Design, Protein Functional &amp; Structural Prediction, Water Quality, and Water Resources Management.</p> <style>.no-margin{margin:0}.custom-figure{margin-top:0}.small-figure{width:100%}.reduce-gap{margin-bottom:5}</style> <hr> <hr> <figure class="custom-figure reduce-gap"> <img src="assets/img/laboratory.gif" class="small-figure"> </figure> <hr> <hr> <p><strong>Inspirational Quotes:</strong></p> <blockquote class="no-margin reduce-gap"> <p><q><i>Your origin does not determine what kind of person you are. Only you can decide who you become.</i></q> - <a href="https://mengchih.com/about/" rel="external nofollow noopener" target="_blank">Meng Chih Chiang</a></p> <p><q><i>Life can be heavy, especially if you try to carry it all at once.</i></q> - <a href="https://www.taylorswift.com/" rel="external nofollow noopener" target="_blank">Taylor Swift</a></p> <p><q><i>Life can be seen as a balance between positive and negative aspects throughout its entirety. Therefore, it's essential to cherish every moment and embrace life fully.</i></q> - <a href="https://nhattruongpham.github.io/">Nhat Truong Pham</a></p> </blockquote> <center> $$ \begin{align*} \text{Life} &amp;= \int_{\text{birth}}^{\infty} \left(\frac{\text{Happiness} \times \text{Love} \times \text{Peace}}{\text{time}} - \frac{\text{Sadness} \times \text{Revenge} \times \text{Discord}}{\text{time}}\right) \, \text{dtime} \\ &amp;= \int_{\text{birth}}^{\rm{t_{2025}}} \left(\frac{\text{Happiness} \times \text{Love} \times \text{Peace}}{\text{time}} - \frac{\text{Sadness} \times \text{Revenge} \times \text{Discord}}{\text{time}}\right) \, \text{dtime} \\ &amp;\quad + \int_{\rm{t_{2025}}}^{\infty} \left(\frac{\text{Happiness}' \times \text{Love}' \times \text{Peace}'}{\text{time}} - \frac{\text{Sadness}' \times \text{Revenge}' \times \text{Discord}'}{\text{time}}\right) \, \text{dtime} \end{align*} $$ </center> <hr> <hr> </div> <h2> <a href="/news/" style="color: inherit">Latest News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Feb 21, 2025</th> <td> <u><i><b>Received the University Innovation - Awards and Living Expenses (ÎåÄÌïôÌòÅÏã†-Ìè¨ÏÉÅÎ∞èÏÉùÌôúÎπÑ(ÏùºÎ∞òÏõê))</b></i></u> </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 20, 2025</th> <td> <u><i><b>Received the Caregen Chung Yong-ji Scholarship (ÏºÄÏñ¥Ï††Ï†ïÏö©ÏßÄÏû•ÌïôÍ∏à(ÏùºÎ∞òÏõê))</b></i></u> </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 04, 2025</th> <td> One collaborative manuscript entitled <b>‚Äú<i>MemoCMT:</i> multimodal emotion recognition using cross-modal transformer-based feature fusion‚Äù</b> has been accepted for publication in the <span style="color: #FF3636;"><i>Scientific Reports</i></span> journal </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 29, 2025</th> <td> One manuscript entitled <b>‚Äú<i>DOGpred:</i> A Novel Deep Learning Framework for Accurate Identification of Human O-linked Threonine Glycosylation Sites‚Äù</b> has been accepted for publication in the <span style="color: #FF3636;"><i>Journal of Molecular Biology</i></span> </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 16, 2025</th> <td> One collaborative manuscript entitled <b>‚ÄúHydraulic performance and wave transmission through nature-inspired perforated hollow-base piles breakwater‚Äù</b> has been accepted for publication in the <span style="color: #FF3636;"><i>Ocean Engineering</i></span> journal </td> </tr> </table> </div> </div> <hr> <hr> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <p> (‚Ä†) denotes equal contribution </p> <p> (*) denotes correspondance </p> <p> <span style="display: inline-block; width: 15px; height: 15px; background-color: #600;"></span> denotes journal </p> <p> <span style="display: inline-block; width: 15px; height: 15px; background-color: #215d42;"></span> denotes conference </p> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://www.nature.com/srep/" rel="external nofollow noopener" target="_blank">Sci Rep</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/MemoCMT.jpg" sizes="200px"> <img src="/assets/img/publication_preview/MemoCMT.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="MemoCMT.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="khan2025memocmt" class="col-sm-8"> <div class="title"> <b><i>MemoCMT:</i></b> multimodal emotion recognition using cross-modal transformer-based feature fusion</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=uEZhRWAAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Mustaqeem Khan<sup>‚Ä†</sup></a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=NKbwDD8AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Phuong-Nam Tran<sup>‚Ä†</sup></a> ,¬†<em><b style="color: #FFA500;">Nhat Truong Pham<sup>‚Ä†</sup></b></em> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=VcOjgngAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Abdulmotaleb El Saddik</a> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=nlonFkwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Alice Othmani</a> </div> <div class="periodical"> <em><i><a href="https://www.nature.com/srep/" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Scientific Reports</a></i></em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.nature.com/articles/s41598-025-89202-x" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/MemoCMT.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a> <a href="https://github.com/tpnam0901/MemoCMT/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1038/s41598-025-89202-x" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:A8cqit5AE6sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Speech emotion recognition has seen a surge in transformer models, which excel at understanding the overall message by analyzing long-term patterns in speech. However, these models come at a computational cost. In contrast, convolutional neural networks are faster but struggle with capturing these long-range relationships. Our proposed system, MemoCMT, tackles this challenge using a novel ‚Äúcross-modal transformer‚Äù (CMT). This CMT can effectively analyze local and global speech features and their corresponding text. To boost efficiency, MemoCMT leverages recent advancements in pre-trained models: HuBERT extracts meaningful features from the audio, while BERT analyzes the text. The core innovation lies in how the CMT component utilizes and integrates these audio and text features. After this integration, different fusion techniques are applied before final emotion classification. Experiments show that MemoCMT achieves impressive performance, with the CMT using min aggregation achieving the highest unweighted accuracy (UW-Acc) of 81.33% and 91.93%, and weighted accuracy (W-Acc) of 81.85% and 91.84% respectively on benchmark IEMOCAP and ESD corpora. The results of our system demonstrate the generalization capacity and robustness for real-world industrial applications. Moreover, the implementation details of MemoCMT are publicly available at <a href="https://github.com/tpnam0901/MemoCMT/" rel="external nofollow noopener" target="_blank">https://github.com/tpnam0901/MemoCMT/</a> for reproducibility purposes.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">khan2025memocmt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MemoCMT: multimodal emotion recognition using cross-modal transformer-based feature fusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khan, Mustaqeem and Tran, Phuong-Nam and Pham, Nhat Truong and El Saddik, Abdulmotaleb and Othmani, Alice}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Scientific Reports}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5473}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Nature Publishing Group UK London}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1038/s41598-025-89202-x}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://www.sciencedirect.com/journal/journal-of-molecular-biology" rel="external nofollow noopener" target="_blank">JMB</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/DOGpred.jpeg" sizes="200px"> <img src="/assets/img/publication_preview/DOGpred.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="DOGpred.jpeg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="lee2025dogpred" class="col-sm-8"> <div class="title"> <b><i>DOGpred:</i></b> A Novel Deep Learning Framework for Accurate Identification of Human O-linked Threonine Glycosylation Sites</div> <div class="author"> Ki Wook Lee<sup>‚Ä†</sup> ,¬†<em><b style="color: #FFA500;">Nhat Truong Pham<sup>‚Ä†</sup></b></em> ,¬†Hye Jung Min ,¬†Hyun Woo Park ,¬†Ji Won Lee ,¬†Han-En Lo ,¬†Na Young Kwon ,¬†Jimin Seo ,¬†Illia Shaginyan ,¬†Heeje Cho ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0EAV03MAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Leyi Wei</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=DkVnJ-wAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Young-Jun Jeon</a> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/journal-of-molecular-biology" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Journal of Molecular Biology</a></i></em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0022283625000439" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/JeonRPM/DOGpred/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://doi.org/10.5281/zenodo.13141341" rel="external nofollow noopener" target="_blank"> <img src="https://zenodo.org/badge/doi/10.5281/zenodo.13141341.svg" alt="DOI"> </a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.jmb.2025.168977" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:MIg0yeAD4ggC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>O-linked glycosylation is a crucial post-transcriptional modification that regulates protein function and biological processes. Dysregulation of this process is associated with various diseases, underscoring the need to accurately identify O-linked glycosylation sites on proteins. Current experimental methods for identifying O-linked threonine glycosylation (OTG) sites are often complex and costly. Consequently, developing computational tools that predict these sites based on protein features is crucial. Such tools can complement experimental approaches, enhancing our understanding of the role of OTG dysregulation in diseases and uncovering potential therapeutic targets. In this study, we developed DOGpred, a deep learning-based predictor for precisely identifying human OTGs using high-latent feature representations. Initially, we extracted nine different conventional feature descriptors (CFDs) and nine pre-trained protein language model (PLM)-based embeddings. Notably, each feature was encoded as a 2D tensor, capturing both the sequential and inherent feature characteristics. Subsequently, we designed a stacked convolutional neural network (CNN) module to learn spatial feature representations from CFDs and a stacked recurrent neural network (RNN) module to learn temporal feature representations from PLM-based embeddings. These features were integrated using attention-based fusion mechanisms to generate high-level feature representations for final classification. Ablation analysis and independent tests demonstrated that the optimal model (DOGpred), employing a stacked 1D CNN and a stacked attention-based RNN module with cross-attention feature fusion, achieved the best performance on the training dataset and significantly outperformed machine learning-based single-feature models and state-of-the-art methods on independent datasets. Furthermore, DOGpred is publicly available at <a href="https://github.com/JeonRPM/DOGpred/" rel="external nofollow noopener" target="_blank">https://github.com/JeonRPM/DOGpred/</a> for free access and usage.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lee2025dogpred</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DOGpred: A Novel Deep Learning Framework for Accurate Identification of Human O-linked Threonine Glycosylation Sites}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lee, Ki Wook and Pham, Nhat Truong and Min, Hye Jung and Park, Hyun Woo and Lee, Ji Won and Lo, Han-En and Kwon, Na Young and Seo, Jimin and Shaginyan, Illia and Cho, Heeje and Wei, Leyi and Manavalan, Balachandran and Jeon, Young-Jun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Molecular Biology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{168977}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.jmb.2025.168977}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://www.sciencedirect.com/journal/computers-in-biology-and-medicine" rel="external nofollow noopener" target="_blank">CIBM</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/XCT-COVID.png" sizes="200px"> <img src="/assets/img/publication_preview/XCT-COVID.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="XCT-COVID.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="pham2025leveraging" class="col-sm-8"> <div class="title">Leveraging deep transfer learning and explainable AI for accurate COVID-19 diagnosis: Insights from a multi-national chest CT scan study</div> <div class="author"> <em><b style="color: #FFA500;">Nhat Truong Pham</b></em> ,¬†Jinsol Ko ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=W5BcBWoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Masaud Shah</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5WP7pOUAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Rajan Rakkiyappan</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=oM75QQMAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Hyun Goo Woo</a> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/computers-in-biology-and-medicine" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Computers in Biology and Medicine</a></i></em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010482524015464" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/cbbl-skku-org/XCT-COVID/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://doi.org/10.5281/zenodo.12772023" rel="external nofollow noopener" target="_blank"> <img src="https://zenodo.org/badge/doi/10.5281/zenodo.12772023.svg" alt="DOI"> </a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.compbiomed.2024.109461" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:Wq2b2clWBLsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The COVID-19 pandemic has emerged as a global health crisis, impacting millions worldwide. Although chest computed tomography (CT) scan images are pivotal in diagnosing COVID-19, their manual interpretation by radiologists is time-consuming and potentially subjective. Automated computer-aided diagnostic (CAD) frameworks offer efficient and objective solutions. However, machine or deep learning methods often face challenges in their reproducibility due to underlying biases and methodological flaws. To address these issues, we propose XCT-COVID, an explainable, transferable, and reproducible CAD framework based on deep transfer learning to predict COVID-19 infection from CT scan images accurately. This is the first study to develop three distinct models within a unified framework by leveraging a previously unexplored large dataset and two widely used smaller datasets. We employed five known convolutional neural network architectures, both with and without pretrained weights, on the larger dataset. We optimized hyperparameters through extensive grid search and 5-fold cross-validation (CV), significantly enhancing the model performance. Experimental results from the larger dataset showed that the VGG16 architecture (XCT-COVID-L) with pretrained weights consistently outperformed other architectures, achieving the best performance, on both 5-fold CV and independent test. When evaluated with the external datasets, XCT-COVID-L performed well with data with similar distributions, demonstrating its transferability. However, its performance significantly decreased on smaller datasets with lower-quality images. To address this, we developed other models, XCT-COVID-S1 and XCT-COVID-S2, specifically for the smaller datasets, outperforming existing methods. Moreover, eXplainable Artificial Intelligence (XAI) analyses were employed to interpret the models‚Äô functionalities. For prediction and reproducibility purposes, the implementation of XCT-COVID is publicly accessible at <a href="https://github.com/cbbl-skku-org/XCT-COVID/" rel="external nofollow noopener" target="_blank">https://github.com/cbbl-skku-org/XCT-COVID/</a>.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2025leveraging</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Leveraging deep transfer learning and explainable AI for accurate COVID-19 diagnosis: Insights from a multi-national chest CT scan study}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Nhat Truong and Ko, Jinsol and Shah, Masaud and Rakkiyappan, Rajan and Woo, Hyun Goo and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computers in Biology and Medicine}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{185}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{109461}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.compbiomed.2024.109461}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#215d42"> <a href="https://language-plus-molecules.github.io/" rel="external nofollow noopener" target="_blank">LPM@ACL</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Mol2Lang-VLM.png" sizes="200px"> <img src="/assets/img/publication_preview/Mol2Lang-VLM.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Mol2Lang-VLM.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tran2024mollangvlm" class="col-sm-8"> <div class="title"> <b><i>Mol2Lang-VLM:</i></b> Vision- and Text-Guided Generative Pre-trained Language Models for Advancing Molecule Captioning through Multimodal Fusion</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=kz_chQ4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Duong Thanh Tran<sup>‚Ä†</sup></a> ,¬†<em><b style="color: #FFA500;">Nhat Truong Pham<sup>‚Ä†</sup></b></em> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=-aEoZCgAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Nguyen Doan Hieu Nguyen</a> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em>In <i><a href="https://language-plus-molecules.github.io/" style="color: #00ab37; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Proceedings of the 1st Workshop on Language + Molecules (L+M 2024)</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2024.langmol-1.12/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://aclanthology.org/2024.langmol-1.12.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/nhattruongpham/mol-lang-bridge/tree/mol2lang" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:AYInfyleIOsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper introduces Mol2Lang-VLM, an enhanced method for refining generative pre-trained language models for molecule captioning using multimodal features to achieve more accurate caption generation. Our approach leverages the encoder and decoder blocks of the Transformer-based architecture by introducing third sub-layers into both. Specifically, we insert sub-layers in the encoder to fuse features from SELFIES strings and molecular images, while the decoder fuses features from SMILES strings and their corresponding descriptions. Moreover, cross multi-head attention is employed instead of common multi-head attention to enable the decoder to attend to the encoder‚Äôs output, thereby integrating the encoded contextual information for better and more accurate caption generation. Performance evaluation on the CheBI-20 and L+M-24 benchmark datasets demonstrates Mol2Lang-VLM‚Äôs superiority, achieving higher accuracy and quality in caption generation compared to existing methods. Our code and pre-processed data are available at <a href="https://github.com/nhattruongpham/mol-lang-bridge/tree/mol2lang" rel="external nofollow noopener" target="_blank">https://github.com/nhattruongpham/mol-lang-bridge/tree/mol2lang/</a>.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tran2024mollangvlm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mol2Lang-{VLM}: Vision- and Text-Guided Generative Pre-trained Language Models for Advancing Molecule Captioning through Multimodal Fusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tran, Duong Thanh and Pham, Nhat Truong and Nguyen, Nguyen Doan Hieu and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 1st Workshop on Language + Molecules L+M 2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{97--102}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2024.langmol-1.12}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#215d42"> <a href="https://language-plus-molecules.github.io/" rel="external nofollow noopener" target="_blank">LPM@ACL</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Lang2Mol-Diff.png" sizes="200px"> <img src="/assets/img/publication_preview/Lang2Mol-Diff.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Lang2Mol-Diff.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="nguyen2024langmoldiff" class="col-sm-8"> <div class="title"> <b><i>Lang2Mol-Diff:</i></b> A Diffusion-Based Generative Model for Language-to-Molecule Translation Leveraging SELFIES Molecular String Representation</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=-aEoZCgAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Nguyen Doan Hieu Nguyen<sup>‚Ä†</sup></a> ,¬†<em><b style="color: #FFA500;">Nhat Truong Pham<sup>‚Ä†</sup></b></em> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=kz_chQ4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Duong Thanh Tran</a> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em>In <i><a href="https://language-plus-molecules.github.io/" style="color: #00ab37; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Proceedings of the 1st Workshop on Language + Molecules (L+M 2024)</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2024.langmol-1.15/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://aclanthology.org/2024.langmol-1.15.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/nhattruongpham/mol-lang-bridge/tree/lang2mol" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:isU91gLudPYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Generating <i>de novo</i> molecules from textual descriptions is challenging due to potential issues with molecule validity in SMILES representation and limitations of autoregressive models. This work introduces Lang2Mol-Diff, a diffusion-based language-to-molecule generative model using the SELFIES representation. Specifically, Lang2Mol-Diff leverages the strengths of two state-of-the-art molecular generative models: BioT5 and TGM-DLM. By employing BioT5 to tokenize the SELFIES representation, Lang2Mol-Diff addresses the validity issues associated with SMILES strings. Additionally, it incorporates a text diffusion mechanism from TGM-DLM to overcome the limitations of autoregressive models in this domain. To the best of our knowledge, this is the first study to leverage the diffusion mechanism for text-based <i>de novo</i> molecule generation using the SELFIES molecular string representation. Performance evaluation on the L+M-24 benchmark dataset shows that Lang2Mol-Diff outperforms all existing methods for molecule generation in terms of validity. Our code and pre-processed data are available at <a href="https://github.com/nhattruongpham/mol-lang-bridge/tree/lang2mol" rel="external nofollow noopener" target="_blank">https://github.com/nhattruongpham/mol-lang-bridge/tree/lang2mol/</a>.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nguyen2024langmoldiff</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Lang2Mol-Diff: A Diffusion-Based Generative Model for Language-to-Molecule Translation Leveraging SELFIES Molecular String Representation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Nguyen Doan Hieu and Pham, Nhat Truong and Tran, Duong Thanh and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 1st Workshop on Language + Molecules L+M 2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{128--134}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2024.langmol-1.15}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://www.sciencedirect.com/journal/computers-in-biology-and-medicine" rel="external nofollow noopener" target="_blank">CIBM</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/HOTGpred.png" sizes="200px"> <img src="/assets/img/publication_preview/HOTGpred.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="HOTGpred.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="pham2024hotgpred" class="col-sm-8"> <div class="title"> <b><i>HOTGpred:</i></b> Enhancing human O-linked threonine glycosylation prediction using integrated pretrained protein language model-based features and multi-stage feature selection approach</div> <div class="author"> <em><b style="color: #FFA500;">Nhat Truong Pham<sup>‚Ä†</sup></b></em> ,¬†Ying Zhang<sup>‚Ä†</sup> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5WP7pOUAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Rajan Rakkiyappan</a> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/computers-in-biology-and-medicine" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Computers in Biology and Medicine</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010482524009442" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://balalab-skku.org/HOTGpred/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.compbiomed.2024.108859" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:2ywjKiB__4kC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-4-4285F4?logo=googlescholar&amp;labelColor=beige" alt="4 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>O-linked glycosylation is a complex post-translational modification (PTM) in human proteins that plays a critical role in regulating various cellular metabolic and signaling pathways. In contrast to N-linked glycosylation, O-linked glycosylation lacks specific sequence features and maintains an unstable core structure. Identifying O-linked threonine glycosylation sites (OTGs) remains challenging, requiring extensive experimental tests. While bioinformatics tools have emerged for predicting OTGs, their reliance on limited conventional features and absence of well-defined feature selection strategies limit their effectiveness. To address these limitations, we introduced HOTGpred (Human O-linked Threonine Glycosylation predictor), employing a multi-stage feature selection process to identify the optimal feature set for accurately identifying OTGs. Initially, we assessed 25 different feature sets derived from various pretrained protein language model (PLM)-based embeddings and conventional feature descriptors using nine classifiers. Subsequently, we integrated the top five embeddings linearly and determined the most effective scoring function for ranking hybrid features, identifying the optimal feature set through a process of sequential forward search. Among the classifiers, the extreme gradient boosting (XGBT)-based model, using the optimal feature set (HOTGpred), achieved 92.03% accuracy on the training dataset and 88.25% on the balanced independent dataset. Notably, HOTGpred significantly outperformed the current state-of-the-art methods on both the balanced and imbalanced independent datasets, demonstrating its superior prediction capabilities. Additionally, SHapley Additive exPlanations (SHAP) and ablation analyses were conducted to identify the features contributing most significantly to HOTGpred. Finally, we developed an easy-to-navigate web server, accessible at <a href="https://balalab-skku.org/HOTGpred/" rel="external nofollow noopener" target="_blank">https://balalab-skku.org/HOTGpred/</a>, to support glycobiologists in their research on glycosylation structure and function.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2024hotgpred</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HOTGpred: Enhancing human O-linked threonine glycosylation prediction using integrated pretrained protein language model-based features and multi-stage feature selection approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Nhat Truong and Zhang, Ying and Rakkiyappan, Rajan and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computers in Biology and Medicine}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{179}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{108859}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.compbiomed.2024.108859}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://www.sciencedirect.com/journal/journal-of-molecular-biology" rel="external nofollow noopener" target="_blank">JMB</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mACPpred2.png" sizes="200px"> <img src="/assets/img/publication_preview/mACPpred2.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mACPpred2.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="sangaraju2024macppred" class="col-sm-8"> <div class="title"> <b><i>mACPpred 2.0:</i></b> Stacked Deep Learning for Anticancer Peptide Prediction with Integrated Spatial and Probabilistic Feature Representations</div> <div class="author"> Vinoth Kumar Sangaraju<sup>‚Ä†</sup> ,¬†<em><b style="color: #FFA500;">Nhat Truong Pham<sup>‚Ä†</sup></b></em> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0EAV03MAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Leyi Wei</a> ,¬†Xue Yu ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/journal-of-molecular-biology" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Journal of Molecular Biology</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0022283624002894" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/nhattruongpham/mACPpred2/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://balalab-skku.org/mACPpred2/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://doi.org/10.5281/zenodo.11350064" rel="external nofollow noopener" target="_blank"> <img src="https://zenodo.org/badge/doi/10.5281/zenodo.11350064.svg" alt="DOI"> </a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.jmb.2024.168687" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:1tZ8xJnm2c8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-9-4285F4?logo=googlescholar&amp;labelColor=beige" alt="9 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Anticancer peptides (ACPs), naturally occurring molecules with remarkable potential to target and kill cancer cells. However, identifying ACPs based solely from their primary amino acid sequences remains a major hurdle in immunoinformatics. In the past, several web-based machine learning (ML) tools have been proposed to assist researchers in identifying potential ACPs for further testing. Notably, our meta- approach method, mACPpred, introduced in 2019, has significantly advanced the field of ACP research. Given the exponential growth in the number of characterized ACPs, there is now a pressing need to create an updated version of mACPpred. To develop mACPpred 2.0, we constructed an up-to-date benchmarking dataset by integrating all publicly available ACP datasets. We employed a large-scale of feature descriptors, encompassing both conventional feature descriptors and advanced pre-trained natural language processing (NLP)-based embeddings. We evaluated their ability to discriminate between ACPs and non-ACPs using eleven different classifiers. Subsequently, we employed a stacked deep learning (SDL) approach, incorporating 1D convolutional neural network (1D CNN) blocks and hybrid features. These features included the top seven performing NLP-based features and 90 probabilistic features, allowing us to identify hidden patterns within these diverse features and improve the accuracy of our ACP prediction model. This is the first study to integrate spatial and probabilistic feature representations for predicting ACPs. Rigorous cross-validation and independent tests conclusively demonstrated that mACPpred 2.0 not only surpassed its predecessor (mACPpred) but also outperformed the existing state-of-the-art predictors, highlighting the importance of advanced feature representation capabilities attained through SDL. To facilitate widespread use and accessibility, we have developed a user-friendly for mACPpred 2.0, available at <a href="https://balalab-skku.org/mACPpred2/" rel="external nofollow noopener" target="_blank">https://balalab-skku.org/mACPpred2/</a>.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sangaraju2024macppred</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{mACPpred 2.0: Stacked Deep Learning for Anticancer Peptide Prediction with Integrated Spatial and Probabilistic Feature Representations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sangaraju, Vinoth Kumar and Pham, Nhat Truong and Wei, Leyi and Yu, Xue and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Molecular Biology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{436}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{168687}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.jmb.2024.168687}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://www.cell.com/molecular-therapy-family/nucleic-acids/home" rel="external nofollow noopener" target="_blank">MTNA</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ac4C-AFL.png" sizes="200px"> <img src="/assets/img/publication_preview/ac4C-AFL.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ac4C-AFL.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="pham2024ac4c" class="col-sm-8"> <div class="title"> <b><i>ac4C-AFL:</i></b> A high-precision identification of human mRNA N4-acetylcytidine sites based on adaptive feature representation learning</div> <div class="author"> <em><b style="color: #FFA500;">Nhat Truong Pham</b></em> ,¬†Annie Terrina Terrance ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=DkVnJ-wAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Young-Jun Jeon</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5WP7pOUAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Rajan Rakkiyappan</a> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><i><a href="https://www.cell.com/molecular-therapy-family/nucleic-acids/home" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Molecular Therapy-Nucleic Acids</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.cell.com/molecular-therapy-family/nucleic-acids/fulltext/S2162-2531(24)00079-9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/ac4C-AFL.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a> <a href="https://balalab-skku.org/ac4C-AFL/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.omtn.2024.102192" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:27LrP4qxOz0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-10-4285F4?logo=googlescholar&amp;labelColor=beige" alt="10 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>RNA N4-acetylcytidine (ac4C) is a highly conserved RNA modification that plays a crucial role in controlling mRNA stability, processing, and translation. Consequently, accurate identification of ac4C sites across the genome is critical for understanding gene expression regulation mechanisms. In this study, we have developed ac4C-AFL, a bioinformatics tool that precisely identifies ac4C sites from primary RNA sequences. In ac4C-AFL, we identified the optimal sequence length for model building and implemented an adaptive feature representation strategy that is capable of extracting the most representative features from RNA. To identify the most relevant features, we proposed a novel ensemble feature importance scoring strategy to rank features effectively. We then used this information to conduct the sequential forward search, which individually determine the optimal feature set from the 16 sequence-derived feature descriptors. Utilizing these optimal feature descriptors, we constructed 176 baseline models using 11 popular classifiers. The most efficient baseline models were identified using the two-step feature selection approach, whose predicted scores were integrated and trained with the appropriate classifier to develop the final prediction model. Our rigorous cross-validations and independent tests demonstrate that ac4C-AFL surpasses contemporary tools in predicting ac4C sites. Moreover, we have developed a publicly accessible web server at <a href="https://balalab-skku.org/ac4C-AFL/" rel="external nofollow noopener" target="_blank">https://balalab-skku.org/ac4C-AFL/</a>.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2024ac4c</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ac4C-AFL: A high-precision identification of human mRNA N4-acetylcytidine sites based on adaptive feature representation learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Nhat Truong and Terrance, Annie Terrina and Jeon, Young-Jun and Rakkiyappan, Rajan and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Molecular Therapy-Nucleic Acids}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{102192}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.omtn.2024.102192}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://academic.oup.com/bib" rel="external nofollow noopener" target="_blank">BiB</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/H2Opred.png" sizes="200px"> <img src="/assets/img/publication_preview/H2Opred.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="H2Opred.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="pham2024h2opred" class="col-sm-8"> <div class="title"> <b><i>H2Opred:</i></b> a robust and efficient hybrid deep learning model for predicting 2‚Äô-O-methylation sites in human RNA</div> <div class="author"> <em><b style="color: #FFA500;">Nhat Truong Pham</b></em> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5WP7pOUAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Rajan Rakkiyappan</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=_YZClBgAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Jongsun Park</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=IRshBmMAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Adeel Malik</a> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><i><a href="https://academic.oup.com/bib" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Briefings in Bioinformatics</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://academic.oup.com/bib/article/25/1/bbad476/7510980" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/H2Opred.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a> <a href="https://balalab-skku.org/H2Opred/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://www.ibric.org/s.do?CWXEPvOoln" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1093/bib/bbad476" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:QsaTk4IG4EwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-23-4285F4?logo=googlescholar&amp;labelColor=beige" alt="23 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>2‚Äô-O-methylation (2OM) is the most common post-transcriptional modification of RNA. It plays a crucial role in RNA splicing, RNA stability and innate immunity. Despite advances in high-throughput detection, the chemical stability of 2OM makes it difficult to detect and map in messenger RNA. Therefore, bioinformatics tools have been developed using machine learning (ML) algorithms to identify 2OM sites. These tools have made significant progress, but their performances remain unsatisfactory and need further improvement. In this study, we introduced H2Opred, a novel hybrid deep learning (HDL) model for accurately identifying 2OM sites in human RNA. Notably, this is the first application of HDL in developing four nucleotide-specific models [adenine (A2OM), cytosine (C2OM), guanine (G2OM) and uracil (U2OM)] as well as a generic model (N2OM). H2Opred incorporated both stacked 1D convolutional neural network (1D-CNN) blocks and stacked attention-based bidirectional gated recurrent unit (Bi-GRU-Att) blocks. 1D-CNN blocks learned effective feature representations from 14 conventional descriptors, while Bi-GRU-Att blocks learned feature representations from five natural language processing-based embeddings extracted from RNA sequences. H2Opred integrated these feature representations to make the final prediction. Rigorous cross-validation analysis demonstrated that H2Opred consistently outperforms conventional ML-based single-feature models on five different datasets. Moreover, the generic model of H2Opred demonstrated a remarkable performance on both training and testing datasets, significantly outperforming the existing predictor and other four nucleotide-specific H2Opred models. To enhance accessibility and usability, we have deployed a user-friendly web server for H2Opred, accessible at <a href="https://balalab-skku.org/H2Opred/" rel="external nofollow noopener" target="_blank">https://balalab-skku.org/H2Opred/</a>. This platform will serve as an invaluable tool for accurately predicting 2OM sites within human RNA, thereby facilitating broader applications in relevant research endeavors.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2024h2opred</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{H2Opred: a robust and efficient hybrid deep learning model for predicting 2'-O-methylation sites in human RNA}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Nhat Truong and Rakkiyappan, Rajan and Park, Jongsun and Malik, Adeel and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Briefings in Bioinformatics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{25}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{bbad476}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{OxfOxford University Pressord}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1093/bib/bbad476}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://academic.oup.com/bib" rel="external nofollow noopener" target="_blank">BiB</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/MeL-STPhos.png" sizes="200px"> <img src="/assets/img/publication_preview/MeL-STPhos.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="MeL-STPhos.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="pham2024advancing" class="col-sm-8"> <div class="title">Advancing the accuracy of SARS-CoV-2 phosphorylation site detection via meta-learning approach</div> <div class="author"> <em><b style="color: #FFA500;">Nhat Truong Pham<sup>‚Ä†</sup></b></em> ,¬†Le Thi Phan<sup>‚Ä†</sup> ,¬†Jimin Seo ,¬†Yeonwoo Kim ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=sBSJfo4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Minkyung Song</a> ,¬†Sukchan Lee ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=DkVnJ-wAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Young-Jun Jeon</a> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><i><a href="https://academic.oup.com/bib" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Briefings in Bioinformatics</a></i></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://academic.oup.com/bib/article/25/1/bbad433/7459584" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/MeL-STPhos.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a> <a href="https://balalab-skku.org/MeL-STPhos/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://www.ibric.org/s.do?nLdsctWFBW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a> <a href="https://www.ibric.org/s.do?ovlRZSpKQk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Interview</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1093/bib/bbad433" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:k_7cPK9k7w8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-20-4285F4?logo=googlescholar&amp;labelColor=beige" alt="20 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The worldwide appearance of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has generated significant concern and posed a considerable challenge to global health. Phosphorylation is a common post-translational modification that affects many vital cellular functions and is closely associated with SARS-CoV-2 infection. Precise identification of phosphorylation sites could provide more in-depth insight into the processes underlying SARS-CoV-2 infection and help alleviate the continuing coronavirus disease 2019 (COVID-19) crisis. Currently, available computational tools for predicting these sites lack accuracy and effectiveness. In this study, we designed an innovative meta-learning model, Meta-Learning for Serine/Threonine Phosphorylation (MeL-STPhos), to precisely identify protein phosphorylation sites. We initially performed a comprehensive assessment of 29 unique sequence-derived features, establishing prediction models for each using 14 renowned machine learning methods, ranging from traditional classifiers to advanced deep learning algorithms. We then selected the most effective model for each feature by integrating the predicted values. Rigorous feature selection strategies were employed to identify the optimal base models and classifier(s) for each cell-specific dataset. To the best of our knowledge, this is the first study to report two cell-specific models and a generic model for phosphorylation site prediction by utilizing an extensive range of sequence-derived features and machine learning algorithms. Extensive cross-validation and independent testing revealed that MeL-STPhos surpasses existing state-of-the-art tools for phosphorylation site prediction. We also developed a publicly accessible platform at <a href="https://balalab-skku.org/MeL-STPhos/" rel="external nofollow noopener" target="_blank">https://balalab-skku.org/MeL-STPhos/</a>. We believe that MeL-STPhos will serve as a valuable tool for accelerating the discovery of serine/threonine phosphorylation sites and elucidating their role in post-translational regulation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2024advancing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Advancing the accuracy of SARS-CoV-2 phosphorylation site detection via meta-learning approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Nhat Truong and Phan, Le Thi and Seo, Jimin and Kim, Yeonwoo and Song, Minkyung and Lee, Sukchan and Jeon, Young-Jun and Manavalan, Balachandran}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Briefings in Bioinformatics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{25}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{bbad433}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{OxfOxford University Pressord}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1093/bib/bbad433}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://www.sciencedirect.com/journal/knowledge-based-systems" rel="external nofollow noopener" target="_blank">KBS</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/AAD-Net.png" sizes="200px"> <img src="/assets/img/publication_preview/AAD-Net.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="AAD-Net.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="mustaqeem2023aad" class="col-sm-8"> <div class="title"> <b><i>AAD-Net:</i></b> Advanced end-to-end signal processing system for human emotion detection &amp; recognition using attention-based deep echo state network</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=uEZhRWAAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Mustaqeem Khan</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=VcOjgngAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Abdulmotaleb El Saddik</a> ,¬†Fahd Saleh Alotaibi ,¬†and¬†<em><b style="color: #FFA500;">Nhat Truong Pham</b></em> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/knowledge-based-systems" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Knowledge-Based Systems</a></i></em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705123002757" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.knosys.2023.110525" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:HGTzPopzzJcC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-49-4285F4?logo=googlescholar&amp;labelColor=beige" alt="49 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Speech signals are the most convenient way of communication between human beings and the eventual method of Human-Computer Interaction (HCI) to exchange emotions and information. Recognizing emotions from speech signals is a challenging task due to the sparse nature of emotional data and features. In this article, we proposed a Deep Echo-State-Network (DeepESN) system for emotion recognition with a dilated convolution neural network and multi-headed attention mechanism. To reduce the model complexity, we incorporate a DeepESN that combines reservoir computing for higher-dimensional mapping. We also used fine-tuned Sparse Random Projection (SRP) to reduce dimensionality and adopted an early fusion strategy to fuse the extracted cues and passed the joint feature vector via a classification layer to recognize emotions. Our proposed model is evaluated on two public speech corpora, EMO-DB and RAVDESS, and tested for subject/speaker-dependent/independent performance. The results show that our proposed system achieves a high recognition rate, 91.14, 85.57 for EMO-DB, and 82.01, 77.02 for RAVDESS, using speaker-dependent and independent experiments, respectively. Our proposed system outperforms the State-of-The-Art (SOTA) while requiring less computational time.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mustaqeem2023aad</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AAD-Net: Advanced end-to-end signal processing system for human emotion detection \&amp; recognition using attention-based deep echo state network}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khan, Mustaqeem and El Saddik, Abdulmotaleb and Alotaibi, Fahd Saleh and Pham, Nhat Truong}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Knowledge-Based Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{270}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{110525}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.knosys.2023.110525}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://www.sciencedirect.com/journal/expert-systems-with-applications" rel="external nofollow noopener" target="_blank">ESWA</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/HDA_mADCRNN.png" sizes="200px"> <img src="/assets/img/publication_preview/HDA_mADCRNN.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="HDA_mADCRNN.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="pham2023hybrid" class="col-sm-8"> <div class="title">Hybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition</div> <div class="author"> <em><b style="color: #FFA500;">Nhat Truong Pham</b></em> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=2UKP440AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=At-Y-H8AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Ngoc Duy Nguyen</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=xr39SOwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Thanh Thi Nguyen</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5b9ncWoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Hai Nguyen</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0vkenbwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=an_4IJkAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Chee Peng Lim</a> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=XFUxOkoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen</a> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/expert-systems-with-applications" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Expert Systems with Applications</a></i></em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2109.09026" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0957417423011107" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/HDA_mADCRNN.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a> <a href="https://github.com/nhattruongpham/hda-adcrnn-ser" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.eswa.2023.120608" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:NDuN12AVoxsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-39-4285F4?logo=googlescholar&amp;labelColor=beige" alt="39 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Recently, speech emotion recognition (SER) has become an active research area in speech processing, particularly with the advent of deep learning (DL). Numerous DL-based methods have been proposed for SER. However, most of the existing DL-based models are complex and require a large amounts of data to achieve a good performance. In this study, a new framework of deep attention-based dilated convolutional-recurrent neural networks coupled with a hybrid data augmentation method was proposed for addressing SER tasks. The hybrid data augmentation method constitutes an upsampling technique for generating more speech data samples based on the traditional and generative adversarial network approaches. By leveraging both convolutional and recurrent neural networks in a dilated form along with an attention mechanism, the proposed DL framework can extract high-level representations from three-dimensional log Mel spectrogram features. Dilated convolutional neural networks acquire larger receptive fields, whereas dilated recurrent neural networks overcome complex dependencies as well as the vanishing and exploding gradient issues. Furthermore, the loss functions are reconfigured by combining the SoftMax loss and the center-based losses to classify various emotional states. The proposed framework was implemented using the Python programming language and the TensorFlow deep learning library. To validate the proposed framework, the EmoDB and ERC benchmark datasets, which are imbalanced and/or small datasets, were employed. The experimental results indicate that the proposed framework outperforms other related state-of-the-art methods, yielding the highest unweighted recall rates of 88.03 ¬± 1.39 (%) and 66.56 ¬± 0.67 (%) for the EmoDB and ERC datasets, respectively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2023hybrid</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Nhat Truong and Dang, Duc Ngoc Minh and Nguyen, Ngoc Duy and Nguyen, Thanh Thi and Nguyen, Hai and Manavalan, Balachandran and Lim, Chee Peng and Nguyen, Sy Dzung}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Expert Systems with Applications}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{120608}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.eswa.2023.120608}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://www.sciencedirect.com/journal/expert-systems-with-applications" rel="external nofollow noopener" target="_blank">ESWA</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Fruit-CoV.png" sizes="200px"> <img src="/assets/img/publication_preview/Fruit-CoV.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Fruit-CoV.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="nguyen2023fruit" class="col-sm-8"> <div class="title"> <b><i>Fruit-CoV:</i></b> An efficient vision-based framework for speedy detection and diagnosis of SARS-CoV-2 infections through recorded cough sounds</div> <div class="author"> Long H. Nguyen<sup>‚Ä†</sup> ,¬†<em><b style="color: #FFA500;">Nhat Truong Pham<sup>‚Ä†*</sup></b></em> ,¬†Van Huong Do ,¬†Liu Tai Nguyen ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=zSAfD80AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Thanh Tin Nguyen</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5b9ncWoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Hai Nguyen</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=At-Y-H8AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Ngoc Duy Nguyen</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=xr39SOwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Thanh Thi Nguyen</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=XFUxOkoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen</a> ,¬†Asim Bhatti ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=an_4IJkAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Chee Peng Lim</a> </div> <div class="periodical"> <em><i><a href="https://www.sciencedirect.com/journal/expert-systems-with-applications" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Expert Systems with Applications</a></i></em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2109.03219" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0957417422022308" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/nhattruongpham/Fruit-CoV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.eswa.2022.119212" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:WHdLCjDvYFkC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-16-4285F4?logo=googlescholar&amp;labelColor=beige" alt="16 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>COVID-19 is an infectious disease caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). This deadly virus has spread worldwide, leading to a global pandemic since March 2020. A recent variant of SARS-CoV-2 named Delta is intractably contagious and responsible for more than four million deaths globally. Therefore, developing an efficient self-testing service for SARS-CoV-2 at home is vital. In this study, a two-stage vision-based framework, namely Fruit-CoV, is introduced for detecting SARS-CoV-2 infections through recorded cough sounds. Specifically, audio signals are converted into Log-Mel spectrograms, and the EfficientNet-V2 network is used to extract their visual features in the first stage. In the second stage, 14 convolutional layers extracted from the large-scale Pretrained Audio Neural Networks for audio pattern recognition (PANNs) and the Wavegram-Log-Mel-CNN are employed to aggregate feature representations of the Log-Mel spectrograms and the waveform. Finally, the combined features are used to train a binary classifier. In this study, a dataset provided by the AICovidVN 115M Challenge is employed for evaluation. It includes 7,371 recorded cough sounds collected throughout Vietnam, India, and Switzerland. Experimental results indicate that the proposed model achieves an Area Under the Receiver Operating Characteristic Curve (AUC) score of 92.8% and ranks first on the final leaderboard of the AICovidVN 115M Challenge. Our code is publicly available.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">nguyen2023fruit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fruit-CoV: An efficient vision-based framework for speedy detection and diagnosis of SARS-CoV-2 infections through recorded cough sounds}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Long H. and Pham, Nhat Truong and Do, Van Huong and Nguyen, Liu Tai and Nguyen, Thanh Tin and Nguyen, Hai and Nguyen, Ngoc Duy and Nguyen, Thanh Thi and Nguyen, Sy Dzung and Bhatti, Asim and Lim, Chee Peng}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Expert Systems with Applications}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{213}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{119212}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.eswa.2022.119212}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://onlinelibrary.wiley.com/journal/1096987x" rel="external nofollow noopener" target="_blank">JCC</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mVina.png" sizes="200px"> <img src="/assets/img/publication_preview/mVina.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mVina.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="pham2022improving" class="col-sm-8"> <div class="title">Improving ligand-ranking of AutoDock Vina by changing the empirical parameters</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=_aQ5P4gAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">T Ngoc Han Pham</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=a4xyHScAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Trung Hai Nguyen</a> ,¬†Nguyen Minh Tam ,¬†Thien Y. Vu ,¬†<em><b style="color: #FFA500;">Nhat Truong Pham</b></em> ,¬†Nguyen Truong Huy ,¬†Binh Khanh Mai ,¬†Nguyen Thanh Tung ,¬†Minh Quan Pham ,¬†Van V. Vu ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=_VxSvQkAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Son Tung Ngo</a> </div> <div class="periodical"> <em><i><a href="https://onlinelibrary.wiley.com/journal/1096987x" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">Journal of Computational Chemistry</a></i></em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.26779" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/nhattruongpham/mvina" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1002/jcc.26779" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:t6usbXjVLHcC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-41-4285F4?logo=googlescholar&amp;labelColor=beige" alt="41 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>AutoDock Vina (Vina) achieved a very high docking-success rate, <i>pÃÇ</i>, but give a rather low correlation coefficient, <i>R</i>, for binding affinity with respect to experiments. This low correlation can be an obstacle for ranking of ligand-binding affinity, which is the main objective of docking simulations. In this context, we evaluated the dependence of Vina <i>R</i> coefficient upon its empirical parameters. <i>R</i> is affected more by changing the gauss2 and rotation than other terms. The docking-success rate <i>pÃÇ</i> is sensitive to the alterations of the gauss1, gauss2, repulsion, and hydrogen bond parameters. Based on our benchmarks, the parameter set1 has been suggested to be the most optimal. The testing study over 800 complexes indicated that the modified Vina provided higher correlation with experiment <i>R<sub>set1</sub>=0.556¬±0.025</i> compared with <i>R<sub>Default</sub>=0.493¬±0.028</i> obtained by the original Vina and <i>R<sub>Vina 1.2</sub>=0.503¬±0.029</i> by Vina version 1.2. Besides, the modified Vina can be also applied more widely, giving <i>R ‚â• 0.500</i> for 32/48 targets, compared with the default package, giving <i>R ‚â• 0.500</i> for 31/48 targets. In addition, validation calculations for 1036 complexes obtained from version 2019 of PDBbind refined structures showed that the set1 of parameters gave higher correlation coefficient (<i>R<sub>set1</sub>=0.617¬±0.017</i>) than the default package (<i>R<sub>Default</sub>=0.543¬±0.020</i>) and Vina version 1.2 (<i>R<sub>Vina 1.2</sub>=0.540¬±0.020</i>). The version of Vina with set1 of parameters can be downloaded at <a href="https://github.com/sontungngo/mvina/" rel="external nofollow noopener" target="_blank">https://github.com/sontungngo/mvina/</a>. The outcomes would enhance the ranking of ligand-binding affinity using Autodock Vina.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2022improving</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improving ligand-ranking of AutoDock Vina by changing the empirical parameters}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, T Ngoc Han and Nguyen, Trung Hai and Tam, Nguyen Minh and Y. Vu, Thien and Pham, Nhat Truong and Huy, Nguyen Truong and Mai, Binh Khanh and Tung, Nguyen Thanh and Pham, Minh Quan and V. Vu, Van and Ngo, Son Tung}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Computational Chemistry}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{43}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{160--169}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Wiley Online Library}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1002/jcc.26779}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#600"> <a href="https://cis.ieee.org/publications/t-fuzzy-systems" rel="external nofollow noopener" target="_blank">IEEE TFS</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fRiskC.png" sizes="200px"> <img src="/assets/img/publication_preview/fRiskC.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fRiskC.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="nguyen2022determination" class="col-sm-8"> <div class="title">Determination of the optimal number of clusters: a fuzzy-set based method</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=XFUxOkoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen</a> ,¬†Vu Song Thuy Nguyen ,¬†and¬†<em><b style="color: #FFA500;">Nhat Truong Pham</b></em> </div> <div class="periodical"> <em><i><a href="https://cis.ieee.org/publications/t-fuzzy-systems" style="color: #FF3636; text-decoration: underline dashed;" rel="external nofollow noopener" target="_blank">IEEE Transactions on Fuzzy Systems</a></i></em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/9562269" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/TFUZZ.2021.3118113" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:pyW8ca7W8N0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-22-4285F4?logo=googlescholar&amp;labelColor=beige" alt="22 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The optimal number of clusters (<i>C<sub>opt</sub></i>) is one of the determinants of clustering efficiency. In this article, we present a new method of quantifying <i>C<sub>opt</sub></i> for centroid-based clustering. First, we propose a new clustering validity index named fRisk(<i>C</i>) based on the fuzzy set theory. It takes the role of normalization and accumulation of local risks coming from each action either splitting data from a cluster or merging data into a cluster. fRisk(<i>C</i>) exploits the local distribution information of the database to catch the global information of the clustering process in the form of the risk degree. Based on the monotonous reduction property of fRisk(<i>C</i>), which is proved theoretically, we present a fRisk-based new algorithm named fRisk4-bA for determining <i>C<sub>opt</sub></i>. In the algorithm, the well-known L-method is employed as a supplemented tool to catch <i>C<sub>opt</sub></i> on the graph of the fRisk(<i>C</i>). Along with the stable convergence trend of the method to be proved theoretically, numerical surveys are also carried out. The surveys show that the high reliability and stability, as well as the sensitivity in separating/merging clusters in high-density areas, even if the presence of noise in the databases, are the strong points of the proposed method.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">nguyen2022determination</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Determination of the optimal number of clusters: a fuzzy-set based method}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Sy Dzung and Nguyen, Vu Song Thuy and Pham, Nhat Truong}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Fuzzy Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3514--3526}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TFUZZ.2021.3118113}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <hr> <hr> </div> <h2> <a href="/publications/" style="color: inherit">Selected Preprints</a> </h2> <div class="publications"> <p> (‚Ä†) denotes equal contribution </p> <p> (*) denotes correspondance </p> <p> <span style="display: inline-block; width: 15px; height: 15px; background-color: #0d93bf;"></span> denotes preprint </p> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0d93bf"> <div>Preprint</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/RSS_RM.png" sizes="200px"> <img src="/assets/img/publication_preview/RSS_RM.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="RSS_RM.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="yang2025advances" class="col-sm-8"> <div class="title">Advances in RNA secondary structure prediction and RNA modifications: Methods, data, and applications</div> <div class="author"> Shu Yang<sup>‚Ä†</sup> ,¬†<em><b style="color: #FFA500;">Nhat Truong Pham<sup>‚Ä†</sup></b></em> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=tROyBasAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Ziyang Li<sup>‚Ä†</sup></a> ,¬†Jae Young Baik<sup>‚Ä†</sup> ,¬†Joseph Lee ,¬†Tianhua Zhai ,¬†Weicheng Yu ,¬†Bojian Hou ,¬†Tianqi Shang ,¬†Weiqing He ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=m_qvsgoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Duy Duong-Tran</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=fmsV6nEAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Mayur Naik</a> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=QnWpiskAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Li Shen</a> </div> <div class="periodical"> <em><i><a href="" style="color: #FF3636; text-decoration: underline dashed;"></a></i></em> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2501.04056" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2501.04056" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:SAZ1SQo2q1kC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Due to the hierarchical organization of RNA structures and their pivotal roles in fulfilling RNA functions, the formation of RNA secondary structure critically influences many biological processes and has thus been a crucial research topic. This review sets out to explore the computational prediction of RNA secondary structure and its connections to RNA modifications, which have emerged as an active domain in recent years. We first examine the progression of RNA secondary structure prediction methodology, focusing on a set of representative works categorized into thermodynamic, comparative, machine learning, and hybrid approaches. Next, we survey the advances in RNA modifications and computational methods for identifying RNA modifications, focusing on the prominent modification types. Subsequently, we highlight the interplay between RNA modifications and secondary structures, emphasizing how modifications such as m6A dynamically affect RNA folding and vice versa. In addition, we also review relevant data sources and provide a discussion of current challenges and opportunities in the field. Ultimately, we hope our review will be able to serve as a cornerstone to aid in the development of innovative methods for this emerging topic and foster therapeutic applications in the future.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yang2025advances</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Advances in RNA secondary structure prediction and RNA modifications: Methods, data, and applications}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Shu and Pham, Nhat Truong and Li, Ziyang and Baik, Jae Young and Lee, Joseph and Zhai, Tianhua and Yu, Weicheng and Hou, Bojian and Shang, Tianqi and He, Weiqing and Duong-Tran, Duy and Naik, Mayur and Shen, Li}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0d93bf"> <div>Preprint</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/QTSeg.png" sizes="200px"> <img src="/assets/img/publication_preview/QTSeg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="QTSeg.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tran2024qtseg" class="col-sm-8"> <div class="title"> <b><i>QTSeg:</i></b> A Query Token-Based Dual-Mix Attention Framework with Multi-Level Feature Distribution for Medical Image Segmentation</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=NKbwDD8AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Phuong-Nam Tran</a> ,¬†<em><b style="color: #FFA500;">Nhat Truong Pham</b></em> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=2UKP440AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0ATpZl0AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Eui-Nam Huh</a> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=oKANWloAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Choong Seon Hong</a> </div> <div class="periodical"> <em><i><a href="" style="color: #FF3636; text-decoration: underline dashed;"></a></i></em> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2412.17241" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2412.17241" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/tpnam0901/QTSeg/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:qE4H1tSSYIIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Medical image segmentation plays a crucial role in assisting healthcare professionals with accurate diagnoses and enabling automated diagnostic processes. Traditional convolutional neural networks (CNNs) often struggle with capturing long-range dependencies, while transformer-based architectures, despite their effectiveness, come with increased computational complexity. Recent efforts have focused on combining CNNs and transformers to balance performance and efficiency, but existing approaches still face challenges in achieving high segmentation accuracy while maintaining low computational costs. Furthermore, many methods underutilize the CNN encoder‚Äôs capability to capture local spatial information, concentrating primarily on mitigating long-range dependency issues. To address these limitations, we propose QTSeg, a novel architecture for medical image segmentation that effectively integrates local and global information. QTSeg features a dual-mix attention decoder designed to enhance segmentation performance through: (1) a cross-attention mechanism for improved feature alignment, (2) a spatial attention module to capture long-range dependencies, and (3) a channel attention block to learn inter-channel relationships. Additionally, we introduce a multi-level feature distribution module, which adaptively balances feature propagation between the encoder and decoder, further boosting performance. Extensive experiments on five publicly available datasets covering diverse segmentation tasks, including lesion, polyp, breast cancer, cell, and retinal vessel segmentation, demonstrate that QTSeg outperforms state-of-the-art methods across multiple evaluation metrics while maintaining lower computational costs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tran2024qtseg</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{QTSeg: A Query Token-Based Dual-Mix Attention Framework with Multi-Level Feature Distribution for Medical Image Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tran, Phuong-Nam and Pham, Nhat Truong and Dang, Duc Ngoc Minh and Huh, Eui-Nam and Hong, Choong Seon}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0d93bf"> <div>Preprint</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/MMF_SER_Review.png" sizes="200px"> <img src="/assets/img/publication_preview/MMF_SER_Review.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="MMF_SER_Review.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="nguyen5063214multi" class="col-sm-8"> <div class="title">Multi-modal fusion in speech emotion recognition: A comprehensive review of methods and technologies</div> <div class="author"> Nhut Minh Nguyen ,¬†Thanh Trung Nguyen ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=NKbwDD8AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Phuong-Nam Tran</a> ,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=an_4IJkAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Chee Peng Lim</a> ,¬†<em><b style="color: #FFA500;">Nhat Truong Pham</b></em> ,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=2UKP440AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a> </div> <div class="periodical"> <em><i><a href="" style="color: #FF3636; text-decoration: underline dashed;"></a></i></em> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5063214" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">SSRN</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/aita-lab/awesome-multimodal-fusion-emotion-recognition/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=HybH2XkAAAAJ&amp;citation_for_view=HybH2XkAAAAJ:qE4H1tSSYIIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Speech emotion recognition (SER) plays a crucial role in human-computer interaction, enhancing numerous applications such as virtual assistants, healthcare monitoring, and customer support by identifying and interpreting emotions conveyed through spoken language. While single-modality SER systems demonstrate notable simplicity and computational efficiency, excelling in extracting critical features like vocal prosody and linguistic content, there is a pressing need to improve their performance in challenging conditions, such as noisy environments and the handling of ambiguous expressions or incomplete information. These challenges underscore the necessity of transitioning to multi-modal approaches, which integrate complementary data sources to achieve more robust and accurate emotion detection. With advancements in artificial intelligence, especially in neural networks and deep learning, many studies have employed advanced deep learning and feature fusion techniques to enhance SER performance. This review synthesizes comprehensive publications from 2020 to 2024, exploring prominent multi-modal fusion strategies, including early fusion, late fusion, deep fusion, and hybrid fusion methods, while also examining data representation, data translation, attention mechanisms, and graph-based fusion technologies. We assess the effectiveness of various fusion techniques across standard SER datasets, highlighting their performance in diverse tasks and addressing challenges related to data alignment, noise management, and computational demands. Additionally, we explore potential future directions for enhancing multi-modal SER systems, emphasizing scalability and adaptability in real-world applications. This survey aims to contribute to the advancement of multi-modal SER and to inform researchers about effective fusion strategies for developing more responsive and emotion-aware systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">nguyen5063214multi</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-modal fusion in speech emotion recognition: A comprehensive review of methods and technologies}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Nhut Minh and Nguyen, Thanh Trung and Tran, Phuong-Nam and Lim, Chee Peng and Pham, Nhat Truong and Dang, Duc Ngoc Minh}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <hr> <hr> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%70%68%61%6D%6E%68%61%74%74%72%75%6F%6E%67.%73%6B%79%6F@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=HybH2XkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://orcid.org/0000-0002-8086-6722" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://www.webofscience.com/wos/author/record/ABF-7566-2021" title="Clarivate" rel="external nofollow noopener" target="_blank"><i class="ai ai-clarivate"></i></a> <a href="https://www.scopus.com/authid/detail.uri?authorId=57243980400" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus"></i></a> <a href="https://www.researchgate.net/profile/Nhat-Truong-Pham/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://dblp.org/pid/290/9204.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a href="https://www.semanticscholar.org/author/2077529712" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://dl.acm.org/profile/99660713417/" title="ACM DL" rel="external nofollow noopener" target="_blank"><i class="ai ai-acm"></i></a> <a href="https://ieeexplore.ieee.org/author/37089489125/" title="IEEE Xplore" rel="external nofollow noopener" target="_blank"><i class="ai ai-ieee"></i></a> <a href="https://www.linkedin.com/in/nhattruongpham" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/nhattruong_pham" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://github.com/nhattruongpham" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.kaggle.com/nhattruongpham" title="Kaggle" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-kaggle"></i></a> <a href="https://www.hackerrank.com/profile/nhattruong_pham" title="HackerRank" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-hackerrank"></i></a> <a href="https://discord.com/users/673453370991837200" title="Discord" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-discord"></i></a> <a href="https://www.zotero.org/nhattruongpham" title="Zotero" rel="external nofollow noopener" target="_blank"><i class="ai ai-zotero"></i></a> </div> <div class="contact-note"></div> </div> <div style="width: 150px; margin: 0 auto"> <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=RuxfKWte4G6yKP1jx6GjTKLxpqdoMdFm-CPzvceTJNY"></script> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2023 - 2025 Nhat Truong Pham. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-ND 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nd.svg?ref=chooser-v1" alt=""></a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-home",title:"Home",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-news",title:"News",description:"",section:"Navigation",handler:()=>{window.location.href="/news/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-collaborators",title:"Collaborators",description:"",section:"Navigation",handler:()=>{window.location.href="/collaborators/"}},{id:"nav-mentees",title:"Mentees",description:"",section:"Navigation",handler:()=>{window.location.href="/mentees/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-molecules",title:"Molecules",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-dna",title:"DNA",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-rna",title:"RNA",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-peptide",title:"Peptide",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-protein",title:"Protein",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-microbiome",title:"Microbiome",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-imaging",title:"Imaging",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-skku-rs-24",title:"SKKU-RS-24",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-ijcai-24",title:"IJCAI-24",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-acl-24",title:"ACL-24",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"news-one-collaborative-manuscript-entitled-drugormerdti-drug-graphormer-for-drug-target-interaction-prediction-has-been-accepted-for-publication-in-the-computers-in-biology-and-medicine-journal",title:"One collaborative manuscript entitled \u201cDrugormerDTI: Drug Graphormer for drug\u2013target interaction prediction\u201d has been...",description:"",section:"News"},{id:"news-one-manuscript-entitled-an-exploratory-simulation-study-and-prediction-model-on-human-brain-behavior-and-activity-using-an-integration-of-deep-neural-network-and-biosensor-rabi-antenna-has-been-accepted-for-publication-in-the-heliyon-journal",title:"One manuscript entitled \u201cAn exploratory simulation study and prediction model on human brain...",description:"",section:"News"},{id:"news-one-collaborative-manuscript-entitled-towards-an-efficient-machine-learning-model-for-financial-time-series-forecasting-has-been-accepted-for-publication-in-the-soft-computing-journal",title:"One collaborative manuscript entitled \u201cTowards an efficient machine learning model for financial time...",description:"",section:"News"},{id:"news-one-manuscript-entitled-hybrid-data-augmentation-and-deep-attention-based-dilated-convolutional-recurrent-neural-networks-for-speech-emotion-recognition-has-been-accepted-for-publication-in-the-expert-systems-with-applications-journal",title:"One manuscript entitled \u201cHybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks...",description:"",section:"News"},{id:"news-released-new-officially-personal-website",title:"Released new officially personal website",description:"",section:"News"},{id:"news-one-collaborative-manuscript-entitled-adp-fuse-a-novel-two-layer-machine-learning-predictor-to-identify-antidiabetic-peptides-and-diabetes-types-using-multiview-information-has-been-accepted-for-publication-in-the-computers-in-biology-and-medicine-journal",title:"One collaborative manuscript entitled \u201cADP-Fuse: A novel two-layer machine learning predictor to identify...",description:"",section:"News"},{id:"news-one-collaborative-manuscript-entitled-comparative-analysis-of-multi-loss-functions-for-enhanced-multi-modal-speech-emotion-recognition-has-been-accepted-for-publication-in-the-2023-14th-international-conference-on-information-and-communication-technology-convergence-ictc",title:"One collaborative manuscript entitled \u201cComparative analysis of multi-loss functions for enhanced multi-modal speech...",description:"",section:"News"},{id:"news-one-manuscript-entitled-ser-fuse-an-emotion-recognition-application-utilizing-multi-modal-multi-lingual-and-multi-feature-fusion-has-been-accepted-for-publication-in-the-proceedings-of-the-12th-international-symposium-on-information-and-communication-technology",title:"One manuscript entitled \u201cSER-Fuse: An Emotion Recognition Application Utilizing Multi-Modal, Multi-Lingual, and Multi-Feature...",description:"",section:"News"},{id:"news-one-of-my-very-first-first-author-manuscripts-in-the-field-of-computational-biology-and-bioinformatics-entitled-advancing-the-accuracy-of-sars-cov-2-phosphorylation-site-detection-via-meta-learning-approach-has-been-accepted-for-publication-in-the-briefings-in-bioinformatics-journal",title:"One of my very first first-author manuscripts in the field of Computational Biology...",description:"",section:"News"},{id:"news-our-project-entitled-identification-of-human-2-o-methylation-2om-sites-using-a-hybrid-deep-learning-framework-\ud558\uc774\ube0c\ub9ac\ub4dc-\ub525-\ub7ec\ub2dd-\ud504\ub808\uc784\uc6cc\ud06c\ub97c-\uc0ac\uc6a9\ud55c-\uc778\uac04-2-o-\uba54\ud2f8\ud654-2om-\ubd80\uc704-\uc2dd\ubcc4-has-been-selected-for-the-2023-2nd-k-bds-analysis-infrastructure-utilization-support-program-track-i-large-innovation-research-by-the-korea-bio-data-station-k-bds-korea-institute-of-science-and-technology-information-republic-of-korea",title:"Our project, entitled \u201cIdentification of human 2\u2019-O-methylation (2OM) sites using a hybrid deep...",description:"",section:"News"},{id:"news-two-collaborative-manuscripts-entitled-deep-learning-based-automated-cashier-system-for-bakeries-and-innovative-multi-modal-control-for-surveillance-spider-robot-an-integration-of-voice-and-hand-gesture-recognition-have-been-accepted-for-publication-in-proceedings-of-the-2024-9th-international-conference-on-intelligent-information-technology",title:"Two collaborative manuscripts entitled \u201cDeep Learning-Based Automated Cashier System for Bakeries\u201d and \u201cInnovative...",description:"",section:"News"},{id:"news-one-collaborative-manuscript-entitled-enhanced-sliding-mode-controller-design-via-meta-heuristic-algorithm-for-robust-and-stable-load-frequency-control-in-multi-area-power-systems-has-been-accepted-for-publication-in-the-iet-generation-transmission-amp-amp-distribution-journal",title:"One collaborative manuscript entitled \u201cEnhanced sliding mode controller design via meta-heuristic algorithm for...",description:"",section:"News"},{id:"news-one-manuscript-entitled-h2opred-a-robust-and-efficient-hybrid-deep-learning-model-for-predicting-2-o-methylation-sites-in-human-rna-has-been-accepted-for-publication-in-the-briefings-in-bioinformatics-journal",title:"One manuscript entitled \u201cH2Opred: a robust and efficient hybrid deep learning model for...",description:"",section:"News"},{id:"news-our-paper-entitled-advancing-the-accuracy-of-sars-cov-2-phosphorylation-site-detection-via-meta-learning-approach-recently-published-in-the-briefings-in-bioinformatics-journal-has-been-cataloged-in-hanbitsa-paper-of-the-biological-research-information-center-bric",title:"Our paper, entitled \u201cAdvancing the accuracy of SARS-CoV-2 phosphorylation site detection via meta-learning...",description:"",section:"News"},{id:"news-my-hanbitsa-interview-regarding-the-recently-published-paper-in-the-briefings-in-bioinformatics-journal-has-been-shared-on-the-biological-research-information-center-bric",title:"My Hanbitsa Interview regarding the recently published paper in the Briefings in Bioinformatics...",description:"",section:"News"},{id:"news-our-paper-entitled-h2opred-a-robust-and-efficient-hybrid-deep-learning-model-for-predicting-2-o-methylation-sites-in-human-rna-recently-published-in-the-briefings-in-bioinformatics-journal-has-been-cataloged-in-hanbitsa-paper-of-the-biological-research-information-center-bric",title:"Our paper, entitled \u201cH2Opred: a robust and efficient hybrid deep learning model for...",description:"",section:"News"},{id:"news-our-research-story-kor-eng-regarding-the-recently-published-papers-in-the-briefings-in-bioinformatics-journal-has-been-highlighted-on-skku-s-research-stories-kor-eng",title:"Our Research Story (KOR/ENG) regarding the recently published papers in the Briefings in...",description:"",section:"News"},{id:"news-our-project-entitled-enhancing-peptide-hla-class-i-binding-prediction-through-siamese-network-based-contrastive-learning-framework-with-feature-fusion-\ud3a9\ud0c0\uc774\ub4dc-hla-class-i-\uacb0\ud569-\uc608\uce21-\uac15\ud654\ub97c-\uc704\ud55c-\ud2b9\uc9d5-\uc735\ud569-\uae30\ubc18-\uc2dc\uc554-\ub124\ud2b8\uc6cc\ud06c-\ub300\uc870-\ud559\uc2b5-\ud504\ub808\uc784\uc6cc\ud06c-has-been-selected-for-the-2024-1st-k-bds-analysis-infrastructure-utilization-support-program-track-i-large-innovation-research-by-the-korea-bio-data-station-k-bds-korea-institute-of-science-and-technology-information-republic-of-korea",title:"Our project, entitled \u201cEnhancing Peptide-HLA Class I Binding Prediction through Siamese Network-Based Contrastive...",description:"",section:"News"},{id:"news-our-project-entitled-enhancing-m6a-rna-modification-prediction-across-cell-lines-and-tissues-using-biological-language-models-and-a-contrastive-learning-framework-has-been-selected-for-the-high-performance-computing-support-project-by-the-korea-association-for-ict-promotion-kait-republic-of-korea",title:"Our project, entitled \u201cEnhancing m6A RNA modification prediction across cell lines and tissues...",description:"",section:"News"},{id:"news-one-manuscript-entitled-ac4c-afl-a-high-precision-identification-of-human-mrna-n4-acetylcytidine-sites-based-on-adaptive-feature-representation-learning-has-been-accepted-for-publication-in-the-molecular-therapy-nucleic-acids-journal",title:"One manuscript entitled \u201cac4C-AFL: A high-precision identification of human mRNA N4-acetylcytidine sites based...",description:"",section:"News"},{id:"news-one-collaborative-manuscript-entitled-meta-2om-a-multi-classifier-meta-model-for-the-accurate-prediction-of-rna-2-o-methylation-sites-in-human-rna-has-been-accepted-for-publication-in-the-plos-one-journal",title:"One collaborative manuscript entitled \u201cMeta-2OM: A multi-classifier meta-model for the accurate prediction of...",description:"",section:"News"},{id:"news-one-collaborative-manuscript-entitled-predicting-drought-stress-under-climate-change-in-the-southern-central-highlands-of-vietnam-has-been-accepted-for-publication-in-the-environmental-monitoring-and-assessment-journal",title:"One collaborative manuscript entitled \u201cPredicting drought stress under climate change in the Southern...",description:"",section:"News"},{id:"news-one-collaborative-manuscript-entitled-sep-algpro-an-efficient-allergen-prediction-tool-utilizing-traditional-machine-learning-and-deep-learning-techniques-with-protein-language-model-features-has-been-accepted-for-publication-in-the-international-journal-of-biological-macromolecules",title:"One collaborative manuscript entitled \u201cSEP-AlgPro: An efficient allergen prediction tool utilizing traditional machine...",description:"",section:"News"},{id:"news-one-manuscript-entitled-macppred-2-0-stacked-deep-learning-for-anticancer-peptide-prediction-with-integrated-spatial-and-probabilistic-feature-representations-has-been-accepted-for-publication-in-the-journal-of-molecular-biology",title:"One manuscript entitled \u201cmACPpred 2.0: Stacked Deep Learning for Anticancer Peptide Prediction with...",description:"",section:"News"},{id:"news-our-project-entitled-predicting-twelve-common-rna-modification-sites-using-advanced-deep-learning-frameworks-with-conventional-and-pre-trained-language-model-features-\uae30\uc874-\ubc0f-\uc0ac\uc804-\ud559\uc2b5\ub41c-\uc5b8\uc5b4-\ubaa8\ub378-\uae30\ubc18-\ud2b9\uc9d5\uc744-\uc0ac\uc6a9\ud558\uc5ec-\uace0\uae09-\ub525\ub7ec\ub2dd-\ud504\ub808\uc784\uc6cc-\ud06c\ub85c-\uc5f4\ub450-\uac1c\uc758-\uc77c\ubc18\uc801\uc778-rna-\uc218\uc815-\ubd80\uc704-\uc608\uce21-has-been-selected-for-the-2024-2nd-k-bds-analysis-infrastructure-utilization-support-program-track-i-large-innovation-research-by-the-korea-bio-data-station-k-bds-korea-institute-of-science-and-technology-information-republic-of-korea",title:"Our project, entitled \u201cPredicting Twelve Common RNA Modification Sites Using Advanced Deep Learning...",description:"",section:"News"},{id:"news-one-manuscript-entitled-hotgpred-enhancing-human-o-linked-threonine-glycosylation-prediction-using-integrated-pretrained-protein-language-model-based-features-and-multi-stage-feature-selection-approach-has-been-accepted-for-publication-in-the-computers-in-biology-and-medicine-journal",title:"One manuscript entitled \u201cHOTGpred: Enhancing human O-linked threonine glycosylation prediction using integrated pretrained...",description:"",section:"News"},{id:"news-two-manuscripts-entitled-mol2lang-vlm-vision-and-text-guided-generative-pre-trained-language-models-for-advancing-molecule-captioning-through-multimodal-fusion-and-lang2mol-diff-a-diffusion-based-generative-model-for-language-to-molecule-translation-leveraging-selfies-representation-have-been-accepted-for-publication-in-the-proceedings-of-the-1st-workshop-on-language-molecules-l-m-2024",title:"Two manuscripts entitled \u201cMol2Lang-VLM: Vision- and Text-Guided Generative Pre-trained Language Models for Advancing...",description:"",section:"News"},{id:"news-two-collaborative-manuscripts-entitled-federated-learning-with-u-net-for-brain-tumor-segmentation-impact-of-client-numbers-and-data-distribution-and-towards-real-time-vietnamese-traffic-sign-recognition-on-embedded-systems-have-been-accepted-for-publication-in-the-2024-15th-international-conference-on-information-and-communication-technology-convergence-ictc",title:"Two collaborative manuscripts entitled \u201cFederated Learning with U-Net for Brain Tumor Segmentation: Impact...",description:"",section:"News"},{id:"news-our-project-entitled-accurate-liver-pathology-identification-via-dual-stream-graph-convolutional-networks-\uc774\uc911-\uc2a4\ud2b8\ub9bc-\uadf8\ub798\ud504-\ud569\uc131\uacf1-\uc2e0\uacbd\ub9dd\uc744-\ud1b5\ud55c-\uc815\ud655\ud55c-\uac04-\ubcd1\ub9ac-\uc2dd\ubcc4-has-been-selected-for-the-2024-3rd-k-bds-analysis-infrastructure-utilization-support-program-track-i-large-innovation-research-by-the-korea-bio-data-station-k-bds-korea-institute-of-science-and-technology-information-republic-of-korea",title:"Our project, entitled \u201cAccurate Liver Pathology Identification via Dual-stream Graph Convolutional Networks (\uc774\uc911...",description:"",section:"News"},{id:"news-one-collaborative-manuscript-entitled-hubert-clap-contrastive-learning-based-multimodal-emotion-recognition-using-self-alignment-approach-has-been-accepted-for-publication-in-the-mmasia-24-proceedings-of-the-6th-acm-international-conference-on-multimedia-in-asia",title:"One collaborative manuscript entitled \u201cHuBERT-CLAP: Contrastive Learning-Based Multimodal Emotion Recognition using Self-Alignment Approach\u201d...",description:"",section:"News"},{id:"news-one-manuscript-entitled-mst-m6a-a-novel-multi-scale-transformer-based-framework-for-accurate-prediction-of-m6a-modification-sites-across-diverse-cellular-contexts-has-been-accepted-for-publication-in-the-journal-of-molecular-biology",title:"One manuscript entitled \u201cMST-m6A: A Novel Multi-Scale Transformer-based Framework for Accurate Prediction of...",description:"",section:"News"},{id:"news-successfully-completed-the-preliminary-defense-for-my-ph-d-dissertation-and-became-a-ph-d-candidate",title:"Successfully completed the preliminary defense for my Ph.D. dissertation and became a Ph.D....",description:"",section:"News"},{id:"news-one-manuscript-entitled-leveraging-deep-transfer-learning-and-explainable-ai-for-accurate-covid-19-diagnosis-insights-from-a-multi-national-chest-ct-scan-study-has-been-accepted-for-publication-in-the-computers-in-biology-and-medicine-journal",title:"One manuscript entitled \u201cLeveraging deep transfer learning and explainable AI for accurate COVID-19...",description:"",section:"News"},{id:"news-received-the-skku-bk21-innovative-research-scholarship-\uad50\uc721\uc5f0\uad6c\ub2e8-\uc7a5\ud559\ud601\uc2e0-skku-bk21-\ud601\uc2e0\uc5f0\uad6c-\uc7a5\ud559\uae08",title:"Received the SKKU BK21 Innovative Research Scholarship (\uad50\uc721\uc5f0\uad6c\ub2e8 \uc7a5\ud559\ud601\uc2e0 (SKKU BK21 \ud601\uc2e0\uc5f0\uad6c \uc7a5\ud559\uae08))...",description:"",section:"News"},{id:"news-one-preprint-entitled-multi-modal-fusion-in-speech-emotion-recognition-a-comprehensive-review-of-methods-and-technologies-has-been-submitted-to-ssrn",title:"One preprint entitled \u201cMulti-modal fusion in speech emotion recognition: A comprehensive review of...",description:"",section:"News"},{id:"news-one-preprint-entitled-qtseg-a-query-token-based-architecture-for-efficient-2d-medical-image-segmentation-has-been-submitted-to-arxiv",title:"One preprint entitled \u201cQTSeg: A Query Token-Based Architecture for Efficient 2D Medical Image...",description:"",section:"News"},{id:"news-one-preprint-entitled-advances-in-rna-secondary-structure-prediction-and-rna-modifications-methods-data-and-applications-has-been-submitted-to-arxiv",title:"One preprint entitled \u201cAdvances in RNA secondary structure prediction and RNA modifications: Methods,...",description:"",section:"News"},{id:"news-one-collaborative-manuscript-entitled-enhancing-daily-runoff-prediction-in-hydropower-basins-with-a-voting-ensemble-model-using-historical-data-has-been-accepted-for-publication-in-the-hydrological-sciences-journal",title:"One collaborative manuscript entitled \u201cEnhancing daily runoff prediction in hydropower basins with a...",description:"",section:"News"},{id:"news-one-collaborative-manuscript-entitled-hydraulic-performance-and-wave-transmission-through-nature-inspired-perforated-hollow-base-piles-breakwater-has-been-accepted-for-publication-in-the-ocean-engineering-journal",title:"One collaborative manuscript entitled \u201cHydraulic performance and wave transmission through nature-inspired perforated hollow-base...",description:"",section:"News"},{id:"news-one-manuscript-entitled-dogpred-a-novel-deep-learning-framework-for-accurate-identification-of-human-o-linked-threonine-glycosylation-sites-has-been-accepted-for-publication-in-the-journal-of-molecular-biology",title:"One manuscript entitled \u201cDOGpred: A Novel Deep Learning Framework for Accurate Identification of...",description:"",section:"News"},{id:"news-one-collaborative-manuscript-entitled-memocmt-multimodal-emotion-recognition-using-cross-modal-transformer-based-feature-fusion-has-been-accepted-for-publication-in-the-scientific-reports-journal",title:"One collaborative manuscript entitled \u201cMemoCMT: multimodal emotion recognition using cross-modal transformer-based feature fusion\u201d...",description:"",section:"News"},{id:"news-received-the-caregen-chung-yong-ji-scholarship-\ucf00\uc5b4\uc820\uc815\uc6a9\uc9c0\uc7a5\ud559\uae08-\uc77c\ubc18\uc6d0",title:"Received the Caregen Chung Yong-ji Scholarship (\ucf00\uc5b4\uc820\uc815\uc6a9\uc9c0\uc7a5\ud559\uae08(\uc77c\ubc18\uc6d0))",description:"",section:"News"},{id:"news-received-the-university-innovation-awards-and-living-expenses-\ub300\ud559\ud601\uc2e0-\ud3ec\uc0c1\ubc0f\uc0dd\ud65c\ube44-\uc77c\ubc18\uc6d0",title:"Received the University Innovation - Awards and Living Expenses (\ub300\ud559\ud601\uc2e0-\ud3ec\uc0c1\ubc0f\uc0dd\ud65c\ube44(\uc77c\ubc18\uc6d0))",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%70%68%61%6D%6E%68%61%74%74%72%75%6F%6E%67.%73%6B%79%6F@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0002-8086-6722","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=HybH2XkAAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/2077529712","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Nhat-Truong-Pham/","_blank")}},{id:"socials-ieee-xplore",title:"IEEE Xplore",section:"Socials",handler:()=>{window.open("https://ieeexplore.ieee.org/author/37089489125/","_blank")}},{id:"socials-acm-dl",title:"ACM DL",section:"Socials",handler:()=>{window.open("https://dl.acm.org/profile/99660713417/","_blank")}},{id:"socials-scopus",title:"Scopus",section:"Socials",handler:()=>{window.open("https://www.scopus.com/authid/detail.uri?authorId=57243980400","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/nhattruongpham","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/nhattruongpham","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/nhattruong_pham","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/290/9204.html","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/nhattruongpham","_blank")}},{id:"socials-discord",title:"Discord",section:"Socials",handler:()=>{window.open("https://discord.com/users/673453370991837200","_blank")}},{id:"socials-zotero",title:"Zotero",section:"Socials",handler:()=>{window.open("https://www.zotero.org/nhattruongpham","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>