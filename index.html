<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>üß¨Nhat Truong Phamüß¨</title> <meta name="author" content="üß¨Nhat Truong Phamüß¨"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha256-1oz1+rx2BF/9OiBUPSh1NEUwyUECNU5O70RoKnClGNs" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://nhattruongpham.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">üß¨Nhat Truong</span> Phamüß¨ </h1> <p class="desc">Ph.D. Student in Integrative Biotechnology specializing in Computational Biology and Bioinformatics</p> </header> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-1400.webp"></source> <img src="/assets/img/profile.png" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="profile.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>Room 62105, Biotechnology and Bioengineering Building 2, Seobu-ro, Jagan-gu, Suwon-si, Gyeonggi-do, 16419, Republic of Korea</p> </div> </div> <div class="clearfix"> <p><strong>Short Bio:</strong> My name is Nhat Truong. I am currently a first-year Ph.D. Student at the <a href="https://skb.skku.edu/eng_gene/index.do" rel="external nofollow noopener" target="_blank">Department of Integrative Biotechnology</a>, <a href="https://biotech.skku.edu/eng_biotech/index.do" rel="external nofollow noopener" target="_blank">College of Biotechnology and Bioengineering</a>, <a href="https://www.skku.edu/eng/" rel="external nofollow noopener" target="_blank">Sungkyunkwan University (SKKU)</a>. I am working under the guidance of Assistant Professor <a href="https://skb.skku.edu/eng_gene/faculty.do?mode=view&amp;perId=LZStrB4DgrAqgzgNgwgGQCwGkAuArAigEQE4BCA5gHYBmMAnkQLw1A" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> in the <a href="https://balalab-skku.org/" rel="external nofollow noopener" target="_blank">Computational Biology and Bioinformatics Laboratory</a>. Prior to joining SKKU, I worked as an Assistant Researcher at <a href="https://tdtu.edu.vn/en" rel="external nofollow noopener" target="_blank">Ton Duc Thang University (TDTU)</a>, Vietnam. I completed my M.E. degree in Automation and Control at TDTU, where I was supervised by Dr. <a href="https://sites.google.com/view/nguyensydzung" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen</a> and co-supervised by Dr. <a href="https://sites.google.com/view/dnmduc/" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a>. I also hold a B.E. degree in Electronics and Telecommunication from TDTU, which I obtained in 2019 under the supervision of Dr. <a href="https://sites.google.com/view/dnmduc/" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a>.</p> <p><strong>Research Interests:</strong> Applied Intelligence, Computational Intelligence, Deep Learning, Machine Learning, Signal Processing, XAI &amp; Optimization, and their applications in the fields of Affective Computing, Bioinformatics, Computational Biology and Medicine, Computer-aided Drug Design, Protein Functional and Structural Prediction, and Healthcare.</p> </div> <h2><a href="/news/" style="color: inherit;">Latest News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jul 1, 2023</th> <td> Released new officially personal website </td> </tr> <tr> <th scope="row">May 28, 2023</th> <td> One manuscript entitled ‚ÄúHybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition‚Äù has been accepted in Expert Systems with Applications </td> </tr> <tr> <th scope="row">May 25, 2023</th> <td> One manuscript entitled ‚ÄúTowards an efficient machine learning model for financial time series forecasting‚Äù has been accepted in Soft Computing </td> </tr> <tr> <th scope="row">Apr 20, 2023</th> <td> One manuscript entitled ‚ÄúAn exploratory simulation study and prediction model on human brain behavior and activity using an integration of deep neural network and biosensor Rabi antenna‚Äù has been accepted in Heliyon </td> </tr> <tr> <th scope="row">Apr 15, 2023</th> <td> One manuscript entitled ‚ÄúDrugormerDTI: Drug Graphormer for drug‚Äìtarget interaction prediction‚Äù has been accepted in Computers in Biology and Medicine </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit;">Selected Publications </a><a href="https://nhattruongpham.github.io/publications/">[Full Publications]</a> </h2> <div class="publications"> <p> (*) denotes equal contribution </p> <p> (‚Ä†) denotes correspondancce </p> <p> <span style="display: inline-block; width: 15px; height: 15px; background-color: #600;"></span> denotes journal </p> <p> <span style="display: inline-block; width: 15px; height: 15px; background-color: #215d42;"></span> denotes conference </p> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#600"><a href="">CIBM</a></abbr></div> <div id="hu2023drugormerdti" class="col-sm-8"> <div class="title">DrugormerDTI: Drug Graphormer for drug‚Äìtarget interaction prediction</div> <div class="author"> Jiayue Hu,¬†Wang Yu,¬†Chao Pang,¬†Junru Jin,¬†<em><b>Nhat Truong</b> <b>Pham</b></em>,¬†<a href="https://scholar.google.com/citations?user=0vkenbwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a>,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=0EAV03MAAAAJ" rel="external nofollow noopener" target="_blank">Leyi Wei</a> </div> <div class="periodical"> <em>Computers in Biology and Medicine</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0010482523004110" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/joannacatj/drugormerDTI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.compbiomed.2023.106946"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.compbiomed.2023.106946" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Drug-target interactions (DTI) prediction is a crucial task in drug discovery. Existing computational methods accelerate the drug discovery in this respect. However, most of them suffer from low feature representation ability, significantly affecting the predictive performance. To address the problem, we propose a novel neural network architecture named DrugormerDTI, which uses Graph Transformer to learn both sequential and topological information through the input molecule graph and Resudual2vec to learn the underlying relation between residues from proteins. By conducting ablation experiments, we verify the importance of each part of the DrugormerDTI. We also demonstrate the good feature extraction and expression capabilities of our model via comparing the mapping results of the attention layer and molecular docking results. Experimental results show that our proposed model performs better than baseline methods on four benchmarks. We demonstrate that the introduction of Graph Transformer and the design of residue are appropriate for drug-target prediction.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#600"><a href="">KBS</a></abbr></div> <div id="mustaqeem2023aad" class="col-sm-8"> <div class="title">AAD-Net: Advanced end-to-end signal processing system for human emotion detection &amp; recognition using attention-based deep echo state network</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=uEZhRWAAAAAJ" rel="external nofollow noopener" target="_blank">Khan Mustaqeem</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=VcOjgngAAAAJ" rel="external nofollow noopener" target="_blank">Abdulmotaleb El Saddik</a>,¬†Fahd Saleh Alotaibi,¬†and¬†<em><b>Nhat Truong</b> <b>Pham</b></em> </div> <div class="periodical"> <em>Knowledge-Based Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705123002757" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.knosys.2023.110525"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.knosys.2023.110525" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Speech signals are the most convenient way of communication between human beings and the eventual method of Human-Computer Interaction (HCI) to exchange emotions and information. Recognizing emotions from speech signals is a challenging task due to the sparse nature of emotional data and features. In this article, we proposed a Deep Echo-State-Network (DeepESN) system for emotion recognition with a dilated convolution neural network and multi-headed attention mechanism. To reduce the model complexity, we incorporate a DeepESN that combines reservoir computing for higher-dimensional mapping. We also used fine-tuned Sparse Random Projection (SRP) to reduce dimensionality and adopted an early fusion strategy to fuse the extracted cues and passed the joint feature vector via a classification layer to recognize emotions. Our proposed model is evaluated on two public speech corpora, EMO-DB and RAVDESS, and tested for subject/speaker-dependent/independent performance. The results show that our proposed system achieves a high recognition rate, 91.14, 85.57 for EMO-DB, and 82.01, 77.02 for RAVDESS, using speaker-dependent and independent experiments, respectively. Our proposed system outperforms the State-of-The-Art (SOTA) while requiring less computational time.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#600"><a href="">ESWA</a></abbr></div> <div id="pham2023hybrid" class="col-sm-8"> <div class="title">Hybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition</div> <div class="author"> <em><b>Nhat Truong</b> <b>Pham</b></em>,¬†<a href="https://scholar.google.com/citations?user=2UKP440AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a>,¬†<a href="https://scholar.google.com/citations?user=At-Y-H8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ngoc Duy Nguyen<sub>2</sub></a>,¬†<a href="https://scholar.google.com/citations?user=xr39SOwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Thanh Thi Nguyen<sub>3</sub></a>,¬†<a href="https://scholar.google.com/citations?user=5b9ncWoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Hai Nguyen<sub>4</sub></a>,¬†<a href="https://scholar.google.com/citations?user=0vkenbwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a>,¬†<a href="https://scholar.google.com/citations?user=an_4IJkAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Chee Peng Lim</a>,¬†and¬†<a href="https://scholar.google.com/citations?user=XFUxOkoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen<sub>1</sub></a> </div> <div class="periodical"> <em>Expert Systems with Applications</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0957417423011107" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/nhattruongpham/hda-adcrnn-ser" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.eswa.2023.120608"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.eswa.2023.120608" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Recently, speech emotion recognition (SER) has become an active research area in speech processing, particularly with the advent of deep learning (DL). Numerous DL-based methods have been proposed for SER. However, most of the existing DL-based models are complex and require a large amounts of data to achieve a good performance. In this study, a new framework of deep attention-based dilated convolutional-recurrent neural networks coupled with a hybrid data augmentation method was proposed for addressing SER tasks. The hybrid data augmentation method constitutes an upsampling technique for generating more speech data samples based on the traditional and generative adversarial network approaches. By leveraging both convolutional and recurrent neural networks in a dilated form along with an attention mechanism, the proposed DL framework can extract high-level representations from three-dimensional log Mel spectrogram features. Dilated convolutional neural networks acquire larger receptive fields, whereas dilated recurrent neural networks overcome complex dependencies as well as the vanishing and exploding gradient issues. Furthermore, the loss functions are reconfigured by combining the SoftMax loss and the center-based losses to classify various emotional states. The proposed framework was implemented using the Python programming language and the TensorFlow deep learning library. To validate the proposed framework, the EmoDB and ERC benchmark datasets, which are imbalanced and/or small datasets, were employed. The experimental results indicate that the proposed framework outperforms other related state-of-the-art methods, yielding the highest unweighted recall rates of 88.03 ¬± 1.39 (%) and 66.56 ¬± 0.67 (%) for the EmoDB and ERC datasets, respectively.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#600"><a href="">IJBIOMAC</a></abbr></div> <div id="charoenkwan2023pretoria" class="col-sm-8"> <div class="title">Pretoria: An effective computational approach for accurate and high-throughput identification of CD8<sup>+</sup> t-cell epitopes of eukaryotic pathogens</div> <div class="author"> Phasit Charoenkwan,¬†Nalini Schaduangrat,¬†<em><b>Nhat Truong</b> <b>Pham</b></em>,¬†<a href="https://scholar.google.com/citations?user=0vkenbwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a>,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=B3cea18AAAAJ" rel="external nofollow noopener" target="_blank">Watshara Shoombuatong</a> </div> <div class="periodical"> <em>International Journal of Biological Macromolecules</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0141813023011224" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="http://pmlabstack.pythonanywhere.com/Pretoria" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.ijbiomac.2023.124228"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.ijbiomac.2023.124228" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>T-cells recognize antigenic epitopes present on major histocompatibility complex (MHC) molecules, triggering an adaptive immune response in the host. T-cell epitope (TCE) identification is challenging because of the extensive number of undetermined proteins found in eukaryotic pathogens, as well as MHC polymorphisms. In addition, conventional experimental approaches for TCE identification are time-consuming and expensive. Thus, computational approaches that can accurately and rapidly identify CD8<sup>+</sup> T-cell epitopes (TCEs) of eukaryotic pathogens based solely on sequence information may facilitate the discovery of novel CD8<sup>+</sup> TCEs in a cost-effective manner. Here, Pretoria (Predictor of CD8<sup>+</sup> TCEs of eukaryotic pathogens) is proposed as the first stack-based approach for accurate and large-scale identification of CD8<sup>+</sup> TCEs of eukaryotic pathogens. In particular, Pretoria enabled the extraction and exploration of crucial information embedded in CD8<sup>+</sup> TCEs by employing a comprehensive set of 12 well-known feature descriptors extracted from multiple groups, including physicochemical properties, composition-transition-distribution, pseudo-amino acid composition, and amino acid composition. These feature descriptors were then utilized to construct a pool of 144 different machine learning (ML)-based classifiers based on 12 popular ML algorithms. Finally, the feature selection method was used to effectively determine the important ML classifiers for the construction of our stacked model. The experimental results indicated that Pretoria is an accurate and effective computational approach for CD8<sup>+</sup> TCE prediction; it was superior to several conventional ML classifiers and the existing method in terms of the independent test, with an accuracy of 0.866, MCC of 0.732, and AUC of 0.921. Additionally, to maximize user convenience for high-throughput identification of CD8<sup>+</sup> TCEs of eukaryotic pathogens, a user-friendly web server of Pretoria (http://pmlabstack.pythonanywhere.com/Pretoria) was developed and made freely available.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#600"><a href="">Heliyon</a></abbr></div> <div id="pham2023exploratory" class="col-sm-8"> <div class="title">An exploratory simulation study and prediction model on human brain behavior and activity using an integration of deep neural network and biosensor Rabi antenna</div> <div class="author"> <em><b>Nhat Truong</b> <b>Pham</b></em>,¬†Montree Bunruangses,¬†Phichai Youplao,¬†Anita Garhwal,¬†Kanad Ray,¬†Arup Roy,¬†Sarawoot Boonkirdram,¬†Preecha Yupapin,¬†Muhammad Arif Jalil,¬†Jalil Ali,¬†Shamim Kaiser,¬†<a href="https://scholar.google.com/citations?user=L8em2YoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Mufti Mahmud</a>,¬†<a href="https://scholar.google.com/citations?user=e-6kvkIAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Saurav Mallik</a>,¬†and¬†<a href="https://scholar.google.com/citations?user=eDRMnHQAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Zhongming Zhao</a> </div> <div class="periodical"> <em>Heliyon</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S2405844023029560" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.cell.com/heliyon/pdf/S2405-8440(23)02956-0.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/nhattruongpham/Deep_Brain_SigNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.heliyon.2023.e15749"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.heliyon.2023.e15749" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The plasmonic antenna probe is constructed using a silver rod embedded in a modified Mach-Zehnder interferometer (MZI) ad-drop filter. Rabi antennas are formed when space-time control reaches two levels of system oscillation and can be used as human brain sensor probes. Photonic neural networks are designed using brain-Rabi antenna communication, and transmissions are connected via neurons. Communication signals are carried by electron spin (up and down) and adjustable Rabi frequency. Hidden variables and deep brain signals can be obtained by external detection. A Rabi antenna has been developed by simulation using computer simulation technology (CST) software. Additionally, a communication device has been developed that uses the Optiwave program with Finite-Difference Time-Domain (OptiFDTD). The output signal is plotted using the MATLAB program with the parameters of the OptiFDTD simulation results. The proposed antenna oscillates in the frequency range of 192 THz to 202 THz with a maximum gain of 22.4 dBi. The sensitivity of the sensor is calculated along with the result of electron spin and applied to form a human brain connection. Moreover, intelligent machine learning algorithms are proposed to identify high-quality transmissions and predict the behavior of transmissions in the near future. During the process, a root mean square error (RMSE) of 2.3332 (¬±0.2338) was obtained. Finally, it can be said that our proposed model can efficiently predict human mind, thoughts, behavior as well as action/reaction, which can be greatly helpful in the diagnosis of various neuro-degenerative/psychological diseases (such as Alzheimer‚Äôs, dementia, etc.) and for security purposes.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#600"><a href="">T&amp;F JIT</a></abbr></div> <div id="pham2023speech" class="col-sm-8"> <div class="title">Speech emotion recognition using overlapping sliding window and Shapley additive explainable deep neural network</div> <div class="author"> <em><b>Nhat Truong</b> <b>Pham</b></em>,¬†<a href="https://scholar.google.com/citations?user=XFUxOkoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen<sub>1</sub></a>,¬†Vu Song Thuy Nguyen,¬†Bich Ngoc Hong Pham,¬†and¬†<a href="https://scholar.google.com/citations?user=2UKP440AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a> </div> <div class="periodical"> <em>Journal of Information and Telecommunication</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.tandfonline.com/doi/full/10.1080/24751839.2023.2187278" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.tandfonline.com/doi/epdf/10.1080/24751839.2023.2187278?needAccess=true&amp;role=button" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/nhattruongpham/osw-1d-prn-shap" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1080/24751839.2023.2187278"></span> <span class="__dimensions_badge_embed__" data-doi="10.1080/24751839.2023.2187278" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Speech emotion recognition (SER) has several applications, such as e-learning, human-computer interaction, customer service, and healthcare systems. Although researchers have investigated lots of techniques to improve the accuracy of SER, it has been challenging with feature extraction, classifier schemes, and computational costs. To address the aforementioned problems, we propose a new set of 1D features extracted by using an overlapping sliding window (OSW) technique for SER in this study. In addition, a deep neural network-based classifier scheme called the deep Pattern Recognition Network (PRN) is designed to categorize emotional states from the new set of 1D features. We evaluate the proposed method on the Emo-DB and the AESSD datasets that contain several different emotional states. The experimental results show that the proposed method achieves an accuracy of 98.5% and 87.1% on the Emo-DB and AESSD datasets, respectively. It is also more comparable with accuracy to and better than the state-of-the-art and current approaches that use 1D features on the same datasets for SER. Furthermore, the SHAP (SHapley Additive exPlanations) analysis is employed for interpreting the prediction model to assist system developers in selecting the optimal features to integrate into the desired system.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#600"><a href="">ESWA</a></abbr></div> <div id="nguyen2023fruit" class="col-sm-8"> <div class="title">Fruit-cov: An efficient vision-based framework for speedy detection and diagnosis of sars-cov-2 infections through recorded cough sounds</div> <div class="author"> Long H Nguyen,¬†<em><b>Nhat Truong</b> <b>Pham</b><sup>(*)(‚Ä†)</sup></em>,¬†Van Huong Do,¬†Liu Tai Nguyen,¬†Thanh Tin Nguyen,¬†<a href="https://scholar.google.com/citations?user=5b9ncWoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Hai Nguyen<sub>4</sub></a>,¬†<a href="https://scholar.google.com/citations?user=At-Y-H8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ngoc Duy Nguyen<sub>2</sub></a>,¬†<a href="https://scholar.google.com/citations?user=xr39SOwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Thanh Thi Nguyen<sub>3</sub></a>,¬†<a href="https://scholar.google.com/citations?user=XFUxOkoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen<sub>1</sub></a>,¬†Asim Bhatti,¬†and¬†<a href="https://scholar.google.com/citations?user=an_4IJkAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Chee Peng Lim</a> </div> <div class="periodical"> <em>Expert Systems with Applications</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0957417422022308" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/nhattruongpham/Fruit-CoV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.eswa.2022.119212"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.eswa.2022.119212" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>COVID-19 is an infectious disease caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). This deadly virus has spread worldwide, leading to a global pandemic since March 2020. A recent variant of SARS-CoV-2 named Delta is intractably contagious and responsible for more than four million deaths globally. Therefore, developing an efficient self-testing service for SARS-CoV-2 at home is vital. In this study, a two-stage vision-based framework, namely Fruit-CoV, is introduced for detecting SARS-CoV-2 infections through recorded cough sounds. Specifically, audio signals are converted into Log-Mel spectrograms, and the EfficientNet-V2 network is used to extract their visual features in the first stage. In the second stage, 14 convolutional layers extracted from the large-scale Pretrained Audio Neural Networks for audio pattern recognition (PANNs) and the Wavegram-Log-Mel-CNN are employed to aggregate feature representations of the Log-Mel spectrograms and the waveform. Finally, the combined features are used to train a binary classifier. In this study, a dataset provided by the AICovidVN 115M Challenge is employed for evaluation. It includes 7,371 recorded cough sounds collected throughout Vietnam, India, and Switzerland. Experimental results indicate that the proposed model achieves an Area Under the Receiver Operating Characteristic Curve (AUC) score of 92.8% and ranks first on the final leaderboard of the AICovidVN 115M Challenge. Our code is publicly available.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#600"><a href="">APIN</a></abbr></div> <div id="nguyen2023towards" class="col-sm-8"> <div class="title">Towards designing a generic and comprehensive deep reinforcement learning framework</div> <div class="author"> <a href="https://scholar.google.com/citations?user=At-Y-H8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ngoc Duy Nguyen<sub>2</sub></a>,¬†<a href="https://scholar.google.com/citations?user=xr39SOwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Thanh Thi Nguyen<sub>3</sub></a>,¬†<em><b>Nhat Truong</b> <b>Pham</b></em>,¬†<a href="https://scholar.google.com/citations?user=5b9ncWoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Hai Nguyen<sub>4</sub></a>,¬†Dang Tu Nguyen,¬†Thanh Dang Nguyen,¬†<a href="https://scholar.google.com/citations?user=an_4IJkAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Chee Peng Lim</a>,¬†Michael Johnstone,¬†Asim Bhatti,¬†<a href="https://scholar.google.com/citations?user=oorbAhoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Douglas Creighton</a>,¬†and¬†<a href="https://scholar.google.com/citations?user=pagzIgsAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Saeid Nahavandi</a> </div> <div class="periodical"> <em>Applied Intelligence</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/article/10.1007/s10489-022-03550-z" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/garlicdevs/Fruit-API" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://fruitlab.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1007/s10489-022-03550-z"></span> <span class="__dimensions_badge_embed__" data-doi="10.1007/s10489-022-03550-z" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Reinforcement learning (RL) has emerged as an effective approach for building an intelligent system, which involves multiple self-operated agents to collectively accomplish a designated task. More importantly, there has been a renewed focus on RL since the introduction of deep learning that essentially makes RL feasible to operate in high-dimensional environments. However, there are many diversified research directions in the current literature, such as multi-agent and multi-objective learning, and human-machine interactions. Therefore, in this paper, we propose a comprehensive software architecture that not only plays a vital role in designing a connect-the-dots deep RL architecture but also provides a guideline to develop a realistic RL application in a short time span. By inheriting the proposed architecture, software managers can foresee any challenges when designing a deep RL-based system. As a result, they can expedite the design process and actively control every stage of software development, which is especially critical in agile development environments. For this reason, we design a deep RL-based framework that strictly ensures flexibility, robustness, and scalability. To enforce generalization, the proposed architecture also does not depend on a specific RL algorithm, a network configuration, the number of agents, or the type of agents.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#600"><a href="">JCC</a></abbr></div> <div id="pham2022improving" class="col-sm-8"> <div class="title">Improving ligand-ranking of AutoDock Vina by changing the empirical parameters</div> <div class="author"> <a href="https://scholar.google.com/citations?user=_aQ5P4gAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">T Ngoc Han Pham</a>,¬†<a href="https://scholar.google.com/citations?user=a4xyHScAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Trung Hai Nguyen<sub>5</sub></a>,¬†Nguyen Minh Tam,¬†Thien Y. Vu,¬†<em><b>Nhat Truong</b> <b>Pham</b></em>,¬†Nguyen Truong Huy,¬†Binh Khanh Mai,¬†Nguyen Thanh Tung,¬†Minh Quan Pham,¬†Van V. Vu,¬†and¬†<a href="https://scholar.google.com/citations?user=_VxSvQkAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Son Tung Ngo</a> </div> <div class="periodical"> <em>Journal of Computational Chemistry</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.26779" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/nhattruongpham/mvina" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1002/jcc.26779"></span> <span class="__dimensions_badge_embed__" data-doi="10.1002/jcc.26779" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>AutoDock Vina (Vina) achieved a very high docking-success rate, <i>pÃÇ</i>, but give a rather low correlation coefficient, <i>R</i>, for binding affinity with respect to experiments. This low correlation can be an obstacle for ranking of ligand-binding affinity, which is the main objective of docking simulations. In this context, we evaluated the dependence of Vina <i>R</i> coefficient upon its empirical parameters. <i>R</i> is affected more by changing the gauss2 and rotation than other terms. The docking-success rate <i>pÃÇ</i> is sensitive to the alterations of the gauss1, gauss2, repulsion, and hydrogen bond parameters. Based on our benchmarks, the parameter set1 has been suggested to be the most optimal. The testing study over 800 complexes indicated that the modified Vina provided higher correlation with experiment <i>R<sub>set1</sub>=0.556¬±0.025</i> compared with <i>R<sub>Default</sub>=0.493¬±0.028</i> obtained by the original Vina and <i>R<sub>Vina 1.2</sub>=0.503¬±0.029</i> by Vina version 1.2. Besides, the modified Vina can be also applied more widely, giving <i>R ‚â• 0.500</i> for 32/48 targets, compared with the default package, giving <i>R ‚â• 0.500</i> for 31/48 targets. In addition, validation calculations for 1036 complexes obtained from version 2019 of PDBbind refined structures showed that the set1 of parameters gave higher correlation coefficient (<i>R<sub>set1</sub>=0.617¬±0.017</i>) than the default package (<i>R<sub>Default</sub>=0.543¬±0.020</i>) and Vina version 1.2 (<i>R<sub>Vina 1.2</sub>=0.540¬±0.020</i>). The version of Vina with set1 of parameters can be downloaded at https://github.com/sontungngo/mvina. The outcomes would enhance the ranking of ligand-binding affinity using Autodock Vina.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#600"><a href="">IEEE TFS</a></abbr></div> <div id="nguyen2022determination" class="col-sm-8"> <div class="title">Determination of the optimal number of clusters: a fuzzy-set based method</div> <div class="author"> <a href="https://scholar.google.com/citations?user=XFUxOkoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen<sub>1</sub></a>,¬†Vu Song Thuy Nguyen,¬†and¬†<em><b>Nhat Truong</b> <b>Pham</b></em> </div> <div class="periodical"> <em>IEEE Transactions on Fuzzy Systems</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9562269" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/TFUZZ.2021.3118113"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/TFUZZ.2021.3118113" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The optimal number of clusters (<i>C<sub>opt</sub></i>) is one of the determinants of clustering efficiency. In this article, we present a new method of quantifying <i>C<sub>opt</sub></i> for centroid-based clustering. First, we propose a new clustering validity index named fRisk(<i>C</i>) based on the fuzzy set theory. It takes the role of normalization and accumulation of local risks coming from each action either splitting data from a cluster or merging data into a cluster. fRisk(<i>C</i>) exploits the local distribution information of the database to catch the global information of the clustering process in the form of the risk degree. Based on the monotonous reduction property of fRisk(<i>C</i>), which is proved theoretically, we present a fRisk-based new algorithm named fRisk4-bA for determining <i>C<sub>opt</sub></i>. In the algorithm, the well-known L-method is employed as a supplemented tool to catch <i>C<sub>opt</sub></i> on the graph of the fRisk(<i>C</i>). Along with the stable convergence trend of the method to be proved theoretically, numerical surveys are also carried out. The surveys show that the high reliability and stability, as well as the sensitivity in separating/merging clusters in high-density areas, even if the presence of noise in the databases, are the strong points of the proposed method.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#215d42"><a href="">De-Factify</a></abbr></div> <div id="nguyen2022hcilab" class="col-sm-8"> <div class="title">HCILab at Memotion 2.0 2022: Analysis of sentiment, emotion and intensity of emotion classes from meme images using single and multi modalities</div> <div class="author"> Thanh Tin Nguyen,¬†<em><b>Nhat Truong</b> <b>Pham</b></em>,¬†<a href="https://scholar.google.com/citations?user=At-Y-H8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ngoc Duy Nguyen<sub>2</sub></a>,¬†<a href="https://scholar.google.com/citations?user=5b9ncWoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Hai Nguyen<sub>4</sub></a>,¬†Long H Nguyen,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=gGfbXFIAAAAJ" rel="external nofollow noopener" target="_blank">Yong-Guk Kim</a> </div> <div class="periodical"> <em>In Proceedings of De-Factify: Workshop on Multimodal Fact Checking and Hate Speech Detection (De-Factify@AAAI 2022), CEUR</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ceur-ws.org/Vol-3199/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ceur-ws.org/Vol-3199/paper12.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ngthanhtin/Memotion2_AAAI_WS_2022" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Nowadays, memes found on internet are overwhelming. Although they are innocuous and sometimes entertaining, there exist memes that contain sarcasm, offensive, or motivational feelings. In this study, several approaches are proposed to solve the multiple modality problem in analysing the given meme dataset. The imbalance issue has been addressed by using a new Auto Augmentation method and the uncorrelation issue has been mitigated by adopting deep Canonical Correlation Analysis to find the most correlated projections of visual and textual feature embedding. In addition, both stacked attention and multi-hop attention network are employed to efficiently generate aggregated features. As a result, our team, i.e. HCILab, achieved a weighted F1 score of 0.4995 for sentiment analysis, 0.7414 for emotion classification, and 0.5301 for scale/intensity of emotion classes on the leaderboard. This results are obtained by using concatenation between image and text model and our code can be found at https://github.com/ngthanhtin/Memotion2_AAAI_WS_2022.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#600"><a href="">JAEC</a></abbr></div> <div id="pham2020method" class="col-sm-8"> <div class="title">A method upon deep learning for speech emotion recognition</div> <div class="author"> <em><b>Nhat Truong</b> <b>Pham</b></em>,¬†<a href="https://scholar.google.com/citations?user=2UKP440AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a>,¬†and¬†<a href="https://scholar.google.com/citations?user=XFUxOkoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen<sub>1</sub></a> </div> <div class="periodical"> <em>Journal of Advanced Engineering and Computation</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://jaec.vn/index.php/JAEC/article/view/311" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://jaec.vn/index.php/JAEC/article/view/311/147" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.25073/jaec.202044.311"></span> <span class="__dimensions_badge_embed__" data-doi="10.25073/jaec.202044.311" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Feature extraction and emotional classification are significant roles in speech emotion recognition. It is hard to extract and select the optimal features, researchers can not be sure what the features should be. With deep learning approaches, features could be extracted by using hierarchical abstraction layers, but it requires high computational resources and a large number of data. In this article, we choose static, differential, and acceleration coefficients of log Mel-spectrogram as inputs for the deep learning model. To avoid performance degradation, we also add a skip connection with dilated convolution network integration. All representatives are fed into a self-attention mechanism with bidirectional recurrent neural networks to learn long term global features and exploit context for each time step. Finally, we investigate contrastive center loss with softmax loss as loss function to improve the accuracy of emotion recognition. For validating robustness and effectiveness, we tested the proposed method on the Emo-DB and ERC2019 datasets. Experimental results show that the performance of the proposed method is strongly comparable with the existing state-of-the-art methods on the Emo-DB and ERC2019 with 88% and 67%, respectively.</p> </div> </div> </div> </li></ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%70%68%61%6D%6E%68%61%74%74%72%75%6F%6E%67.%73%6B%79%6F@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-8086-6722" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=HybH2XkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.webofscience.com/wos/author/record/ABF-7566-2021" title="Clarivate" rel="external nofollow noopener" target="_blank"><i class="ai ai-clarivate"></i></a> <a href="https://www.researchgate.net/profile/Nhat-Truong-Pham/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://www.scopus.com/authid/detail.uri?authorId=57243980400" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus"></i></a> <a href="https://github.com/nhattruongpham" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/nhattruongpham" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/nhattruong_pham" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a href="https://dblp.org/pid/290/9204.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> </div> <div class="contact-note"> </div> </div> <div style="width: 150px; margin: 0 auto"> <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=RuxfKWte4G6yKP1jx6GjTKLxpqdoMdFm-CPzvceTJNY"></script> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2023 üß¨Nhat Truong Phamüß¨. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: July 03, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>