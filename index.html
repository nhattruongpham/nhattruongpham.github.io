<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Nhat Truong Pham</title> <meta name="author" content="Nhat Truong Pham"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha256-1oz1+rx2BF/9OiBUPSh1NEUwyUECNU5O70RoKnClGNs" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://nhattruongpham.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/collaborators/">Collaborators</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Nhat Truong Pham </h1> <p class="desc">Ph.D. Student in Integrative Biotechnology specializing in üß¨Applied Machine Learning/Deep Learning for Computational Biology and Bioinformaticsüß¨</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-1400.webp"></source> <img src="/assets/img/profile.png" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="profile.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>Room 62105, Biotechnology and Bioengineering Building 2, Seobu-ro, Jangan-gu, Suwon-si, Gyeonggi-do, 16419, Republic of Korea</p> </div> </div> <div class="clearfix"> <p><strong>Short Bio:</strong> My name is <i>Ph·∫°m Nh·∫≠t Tr∆∞·ªùng</i> - Vietnamese. I am currently a <del>first</del>/second-year Ph.D. Student at the <a href="https://skb.skku.edu/eng_gene/index.do" rel="external nofollow noopener" target="_blank">Department of Integrative Biotechnology</a>, <a href="https://biotech.skku.edu/eng_biotech/index.do" rel="external nofollow noopener" target="_blank">College of Biotechnology and Bioengineering</a>, <a href="https://www.skku.edu/eng/" rel="external nofollow noopener" target="_blank">Sungkyunkwan University (SKKU)</a>, Republic of Korea. I am working under the guidance of Assistant Professor <a href="https://skb.skku.edu/eng_gene/faculty.do?mode=view&amp;perId=LZStrB4DgrAqgzgNgwgGQCwGkAuArAigEQE4BCA5gHYBmMAnkQLw1A" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> in the <a href="https://balalab-skku.org/" rel="external nofollow noopener" target="_blank">Computational Biology and Bioinformatics Laboratory</a>. Prior to joining SKKU, I have worked as an Assistant Researcher at <a href="https://tdtu.edu.vn/en" rel="external nofollow noopener" target="_blank">Ton Duc Thang University (TDTU)</a>, Vietnam. I completed my M.E. degree in Automation and Control at TDTU, where I was supervised by Dr. <a href="https://sites.google.com/view/nguyensydzung" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen</a> and co-supervised by Dr. <a href="https://dnmduc.github.io/" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a>. I also hold a B.E. degree in Electronics and Telecommunication from TDTU, which I obtained in 2019 under the supervision of Dr. <a href="https://dnmduc.github.io/" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a>.</p> <p><strong>Research Interests:</strong> Artificial Intelligence, Computational Intelligence, Deep Learning, Machine Learning, Signal Processing, XAI &amp; Optimization, and their applications in the fields of Affective Computing, Audio &amp; Speech Processing, Bioinformatics, Computational Biology &amp; Medicine, Computer-aided Drug Design, Multi-omics, NeuroAI, Protein Design, and Protein Functional &amp; Structural Prediction.</p> <style>.no-margin{margin:0}.custom-figure{margin-top:0}.small-figure{width:100%}.reduce-gap{margin-bottom:5}</style> <p><strong>Inspirational Quotes:</strong></p> <blockquote class="no-margin reduce-gap"> <p><q><i>Your origin does not determine what kind of person you are. Only you can decide who you become.</i></q> - <a href="https://mengchih.com/about/" rel="external nofollow noopener" target="_blank">Meng Chih Chiang</a></p> <p><q><i>Life can be heavy, especially if you try to carry it all at once.</i></q> - <a href="https://www.taylorswift.com/" rel="external nofollow noopener" target="_blank">Taylor Swift</a></p> <p><q><i>Life can be seen as a balance between positive and negative aspects throughout its entirety. Therefore, it's essential to cherish every moment and embrace life fully.</i></q> - <a href="https://nhattruongpham.github.io/">Nhat Truong Pham</a></p> </blockquote> <center> $$ \begin{align*} \text{Life} &amp;= \int_{\text{birth}}^{\infty} \left(\frac{\text{Happiness} \times \text{Love} \times \text{Peace}}{\text{time}} - \frac{\text{Sadness} \times \text{Revenge} \times \text{Discord}}{\text{time}}\right) \, \text{dtime} \\ &amp;= \int_{\text{birth}}^{\rm{t_{2024}}} \left(\frac{\text{Happiness} \times \text{Love} \times \text{Peace}}{\text{time}} - \frac{\text{Sadness} \times \text{Revenge} \times \text{Discord}}{\text{time}}\right) \, \text{dtime} \\ &amp;\quad + \int_{\rm{t_{2024}}}^{\infty} \left(\frac{\text{Happiness}' \times \text{Love}' \times \text{Peace}'}{\text{time}} - \frac{\text{Sadness}' \times \text{Revenge}' \times \text{Discord}'}{\text{time}}\right) \, \text{dtime} \end{align*} $$ </center> <figure class="custom-figure reduce-gap"> <img src="assets/img/laboratory.gif" class="small-figure"> </figure> </div> <h2><a href="/news/" style="color: inherit;">Latest News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Apr 18, 2024</th> <td> One manuscript entitled <b>‚Äú<i>ac4C-AFL:</i> A high-precision identification of human mRNA N4-acetylcytidine sites based on adaptive feature representation learning‚Äù</b> has been accepted for publication in the <span style="color: #FF3636;"><i>Molecular Therapy - Nucleic Acids</i></span> journal </td> </tr> <tr> <th scope="row">Apr 12, 2024</th> <td> Our project, entitled <b>‚ÄúEnhancing m6A RNA modification prediction across cell lines and tissues using biological language models and a contrastive learning framework‚Äù</b>, has been selected for the <span style="color: #F29105;"><i>High-Performance Computing Support Project</i></span> by the <a href="https://www.kait.or.kr/eng/user/index.do" rel="external nofollow noopener" target="_blank">Korea Association for ICT Promotion (KAIT)</a>, Republic of Korea </td> </tr> <tr> <th scope="row">Apr 1, 2024</th> <td> Our project, entitled <b>‚ÄúEnhancing Peptide-HLA Class I Binding Prediction through Siamese Network-Based Contrastive Learning Framework with Feature Fusion (Ìé©ÌÉÄÏù¥Îìú-HLA Class I Í≤∞Ìï© ÏòàÏ∏° Í∞ïÌôîÎ•º ÏúÑÌïú ÌäπÏßï ÏúµÌï© Í∏∞Î∞ò ÏãúÏïî ÎÑ§Ìä∏ÏõåÌÅ¨ ÎåÄÏ°∞ ÌïôÏäµ ÌîÑÎ†àÏûÑÏõåÌÅ¨)‚Äù</b>, has been selected for the <span style="color: #F29105;"><i>2024 1st K-BDS analysis infrastructure utilization support program ([Track I] Large innovation research)</i></span> by the <a href="https://kbdsc.kisti.re.kr/index" rel="external nofollow noopener" target="_blank">Korea Bio Data Station (K-BDS)</a>, <a href="https://www.kisti.re.kr/eng/" rel="external nofollow noopener" target="_blank">Korea Institute of Science and Technology Information</a>, Republic of Korea </td> </tr> <tr> <th scope="row">Mar 12, 2024</th> <td> Our <b><i>Research Story</i></b> (<a href="https://www.skku.edu/skku/research/industry/researchStory_view.do?mode=view&amp;articleNo=116191" rel="external nofollow noopener" target="_blank">KOR</a>/<a href="https://www.skku.edu/eng/Research/industry/researchStory_view.do?mode=view&amp;articleNo=116236" rel="external nofollow noopener" target="_blank">ENG</a>) regarding the recently published papers in the <span style="color: #FF3636;"><i>Briefings in Bioinformatics</i></span> journal has been highlighted on <b><i>SKKU‚Äôs Research Stories</i></b> (<a href="https://www.skku.edu/skku/research/industry/researchStory.do" rel="external nofollow noopener" target="_blank">KOR</a>/<a href="https://www.skku.edu/eng/Research/industry/researchStory.do" rel="external nofollow noopener" target="_blank">ENG</a>) </td> </tr> <tr> <th scope="row">Jan 8, 2024</th> <td> Our paper, entitled <b>‚Äú<i>H2Opred:</i> a robust and efficient hybrid deep learning model for predicting 2‚Äô-O-methylation sites in human RNA‚Äù</b>, recently published in the <span style="color: #FF3636;"><i>Briefings in Bioinformatics</i></span> journal, has been cataloged in <a href="https://www.ibric.org/s.do?CWXEPvOoln" rel="external nofollow noopener" target="_blank"><i>Hanbitsa Paper</i></a> of the <a href="https://www.ibric.org/s.do?CWXEPvOoln" rel="external nofollow noopener" target="_blank"><i>Biological Research Information Center (BRIC)</i></a> </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">Selected Publications</a></h2> <div class="publications"> <p> (‚Ä†) denotes equal contribution </p> <p> (*) denotes correspondance </p> <p> <span style="display: inline-block; width: 15px; height: 15px; background-color: #600;"></span> denotes journal </p> <p> <span style="display: inline-block; width: 15px; height: 15px; background-color: #215d42;"></span> denotes conference </p> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ac4C-AFL-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ac4C-AFL-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ac4C-AFL-1400.webp"></source> <img src="/assets/img/publication_preview/ac4C-AFL.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ac4C-AFL.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pham2024ac4c" class="col-sm-8"> <div class="title"> <b><i>ac4C-AFL:</i></b> A high-precision identification of human mRNA N4-acetylcytidine sites based on adaptive feature representation learning</div> <div class="author"> <em><b>Nhat Truong</b> <b>Pham</b></em>,¬†Annie Terrina Terrance,¬†Young-Jun Jeon,¬†<a href="https://scholar.google.com/citations?user=5WP7pOUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Rajan Rakkiyapan</a>,¬†and¬†<a href="https://scholar.google.com/citations?user=0vkenbwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em>Molecular Therapy-Nucleic Acids</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.cell.com/molecular-therapy-family/nucleic-acids/fulltext/S2162-2531(24)00079-9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://balalab-skku.org/ac4C-AFL/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.omtn.2024.102192"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.omtn.2024.102192" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>RNA N4-acetylcytidine (ac4C) is a highly conserved RNA modification that plays a crucial role in controlling mRNA stability, processing, and translation. Consequently, accurate identification of ac4C sites across the genome is critical for understanding gene expression regulation mechanisms. In this study, we have developed ac4C-AFL, a bioinformatics tool that precisely identifies ac4C sites from primary RNA sequences. In ac4C-AFL, we identified the optimal sequence length for model building and implemented an adaptive feature representation strategy that is capable of extracting the most representative features from RNA. To identify the most relevant features, we proposed a novel ensemble feature importance scoring strategy to rank features effectively. We then used this information to conduct the sequential forward search, which individually determine the optimal feature set from the 16 sequence-derived feature descriptors. Utilizing these optimal feature descriptors, we constructed 176 baseline models using 11 popular classifiers. The most efficient baseline models were identified using the two-step feature selection approach, whose predicted scores were integrated and trained with the appropriate classifier to develop the final prediction model. Our rigorous cross-validations and independent tests demonstrate that ac4C-AFL surpasses contemporary tools in predicting ac4C sites. Moreover, we have developed a publicly accessible web server at https://balalab-skku.org/ac4C-AFL/.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/H2Opred-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/H2Opred-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/H2Opred-1400.webp"></source> <img src="/assets/img/publication_preview/H2Opred.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="H2Opred.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pham2024h2opred" class="col-sm-8"> <div class="title"> <b><i>H2Opred:</i></b> a robust and efficient hybrid deep learning model for predicting 2‚Äô-O-methylation sites in human RNA</div> <div class="author"> <em><b>Nhat Truong</b> <b>Pham</b></em>,¬†<a href="https://scholar.google.com/citations?user=5WP7pOUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Rajan Rakkiyapan</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=_YZClBgAAAAJ" rel="external nofollow noopener" target="_blank">Jongsun Park</a>,¬†<a href="https://scholar.google.com/citations?user=IRshBmMAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Adeel Malik</a>,¬†and¬†<a href="https://scholar.google.com/citations?user=0vkenbwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><span style="color: #FF3636;">Briefings in Bioinformatics</span></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://academic.oup.com/bib/article/25/1/bbad476/7510980" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://balalab-skku.org/H2Opred/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://www.ibric.org/s.do?CWXEPvOoln" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1093/bib/bbad476"></span> <span class="__dimensions_badge_embed__" data-doi="10.1093/bib/bbad476" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>2‚Äô-O-methylation (2OM) is the most common post-transcriptional modification of RNA. It plays a crucial role in RNA splicing, RNA stability and innate immunity. Despite advances in high-throughput detection, the chemical stability of 2OM makes it difficult to detect and map in messenger RNA. Therefore, bioinformatics tools have been developed using machine learning (ML) algorithms to identify 2OM sites. These tools have made significant progress, but their performances remain unsatisfactory and need further improvement. In this study, we introduced H2Opred, a novel hybrid deep learning (HDL) model for accurately identifying 2OM sites in human RNA. Notably, this is the first application of HDL in developing four nucleotide-specific models [adenine (A2OM), cytosine (C2OM), guanine (G2OM) and uracil (U2OM)] as well as a generic model (N2OM). H2Opred incorporated both stacked 1D convolutional neural network (1D-CNN) blocks and stacked attention-based bidirectional gated recurrent unit (Bi-GRU-Att) blocks. 1D-CNN blocks learned effective feature representations from 14 conventional descriptors, while Bi-GRU-Att blocks learned feature representations from five natural language processing-based embeddings extracted from RNA sequences. H2Opred integrated these feature representations to make the final prediction. Rigorous cross-validation analysis demonstrated that H2Opred consistently outperforms conventional ML-based single-feature models on five different datasets. Moreover, the generic model of H2Opred demonstrated a remarkable performance on both training and testing datasets, significantly outperforming the existing predictor and other four nucleotide-specific H2Opred models. To enhance accessibility and usability, we have deployed a user-friendly web server for H2Opred, accessible at https://balalab-skku.org/H2Opred/. This platform will serve as an invaluable tool for accurately predicting 2OM sites within human RNA, thereby facilitating broader applications in relevant research endeavors.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/MeL-STPhos-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/MeL-STPhos-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/MeL-STPhos-1400.webp"></source> <img src="/assets/img/publication_preview/MeL-STPhos.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="MeL-STPhos.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pham2024advancing" class="col-sm-8"> <div class="title">Advancing the accuracy of SARS-CoV-2 phosphorylation site detection via meta-learning approach</div> <div class="author"> <em><b>Nhat Truong</b> <b>Pham<sup>(‚Ä†)</sup></b></em>,¬†Le Thi Phan<sup>(‚Ä†)</sup>,¬†Jimin Seo,¬†Yeonwoo Kim,¬†Minkyung Song,¬†Sukchan Lee,¬†Young-Jun Jeon,¬†and¬†<a href="https://scholar.google.com/citations?user=0vkenbwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a> </div> <div class="periodical"> <em><span style="color: #FF3636;">Briefings in Bioinformatics</span></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://academic.oup.com/bib/article/25/1/bbad433/7459584" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://balalab-skku.org/MeL-STPhos/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://www.ibric.org/s.do?nLdsctWFBW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a> <a href="https://www.ibric.org/s.do?ovlRZSpKQk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Interview</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1093/bib/bbad433"></span> <span class="__dimensions_badge_embed__" data-doi="10.1093/bib/bbad433" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The worldwide appearance of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has generated significant concern and posed a considerable challenge to global health. Phosphorylation is a common post-translational modification that affects many vital cellular functions and is closely associated with SARS-CoV-2 infection. Precise identification of phosphorylation sites could provide more in-depth insight into the processes underlying SARS-CoV-2 infection and help alleviate the continuing coronavirus disease 2019 (COVID-19) crisis. Currently, available computational tools for predicting these sites lack accuracy and effectiveness. In this study, we designed an innovative meta-learning model, Meta-Learning for Serine/Threonine Phosphorylation (MeL-STPhos), to precisely identify protein phosphorylation sites. We initially performed a comprehensive assessment of 29 unique sequence-derived features, establishing prediction models for each using 14 renowned machine learning methods, ranging from traditional classifiers to advanced deep learning algorithms. We then selected the most effective model for each feature by integrating the predicted values. Rigorous feature selection strategies were employed to identify the optimal base models and classifier(s) for each cell-specific dataset. To the best of our knowledge, this is the first study to report two cell-specific models and a generic model for phosphorylation site prediction by utilizing an extensive range of sequence-derived features and machine learning algorithms. Extensive cross-validation and independent testing revealed that MeL-STPhos surpasses existing state-of-the-art tools for phosphorylation site prediction. We also developed a publicly accessible platform at https://balalab-skku.org/MeL-STPhos. We believe that MeL-STPhos will serve as a valuable tool for accelerating the discovery of serine/threonine phosphorylation sites and elucidating their role in post-translational regulation.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/AAD-Net-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/AAD-Net-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/AAD-Net-1400.webp"></source> <img src="/assets/img/publication_preview/AAD-Net.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="AAD-Net.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mustaqeem2023aad" class="col-sm-8"> <div class="title"> <b><i>AAD-Net:</i></b> Advanced end-to-end signal processing system for human emotion detection &amp; recognition using attention-based deep echo state network</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=uEZhRWAAAAAJ" rel="external nofollow noopener" target="_blank">Mustaqeem Khan</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=VcOjgngAAAAJ" rel="external nofollow noopener" target="_blank">Abdulmotaleb El Saddik</a>,¬†Fahd Saleh Alotaibi,¬†and¬†<em><b>Nhat Truong</b> <b>Pham</b></em> </div> <div class="periodical"> <em><span style="color: #FF3636;">Knowledge-Based Systems</span></em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705123002757" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.knosys.2023.110525"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.knosys.2023.110525" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Speech signals are the most convenient way of communication between human beings and the eventual method of Human-Computer Interaction (HCI) to exchange emotions and information. Recognizing emotions from speech signals is a challenging task due to the sparse nature of emotional data and features. In this article, we proposed a Deep Echo-State-Network (DeepESN) system for emotion recognition with a dilated convolution neural network and multi-headed attention mechanism. To reduce the model complexity, we incorporate a DeepESN that combines reservoir computing for higher-dimensional mapping. We also used fine-tuned Sparse Random Projection (SRP) to reduce dimensionality and adopted an early fusion strategy to fuse the extracted cues and passed the joint feature vector via a classification layer to recognize emotions. Our proposed model is evaluated on two public speech corpora, EMO-DB and RAVDESS, and tested for subject/speaker-dependent/independent performance. The results show that our proposed system achieves a high recognition rate, 91.14, 85.57 for EMO-DB, and 82.01, 77.02 for RAVDESS, using speaker-dependent and independent experiments, respectively. Our proposed system outperforms the State-of-The-Art (SOTA) while requiring less computational time.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/HDA_mADCRNN-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/HDA_mADCRNN-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/HDA_mADCRNN-1400.webp"></source> <img src="/assets/img/publication_preview/HDA_mADCRNN.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="HDA_mADCRNN.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pham2023hybrid" class="col-sm-8"> <div class="title">Hybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition</div> <div class="author"> <em><b>Nhat Truong</b> <b>Pham</b></em>,¬†<a href="https://scholar.google.com/citations?user=2UKP440AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Duc Ngoc Minh Dang</a>,¬†<a href="https://scholar.google.com/citations?user=At-Y-H8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ngoc Duy Nguyen<sub>2</sub></a>,¬†<a href="https://scholar.google.com/citations?user=xr39SOwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Thanh Thi Nguyen<sub>3</sub></a>,¬†<a href="https://scholar.google.com/citations?user=5b9ncWoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Hai Nguyen<sub>4</sub></a>,¬†<a href="https://scholar.google.com/citations?user=0vkenbwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Balachandran Manavalan</a>,¬†<a href="https://scholar.google.com/citations?user=an_4IJkAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Chee Peng Lim</a>,¬†and¬†<a href="https://scholar.google.com/citations?user=XFUxOkoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen<sub>1</sub></a> </div> <div class="periodical"> <em><span style="color: #FF3636;">Expert Systems with Applications</span></em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0957417423011107" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/nhattruongpham/hda-adcrnn-ser" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.eswa.2023.120608"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.eswa.2023.120608" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Recently, speech emotion recognition (SER) has become an active research area in speech processing, particularly with the advent of deep learning (DL). Numerous DL-based methods have been proposed for SER. However, most of the existing DL-based models are complex and require a large amounts of data to achieve a good performance. In this study, a new framework of deep attention-based dilated convolutional-recurrent neural networks coupled with a hybrid data augmentation method was proposed for addressing SER tasks. The hybrid data augmentation method constitutes an upsampling technique for generating more speech data samples based on the traditional and generative adversarial network approaches. By leveraging both convolutional and recurrent neural networks in a dilated form along with an attention mechanism, the proposed DL framework can extract high-level representations from three-dimensional log Mel spectrogram features. Dilated convolutional neural networks acquire larger receptive fields, whereas dilated recurrent neural networks overcome complex dependencies as well as the vanishing and exploding gradient issues. Furthermore, the loss functions are reconfigured by combining the SoftMax loss and the center-based losses to classify various emotional states. The proposed framework was implemented using the Python programming language and the TensorFlow deep learning library. To validate the proposed framework, the EmoDB and ERC benchmark datasets, which are imbalanced and/or small datasets, were employed. The experimental results indicate that the proposed framework outperforms other related state-of-the-art methods, yielding the highest unweighted recall rates of 88.03 ¬± 1.39 (%) and 66.56 ¬± 0.67 (%) for the EmoDB and ERC datasets, respectively.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Fruit-CoV-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Fruit-CoV-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Fruit-CoV-1400.webp"></source> <img src="/assets/img/publication_preview/Fruit-CoV.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Fruit-CoV.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="nguyen2023fruit" class="col-sm-8"> <div class="title"> <b><i>Fruit-CoV:</i></b> An efficient vision-based framework for speedy detection and diagnosis of SARS-CoV-2 infections through recorded cough sounds</div> <div class="author"> Long H Nguyen<sup>(‚Ä†)</sup>,¬†<em><b>Nhat Truong</b> <b>Pham<sup>(‚Ä†)(*)</sup></b></em>,¬†Van Huong Do,¬†Liu Tai Nguyen,¬†<a href="https://scholar.google.com/citations?user=zSAfD80AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Thanh Tin Nguyen<sub>6</sub></a>,¬†<a href="https://scholar.google.com/citations?user=5b9ncWoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Hai Nguyen<sub>4</sub></a>,¬†<a href="https://scholar.google.com/citations?user=At-Y-H8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ngoc Duy Nguyen<sub>2</sub></a>,¬†<a href="https://scholar.google.com/citations?user=xr39SOwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Thanh Thi Nguyen<sub>3</sub></a>,¬†<a href="https://scholar.google.com/citations?user=XFUxOkoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen<sub>1</sub></a>,¬†Asim Bhatti,¬†and¬†<a href="https://scholar.google.com/citations?user=an_4IJkAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Chee Peng Lim</a> </div> <div class="periodical"> <em><span style="color: #FF3636;">Expert Systems with Applications</span></em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0957417422022308" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/nhattruongpham/Fruit-CoV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1016/j.eswa.2022.119212"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.eswa.2022.119212" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>COVID-19 is an infectious disease caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). This deadly virus has spread worldwide, leading to a global pandemic since March 2020. A recent variant of SARS-CoV-2 named Delta is intractably contagious and responsible for more than four million deaths globally. Therefore, developing an efficient self-testing service for SARS-CoV-2 at home is vital. In this study, a two-stage vision-based framework, namely Fruit-CoV, is introduced for detecting SARS-CoV-2 infections through recorded cough sounds. Specifically, audio signals are converted into Log-Mel spectrograms, and the EfficientNet-V2 network is used to extract their visual features in the first stage. In the second stage, 14 convolutional layers extracted from the large-scale Pretrained Audio Neural Networks for audio pattern recognition (PANNs) and the Wavegram-Log-Mel-CNN are employed to aggregate feature representations of the Log-Mel spectrograms and the waveform. Finally, the combined features are used to train a binary classifier. In this study, a dataset provided by the AICovidVN 115M Challenge is employed for evaluation. It includes 7,371 recorded cough sounds collected throughout Vietnam, India, and Switzerland. Experimental results indicate that the proposed model achieves an Area Under the Receiver Operating Characteristic Curve (AUC) score of 92.8% and ranks first on the final leaderboard of the AICovidVN 115M Challenge. Our code is publicly available.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mVina-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mVina-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mVina-1400.webp"></source> <img src="/assets/img/publication_preview/mVina.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="mVina.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pham2022improving" class="col-sm-8"> <div class="title">Improving ligand-ranking of AutoDock Vina by changing the empirical parameters</div> <div class="author"> <a href="https://scholar.google.com/citations?user=_aQ5P4gAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">T Ngoc Han Pham</a>,¬†<a href="https://scholar.google.com/citations?user=a4xyHScAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Trung Hai Nguyen<sub>5</sub></a>,¬†Nguyen Minh Tam,¬†Thien Y. Vu,¬†<em><b>Nhat Truong</b> <b>Pham</b></em>,¬†Nguyen Truong Huy,¬†Binh Khanh Mai,¬†Nguyen Thanh Tung,¬†Minh Quan Pham,¬†Van V. Vu,¬†and¬†<a href="https://scholar.google.com/citations?user=_VxSvQkAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Son Tung Ngo</a> </div> <div class="periodical"> <em><span style="color: #FF3636;">Journal of Computational Chemistry</span></em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.26779" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/nhattruongpham/mvina" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1002/jcc.26779"></span> <span class="__dimensions_badge_embed__" data-doi="10.1002/jcc.26779" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>AutoDock Vina (Vina) achieved a very high docking-success rate, <i>pÃÇ</i>, but give a rather low correlation coefficient, <i>R</i>, for binding affinity with respect to experiments. This low correlation can be an obstacle for ranking of ligand-binding affinity, which is the main objective of docking simulations. In this context, we evaluated the dependence of Vina <i>R</i> coefficient upon its empirical parameters. <i>R</i> is affected more by changing the gauss2 and rotation than other terms. The docking-success rate <i>pÃÇ</i> is sensitive to the alterations of the gauss1, gauss2, repulsion, and hydrogen bond parameters. Based on our benchmarks, the parameter set1 has been suggested to be the most optimal. The testing study over 800 complexes indicated that the modified Vina provided higher correlation with experiment <i>R<sub>set1</sub>=0.556¬±0.025</i> compared with <i>R<sub>Default</sub>=0.493¬±0.028</i> obtained by the original Vina and <i>R<sub>Vina 1.2</sub>=0.503¬±0.029</i> by Vina version 1.2. Besides, the modified Vina can be also applied more widely, giving <i>R ‚â• 0.500</i> for 32/48 targets, compared with the default package, giving <i>R ‚â• 0.500</i> for 31/48 targets. In addition, validation calculations for 1036 complexes obtained from version 2019 of PDBbind refined structures showed that the set1 of parameters gave higher correlation coefficient (<i>R<sub>set1</sub>=0.617¬±0.017</i>) than the default package (<i>R<sub>Default</sub>=0.543¬±0.020</i>) and Vina version 1.2 (<i>R<sub>Vina 1.2</sub>=0.540¬±0.020</i>). The version of Vina with set1 of parameters can be downloaded at https://github.com/sontungngo/mvina. The outcomes would enhance the ranking of ligand-binding affinity using Autodock Vina.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/fRiskC.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/fRiskC.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/fRiskC.gif-1400.webp"></source> <img src="/assets/img/publication_preview/fRiskC.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="fRiskC.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="nguyen2022determination" class="col-sm-8"> <div class="title">Determination of the optimal number of clusters: a fuzzy-set based method</div> <div class="author"> <a href="https://scholar.google.com/citations?user=XFUxOkoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sy Dzung Nguyen<sub>1</sub></a>,¬†Vu Song Thuy Nguyen,¬†and¬†<em><b>Nhat Truong</b> <b>Pham</b></em> </div> <div class="periodical"> <em><span style="color: #FF3636;">IEEE Transactions on Fuzzy Systems</span></em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9562269" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/TFUZZ.2021.3118113"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/TFUZZ.2021.3118113" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The optimal number of clusters (<i>C<sub>opt</sub></i>) is one of the determinants of clustering efficiency. In this article, we present a new method of quantifying <i>C<sub>opt</sub></i> for centroid-based clustering. First, we propose a new clustering validity index named fRisk(<i>C</i>) based on the fuzzy set theory. It takes the role of normalization and accumulation of local risks coming from each action either splitting data from a cluster or merging data into a cluster. fRisk(<i>C</i>) exploits the local distribution information of the database to catch the global information of the clustering process in the form of the risk degree. Based on the monotonous reduction property of fRisk(<i>C</i>), which is proved theoretically, we present a fRisk-based new algorithm named fRisk4-bA for determining <i>C<sub>opt</sub></i>. In the algorithm, the well-known L-method is employed as a supplemented tool to catch <i>C<sub>opt</sub></i> on the graph of the fRisk(<i>C</i>). Along with the stable convergence trend of the method to be proved theoretically, numerical surveys are also carried out. The surveys show that the high reliability and stability, as well as the sensitivity in separating/merging clusters in high-density areas, even if the presence of noise in the databases, are the strong points of the proposed method.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%70%68%61%6D%6E%68%61%74%74%72%75%6F%6E%67.%73%6B%79%6F@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-8086-6722" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://www.webofscience.com/wos/author/record/ABF-7566-2021" title="Clarivate" rel="external nofollow noopener" target="_blank"><i class="ai ai-clarivate"></i></a> <a href="https://www.scopus.com/authid/detail.uri?authorId=57243980400" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus"></i></a> <a href="https://scholar.google.com/citations?user=HybH2XkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Nhat-Truong-Pham/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://dblp.org/pid/290/9204.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a href="https://www.semanticscholar.org/author/2077529712" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://www.linkedin.com/in/nhattruongpham" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/nhattruong_pham" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a href="https://github.com/nhattruongpham" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.kaggle.com/nhattruongpham" title="Kaggle" rel="external nofollow noopener" target="_blank"><i class="fab fa-kaggle"></i></a> <a href="https://www.hackerrank.com/profile/nhattruong_pham" title="HackerRank" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-hackerrank"></i></a> <a href="https://discord.com/users/673453370991837196" title="Discord" rel="external nofollow noopener" target="_blank"><i class="fab fa-discord"></i></a> </div> <div class="contact-note"> </div> </div> <div style="width: 150px; margin: 0 auto"> <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=RuxfKWte4G6yKP1jx6GjTKLxpqdoMdFm-CPzvceTJNY"></script> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2023 - 2024 Nhat Truong Pham. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>